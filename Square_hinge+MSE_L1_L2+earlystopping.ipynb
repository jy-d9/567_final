{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn            \n",
    "import torch.optim as optim      \n",
    "import torch.nn.functional as F  \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from matplotlib import rc\n",
    "\n",
    "#device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#number of epoch:\n",
    "epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for MNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 preload=False):\n",
    "        \"\"\" Intialize the MNIST dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # read filenames\n",
    "        for i in range(10):\n",
    "            filenames = glob.glob(osp.join(root, str(i), '*.png'))\n",
    "            for fn in filenames:\n",
    "                self.filenames.append((fn, i)) # (filename, label) pair\n",
    "                \n",
    "        # if preload dataset into memory\n",
    "        if preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for image_fn, label in self.filenames:            \n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(image.copy())\n",
    "            # avoid too many opened files bug\n",
    "            image.close()\n",
    "            self.labels.append(label)\n",
    "\n",
    "    # probably the most important to customize.\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.images is not None:\n",
    "            # If dataset is preloaded\n",
    "            image = self.images[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            image_fn, label = self.filenames[index]\n",
    "            image = Image.open(image_fn)\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        # return image and label\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set:\n",
    "trainset = MNIST(\n",
    "    root='./mnist_png/training',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "# Load the testset:\n",
    "testset = MNIST(\n",
    "    root='./mnist_png/testing',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "testset_loader = DataLoader(testset, batch_size=1000, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSeq, self).__init__()\n",
    "\n",
    "        #set seed\n",
    "        random.seed(100)\n",
    "        torch.manual_seed(100)\n",
    "\n",
    "        # conv layers: feature extractor\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            #nn.Dropout2d(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # fc layers: classifier\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(320, 300),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(),\n",
    "            nn.Linear(300, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to compute average loss and accuracy rate\n",
    "def result_hinge(loader):\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            sum_loss += nn.MultiMarginLoss(p=2,reduction='sum')(output, target).item()\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return (sum_loss/len(loader.dataset)), (100. * correct / len(loader.dataset)), correct, len(loader.dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squared hinge loss with l1 relularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hinge loss function\n",
    "def square_hinge_loss(output, target):\n",
    "    output_y = output[torch.arange(0,target.size(0)).long(), target.data].view(-1,1)\n",
    "    loss = output - output_y + 1\n",
    "    loss[torch.arange(0, target.size(0)).long(), target.data] = 0\n",
    "    loss[loss<0]=0\n",
    "    \n",
    "    return torch.sum(loss**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of regularization parameters from 0 to 0.03 with step size 0.001\n",
    "reg_params = np.linspace(0, 0.03, num=31, endpoint=True)\n",
    "\n",
    "#initiate a list storing training average loss for different reg_param\n",
    "train_ave_loss_L1 = []\n",
    "\n",
    "#initiate a list storing test average loss for different reg_param\n",
    "test_ave_loss_L1 = []\n",
    "\n",
    "#initiate a list storing training accuracy for different reg_param\n",
    "train_acc_L1 = []\n",
    "\n",
    "#initiate a list storing test accuracy for different reg_param\n",
    "test_acc_L1 = []\n",
    "\n",
    "#initiate a list storing matrix norm for different reg_param\n",
    "matrix_norm_L1 = []\n",
    "\n",
    "#initiate a list of lists tracking the training loss for different reg_param\n",
    "listoflist_L1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.000 Train Epoch: 1 [0/60000 (0%)]\tLoss: 8.971075\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.298104\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.506938\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.582439\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.311650\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.267367\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.236400\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.124812\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.174853\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.224026\n",
      "4.71s\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.192762\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.099457\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.140308\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.060311\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.054393\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.024118\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.330395\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.047682\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.202058\n",
      "4.63s\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.067100\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.132626\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.234675\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.127984\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.019500\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.180749\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.127405\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.082341\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.126526\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.100171\n",
      "4.55s\n",
      "\n",
      "Test set: Average loss: 0.0085, Accuracy: 9791/10000 (98%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437]\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.058472\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.387516\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.613859\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.672794\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.416511\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.377339\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.335394\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.221158\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.280651\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.336026\n",
      "4.21s\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.298113\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.202628\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.232316\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.163673\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.147643\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.115757\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.464056\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.135240\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.288170\n",
      "4.65s\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.161379\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.213035\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.330474\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.214653\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.111269\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.305153\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.211668\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.151622\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.214540\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.177951\n",
      "4.40s\n",
      "\n",
      "Test set: Average loss: 0.0094, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037]\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.145868\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.481565\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.715737\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.770402\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.517517\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.456544\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.426730\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.294580\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.356275\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.400938\n",
      "4.04s\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.369332\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.258954\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.276797\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.221360\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.189941\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.161232\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.515705\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.165021\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.345909\n",
      "4.26s\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.201364\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.261259\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.367008\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.248084\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.140558\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.355400\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.263160\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.172265\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.226019\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.213421\n",
      "4.90s\n",
      "\n",
      "Test set: Average loss: 0.0105, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051]\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.233265\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.561810\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.793523\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.845733\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.579074\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.536121\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.470502\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.351561\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.402119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.003 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.432020\n",
      "4.21s\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.392773\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.294262\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.292488\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.248163\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.210248\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.179372\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.567546\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.178709\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.378599\n",
      "4.40s\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.212856\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.278063\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.401630\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.262339\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.160322\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.363380\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.287548\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.183194\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.245987\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.217214\n",
      "4.66s\n",
      "\n",
      "Test set: Average loss: 0.0111, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709]\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.320662\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.653591\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.867799\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.905194\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.638011\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.576577\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.521912\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.379114\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.434170\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.451727\n",
      "4.58s\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.399637\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.312349\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.293117\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.264349\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.221154\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.190826\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.588997\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.182575\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.407557\n",
      "4.55s\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.226694\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.289268\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.435056\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.273882\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.179655\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.409189\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.321911\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.194026\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.263688\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.218703\n",
      "4.51s\n",
      "\n",
      "Test set: Average loss: 0.0120, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829]\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.408058\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.736868\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.935798\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.967899\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.682061\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.622353\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.555683\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.407278\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.458532\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.448716\n",
      "4.63s\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.403893\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.320478\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.295405\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.282836\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.218935\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.207259\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.587754\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.191014\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.425742\n",
      "4.75s\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.241959\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.306410\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.444511\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.286135\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.194446\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.415411\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.337267\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.201163\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.273169\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.224487\n",
      "4.65s\n",
      "\n",
      "Test set: Average loss: 0.0125, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375]\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.495455\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.810048\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.989458\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.002346\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.719278\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.656992\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.575389\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.410815\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.474744\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.445399\n",
      "4.60s\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.406496\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.325114\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.301622\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.294643\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.234693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.006 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.220409\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.608424\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.203838\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.440685\n",
      "4.56s\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.257001\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.335790\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.445120\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.296284\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.205386\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.422081\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.355326\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.215083\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.278801\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.232936\n",
      "4.16s\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467]\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.582850\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.878778\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.032050\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.029779\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.739827\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.681401\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.600689\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.425438\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.502800\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.451133\n",
      "4.82s\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.413920\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.332922\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.300360\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.310243\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.235424\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.231695\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.619649\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.213441\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.445185\n",
      "4.47s\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.267709\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.347923\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.447959\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.305528\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.212794\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.436678\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.361492\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.229069\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.280951\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.242645\n",
      "4.38s\n",
      "\n",
      "Test set: Average loss: 0.0133, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841]\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.670247\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.937908\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.078911\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.055083\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.756438\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.696225\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.608620\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.433641\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.520368\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.455700\n",
      "4.75s\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.425579\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.335367\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.298383\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.323035\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.233076\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.239991\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.615957\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.226029\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.462879\n",
      "4.36s\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.277623\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.365103\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.473579\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.316274\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.216330\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.418470\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.373377\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.238113\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.288530\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.249129\n",
      "4.64s\n",
      "\n",
      "Test set: Average loss: 0.0135, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668]\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.757644\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.006835\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.112297\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.069765\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.765682\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.698100\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.619493\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.442546\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.540499\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.459436\n",
      "4.55s\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.434904\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.342030\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.302184\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.332107\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.232422\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.250368\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.642955\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.234495\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.467979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.48s\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.281754\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.380441\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.467366\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.328869\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.222426\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.424288\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.374633\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.244189\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.301861\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.253741\n",
      "4.52s\n",
      "\n",
      "Test set: Average loss: 0.0135, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154]\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.845040\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.066028\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.144101\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.073871\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.774378\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.713512\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.627651\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.454259\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.560340\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.473538\n",
      "4.09s\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.447612\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.344614\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.309571\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.341960\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.233290\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.258334\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.657449\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.245207\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.465972\n",
      "5.10s\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.292267\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.392681\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.480543\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.339774\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.230978\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.408678\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.380444\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.253514\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.318326\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.261130\n",
      "4.60s\n",
      "\n",
      "Test set: Average loss: 0.0137, Accuracy: 9689/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268]\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [0/60000 (0%)]\tLoss: 9.932437\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.130770\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.161732\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.069756\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.782742\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.723014\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.632990\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.464814\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.572897\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.487689\n",
      "4.24s\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.455468\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.349606\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.313149\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.356175\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.238559\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.262886\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.674133\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.249239\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.482422\n",
      "4.18s\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.294731\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.406070\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.488360\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.352380\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.237296\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.401070\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.390277\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.260323\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.318513\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.264855\n",
      "4.54s\n",
      "\n",
      "Test set: Average loss: 0.0138, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982]\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.019834\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.188241\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.182111\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.072407\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.792945\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.739619\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.640589\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.471994\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.583093\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.501009\n",
      "4.30s\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.462233\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.355858\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.322080\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.363644\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.242292\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.269614\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.685635\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.261848\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.479835\n",
      "4.29s\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.301922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.012 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.403869\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.506241\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.359843\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.238930\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.406039\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.397450\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.266341\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.315474\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.268241\n",
      "4.30s\n",
      "\n",
      "Test set: Average loss: 0.0138, Accuracy: 9693/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855]\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.107230\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.235720\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.200639\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.070174\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.801407\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.752370\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.639618\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.481916\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.595241\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.514712\n",
      "4.15s\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.463897\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.360292\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.333385\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.371342\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.249973\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.276353\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.677873\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.261411\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.488817\n",
      "4.39s\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.310410\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.413469\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.521259\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.369035\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.243912\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.409364\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.405970\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.266622\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.314691\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.274738\n",
      "4.54s\n",
      "\n",
      "Test set: Average loss: 0.0139, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376]\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.194627\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.284494\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.203230\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.060990\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.801336\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.770519\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.644484\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.484816\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.609982\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.526832\n",
      "4.49s\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.469833\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.366298\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.332016\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.385928\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.258599\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.281818\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.669526\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.264599\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.505447\n",
      "4.78s\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.319304\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.418897\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.522269\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.377753\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.243934\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.411836\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.413366\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.270974\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.308871\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.278336\n",
      "4.76s\n",
      "\n",
      "Test set: Average loss: 0.0138, Accuracy: 9695/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811]\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.282023\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.323676\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.216099\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.055918\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.811963\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.778653\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.643488\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.496662\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.619107\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.537896\n",
      "4.96s\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.475432\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.370036\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.341396\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.393541\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.261027\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.286917\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.679660\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.264279\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.510699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.59s\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.326239\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.428343\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.527744\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.379329\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.246571\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.409288\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.414726\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.273284\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.308975\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.287980\n",
      "4.37s\n",
      "\n",
      "Test set: Average loss: 0.0138, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454]\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.369420\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.371476\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.217187\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.046088\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.815731\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.785129\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.651232\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.505546\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.626344\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.546469\n",
      "4.60s\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.470211\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.374400\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.342194\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.406867\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.261657\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.293887\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.678033\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.276307\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.514430\n",
      "4.73s\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.334600\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.440548\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.520749\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.385716\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.243845\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.388345\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.421209\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.269035\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.308987\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.285126\n",
      "4.47s\n",
      "\n",
      "Test set: Average loss: 0.0139, Accuracy: 9700/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029]\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.456817\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.399129\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.218137\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.039685\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.824589\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.796412\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.657867\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.508510\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.638969\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.556017\n",
      "4.40s\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.473936\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.374546\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.344862\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.408948\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.266913\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.299172\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.682027\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.278839\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.540488\n",
      "4.09s\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.342073\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.446369\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.509510\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.385142\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.245327\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.388768\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.417101\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.270990\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.308978\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.290661\n",
      "4.07s\n",
      "\n",
      "Test set: Average loss: 0.0142, Accuracy: 9679/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694]\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.544212\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.437177\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.222805\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.034394\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.828287\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.799432\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.667567\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.514759\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.655822\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.568376\n",
      "4.69s\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.476496\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.378435\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.347475\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.417273\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.269916\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.303242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.018 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.698477\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.285939\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.538399\n",
      "4.47s\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.349972\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.447807\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.510585\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.387446\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.247987\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.390340\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.413426\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.262525\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.302051\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.288330\n",
      "4.36s\n",
      "\n",
      "Test set: Average loss: 0.0143, Accuracy: 9687/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726]\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.631609\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.479605\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.235280\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.031614\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.841022\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.796035\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.675294\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.522890\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.661886\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.570061\n",
      "4.17s\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.471683\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.374848\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.353611\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.424703\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.271699\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.308788\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.696667\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.286731\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.542883\n",
      "4.64s\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.357635\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.437624\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.521855\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.389042\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.246541\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.381085\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.416992\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.265084\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.312118\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.289308\n",
      "4.57s\n",
      "\n",
      "Test set: Average loss: 0.0144, Accuracy: 9680/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832]\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.719006\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.498502\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.238045\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.034959\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.847273\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.808582\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.684092\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.520947\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.662971\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.581417\n",
      "4.90s\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.464018\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.381886\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.358697\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.428011\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.282473\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.308410\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.709422\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.298271\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.540242\n",
      "4.56s\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.367284\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.440897\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.527118\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.386385\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.246056\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.384611\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.409991\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.268185\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.309570\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.295951\n",
      "4.45s\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 9674/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734]\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.806402\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.536144\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.243800\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.044617\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.853576\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.809406\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.685235\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.522636\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.669381\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.591225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.60s\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.466266\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.378144\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.359653\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.425460\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.284356\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.315168\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.713309\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.302638\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.550541\n",
      "4.48s\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.375968\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.436587\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.544648\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.388984\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.245318\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.392799\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.416350\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.279188\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.311186\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.301608\n",
      "4.19s\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 9679/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265]\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.893799\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.556319\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.251160\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.055892\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.854369\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.824659\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.696931\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.530717\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.670780\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.595929\n",
      "4.97s\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.460248\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.380073\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.363161\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.424448\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.297721\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.316808\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.731273\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.308665\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.553353\n",
      "4.23s\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.377509\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.431388\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.539044\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.392787\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.240482\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.398842\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.417167\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.276777\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.317925\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.300506\n",
      "4.79s\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537]\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [0/60000 (0%)]\tLoss: 10.981195\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.578672\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.258460\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.064363\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.854960\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.822810\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.709491\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.526914\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.672986\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.613157\n",
      "4.66s\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.457668\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.387815\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.365304\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.432344\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.301236\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.317273\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.740028\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.311104\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.563676\n",
      "4.47s\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.382335\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.422955\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.547574\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.395440\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.237934\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.395073\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.409493\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.273499\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.311328\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.301477\n",
      "4.57s\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9673/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489]\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.068592\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.597480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.024 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.267989\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.075555\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.859502\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.825704\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.714267\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.535037\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.680476\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.615374\n",
      "4.51s\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.459888\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.389256\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.368559\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.437628\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.303210\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.315873\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.745477\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.318904\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.559429\n",
      "4.63s\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.388463\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.418155\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.540697\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.394089\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.239632\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.410816\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.408630\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.275044\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.312371\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.303606\n",
      "4.47s\n",
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9667/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988]\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.155989\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.613373\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.269933\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.086477\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.858844\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.826532\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.724863\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.536103\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.683211\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.615833\n",
      "4.50s\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.454685\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.388897\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.370166\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.434474\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.305820\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.317383\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.764450\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.323526\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.545220\n",
      "4.69s\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.379486\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.429034\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.537923\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.395193\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.245010\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.402862\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.416779\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.282010\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.315608\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.305887\n",
      "4.51s\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9664/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988, 0.01704988018249472]\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.243385\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.629936\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.275322\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.097897\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.851910\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.834211\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.723677\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.542052\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.688833\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.622909\n",
      "4.30s\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.452315\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.387131\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.370298\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.443639\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.301360\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.320927\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.753191\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.332413\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.543169\n",
      "4.76s\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.388151\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.429533\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.543942\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.390825\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.239683\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.388798\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.423290\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.289029\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.314250\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.307094\n",
      "3.98s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0152, Accuracy: 9663/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988, 0.01704988018249472, 0.017135734559906024]\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.330782\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.641917\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.276499\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.105732\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.853958\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.846509\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.734171\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.539855\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.695962\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.636689\n",
      "4.52s\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.453615\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.386526\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.375467\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.444713\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.310612\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.325113\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.765569\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.339536\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.545781\n",
      "4.38s\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.384165\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.429777\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.537493\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.390148\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.235741\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.407161\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.419499\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.290912\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.311411\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.303446\n",
      "4.46s\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy: 9658/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988, 0.01704988018249472, 0.017135734559906024, 0.017363721794759233]\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.418179\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.659311\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.281542\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.108870\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.855724\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.847249\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.740401\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.544611\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.690654\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.627375\n",
      "4.50s\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.451908\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.390785\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.371057\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.443728\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.316397\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.326680\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.766594\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.347052\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.543402\n",
      "4.54s\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.398672\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.434317\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.548869\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.391206\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.238470\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.387042\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.432739\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.292368\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.317055\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.315171\n",
      "4.57s\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy: 9654/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988, 0.01704988018249472, 0.017135734559906024, 0.017363721794759233, 0.01736933689309905]\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.505575\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.670958\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.278481\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.115555\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.854383\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.856769\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.749205\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.553198\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.698930\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.641261\n",
      "4.60s\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.451864\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.393013\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.373446\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.444993\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.313604\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.330230\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.779660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.029 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.347822\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.543318\n",
      "4.52s\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.385541\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.428881\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.563176\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.395562\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.244302\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.396086\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.433231\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.293425\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.318092\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.312667\n",
      "4.32s\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988, 0.01704988018249472, 0.017135734559906024, 0.017363721794759233, 0.01736933689309905, 0.017375302124582232]\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [0/60000 (0%)]\tLoss: 11.592972\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.689184\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.272420\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.119908\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.856417\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.866140\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.751747\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.546646\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.708501\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.639573\n",
      "4.50s\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.451512\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.394592\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.376452\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.450058\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.320232\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.330843\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.778680\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.359092\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.542639\n",
      "4.64s\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.391052\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.428894\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.561722\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.397370\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.246224\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.380709\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.437044\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.293710\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.320875\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.315301\n",
      "4.29s\n",
      "\n",
      "Test set: Average loss: 0.0155, Accuracy: 9653/10000 (97%)\n",
      "\n",
      "train_ave_loss_L1 [0.008281064413990437, 0.009452169128973037, 0.01100007974524051, 0.011976110033535709, 0.013215926541946829, 0.013985818937793375, 0.014583101227134467, 0.015063597352864841, 0.015073462093745668, 0.01516144849335154, 0.015263297972207268, 0.015332203572615982, 0.01536654927233855, 0.015451250860343376, 0.01546810204597811, 0.015519966731903454, 0.015562347318915029, 0.01588620252919694, 0.015974459544196726, 0.01618701728476832, 0.016579372041858734, 0.016666501611533265, 0.016753613388662537, 0.01697733266974489, 0.017107790105851988, 0.01704988018249472, 0.017135734559906024, 0.017363721794759233, 0.01736933689309905, 0.017375302124582232, 0.01746247625866284]\n"
     ]
    }
   ],
   "source": [
    "for reg_param in reg_params:\n",
    "    model = NetSeq().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    model.train() #training mode\n",
    "    iteration = 0\n",
    "    a_list = [] #tracking the loss function value\n",
    "    for ep in range(epoch):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            #compute the L1 norm of weight matrix on the last layer \n",
    "            l1_norm = torch.norm(model.fc_layers[-1].weight, p=1)\n",
    "            \n",
    "            #compute loss\n",
    "            loss = square_hinge_loss(output, target)/target.size(0)   + reg_param*l1_norm\n",
    "            a_list.append(loss.item())\n",
    "            \n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Regularization parameter: {:.3f} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    reg_param, ep+1, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "    \n",
    "    result_train = result_hinge(trainset_loader)\n",
    "    result_test = result_hinge(testset_loader)\n",
    "    train_ave_loss_L1.append(result_train[0])\n",
    "    test_ave_loss_L1.append(result_test[0])\n",
    "    train_acc_L1.append(result_train[1])\n",
    "    test_acc_L1.append(result_test[1])\n",
    "    matrix_norm_L1.append(np.linalg.norm(model.fc_layers[-1].weight.cpu().detach().numpy()))\n",
    "    listoflist_L1.append(list(a_list))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        result_test[0], result_test[2], result_test[3],\n",
    "        result_test[1]))\n",
    "    print('train_ave_loss_L1',train_ave_loss_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average loss versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Average Training and Test Loss Squared Hinge Loss with L1 Regularization')\n",
    "plt.plot(reg_params, train_ave_loss_L1, label=\"Training\")\n",
    "plt.plot(reg_params, test_ave_loss_L1, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy rate versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy Rate (Squared Hinge Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params, train_acc_L1, label=\"Training\")\n",
    "plt.plot(reg_params, test_acc_L1, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"100 * Accuracy rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the Frobenius norm of the last weight matrix versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Frobenius Norm of the Last Weight Matrix (Squared Hinge Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params, matrix_norm_L1)\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Frobenius norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of epoch:\n",
    "epoch1 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params1 = np.linspace(0.02, 0.03, num=11, endpoint=True)\n",
    "#initiate a list storing training average loss for different reg_param\n",
    "train_ave_loss_L1_1 = []\n",
    "\n",
    "#initiate a list storing test average loss for different reg_param\n",
    "test_ave_loss_L1_1 = []\n",
    "\n",
    "#initiate a list storing training accuracy for different reg_param\n",
    "train_acc_L1_1 = []\n",
    "\n",
    "#initiate a list storing test accuracy for different reg_param\n",
    "test_acc_L1_1 = []\n",
    "\n",
    "#initiate a list storing matrix norm for different reg_param\n",
    "matrix_norm_L1_1 = []\n",
    "\n",
    "#initiate a list of lists tracking the training loss for different reg_param\n",
    "listoflist_L1_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.020 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.645039\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.709268\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.098234\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.916759\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.896754\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904520\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.908108\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.896070\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.896424\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.885781\n",
      "4.14s\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.874824\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.814830\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.724968\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.651197\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.573127\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.390196\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.309517\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.253574\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.283871\n",
      "4.42s\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.266977\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.213321\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.236505\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.200119\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.184560\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.213851\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.154594\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.194554\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.186207\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.144563\n",
      "4.31s\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.186301\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.169135\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.182382\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.178266\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.118689\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.141674\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.156575\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.147432\n",
      "Regularization parameter: 0.020 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.191662\n",
      "4.01s\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.113388\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.170142\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.126310\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.114071\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.196717\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.100884\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.133587\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.111381\n",
      "Regularization parameter: 0.020 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.137583\n",
      "4.11s\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.144355\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.146900\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.097117\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.120869\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.118662\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.096361\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.095457\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.100269\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.109235\n",
      "Regularization parameter: 0.020 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.105411\n",
      "4.12s\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.137067\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.119553\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.106067\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.110603\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.115073\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.115687\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.097492\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.097752\n",
      "Regularization parameter: 0.020 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.073758\n",
      "4.08s\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.098551\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.107342\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.080942\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.107757\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.098657\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.091907\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.092554\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.068410\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.106769\n",
      "Regularization parameter: 0.020 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.128232\n",
      "4.42s\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.079648\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.114047\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.105832\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.111911\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.117259\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.072294\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.081459\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.105471\n",
      "Regularization parameter: 0.020 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.103660\n",
      "4.04s\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.128609\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.081848\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.091367\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.114873\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.134671\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.111646\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.086573\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.097441\n",
      "Regularization parameter: 0.020 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.067288\n",
      "4.14s\n",
      "\n",
      "Test set: Average loss: 0.0507, Accuracy: 9103/10000 (91%)\n",
      "\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.732435\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.711653\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.072134\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.913032\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.896743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.021 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.903775\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.908613\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.898157\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.901562\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.898676\n",
      "3.97s\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.894947\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.893533\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.879709\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.844732\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.842735\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.732926\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.665460\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.490714\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.451968\n",
      "3.86s\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.371889\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.279824\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.290825\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.238262\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.212445\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.263364\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.175695\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.218875\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.202174\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.164980\n",
      "3.97s\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.192024\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.178964\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.195487\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.187614\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.124294\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.152825\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.165600\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.159986\n",
      "Regularization parameter: 0.021 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.191619\n",
      "4.14s\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.119259\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.188756\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.121901\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.117056\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.181855\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.104430\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.141273\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.112804\n",
      "Regularization parameter: 0.021 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.138895\n",
      "3.86s\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.148087\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.146188\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.094009\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.124860\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.120903\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.096322\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.094378\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.103358\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.111127\n",
      "Regularization parameter: 0.021 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.107126\n",
      "4.02s\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.136280\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.124308\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.106079\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.109371\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.114277\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.116856\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.103532\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.104877\n",
      "Regularization parameter: 0.021 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.074347\n",
      "4.09s\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.100448\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.111905\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.083497\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.111003\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.104421\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.095934\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.093566\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.072314\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.108267\n",
      "Regularization parameter: 0.021 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.125986\n",
      "4.19s\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.083407\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.117423\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.107588\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.108414\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.124972\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.073540\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.081896\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.110348\n",
      "Regularization parameter: 0.021 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.105705\n",
      "4.08s\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.133617\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.088482\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.089581\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.125418\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.140614\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.120264\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.088469\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.094858\n",
      "Regularization parameter: 0.021 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.065191\n",
      "4.16s\n",
      "\n",
      "Test set: Average loss: 0.0529, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.819832\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.711188\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.046139\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.910767\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.896707\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.903985\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.908889\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.898720\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.902408\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.900915\n",
      "4.02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.022 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.898040\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.906358\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.899127\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.887427\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.894857\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.872121\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.819468\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.777040\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.773914\n",
      "4.33s\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.640201\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.475961\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.413891\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.298109\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.266100\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.308567\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.210752\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.248639\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.227651\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.191994\n",
      "4.23s\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.212561\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.199201\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.210114\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.200982\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.142712\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.160431\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.176230\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.168403\n",
      "Regularization parameter: 0.022 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.199946\n",
      "4.10s\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.126216\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.201287\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.128881\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.126041\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.190017\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.110983\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.149138\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.120226\n",
      "Regularization parameter: 0.022 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.143248\n",
      "4.12s\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.155641\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.151289\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.099448\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.127951\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.127214\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.101627\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.098137\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.109248\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.115978\n",
      "Regularization parameter: 0.022 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.111718\n",
      "3.97s\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.140320\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.129911\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.110540\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.113468\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.120289\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.120341\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.108017\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.108350\n",
      "Regularization parameter: 0.022 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.077564\n",
      "3.96s\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.104479\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.116220\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.086642\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.115077\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.107364\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.097809\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.096431\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.075496\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.110330\n",
      "Regularization parameter: 0.022 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.126061\n",
      "3.68s\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.086616\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.122647\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.113352\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.111394\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.132023\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.076564\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.085340\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.112941\n",
      "Regularization parameter: 0.022 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.109272\n",
      "4.18s\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.137465\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.090512\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.091698\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.128864\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.142634\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.122846\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.093062\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.099216\n",
      "Regularization parameter: 0.022 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.067116\n",
      "4.23s\n",
      "\n",
      "Test set: Average loss: 0.0544, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.907228\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.708150\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.021153\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.909157\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.896776\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904148\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.909189\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.898998\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.902697\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.901604\n",
      "4.10s\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.898960\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.908941\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.902440\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.893965\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.913514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.023 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.909129\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.908675\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.906010\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.903837\n",
      "4.00s\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.891176\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.855089\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.820286\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.742054\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.717585\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.600427\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.488686\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.440207\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.341694\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.281243\n",
      "4.08s\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.280143\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.258654\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.260519\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.255443\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.190763\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.188039\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.203827\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.193101\n",
      "Regularization parameter: 0.023 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.222432\n",
      "4.15s\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.147524\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.228914\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.148378\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.144390\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.213222\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.125625\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.166555\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.134438\n",
      "Regularization parameter: 0.023 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.152183\n",
      "4.33s\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.173436\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.159406\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.108811\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.138818\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.138650\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.110939\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.102355\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.119797\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.123053\n",
      "Regularization parameter: 0.023 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.119452\n",
      "4.12s\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.145587\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.139201\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.117099\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.120273\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.127359\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.127348\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.113577\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.114440\n",
      "Regularization parameter: 0.023 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.083019\n",
      "3.91s\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.112182\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.118694\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.089514\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.119541\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.113988\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.102333\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.099809\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.078286\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.114108\n",
      "Regularization parameter: 0.023 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.127195\n",
      "4.32s\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.089566\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.126229\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.121379\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.116863\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.137401\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.081324\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.088574\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.115836\n",
      "Regularization parameter: 0.023 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.112769\n",
      "4.13s\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.143593\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.091085\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.094812\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.130837\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.149989\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.125980\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.097471\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.102661\n",
      "Regularization parameter: 0.023 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.067566\n",
      "4.03s\n",
      "\n",
      "Test set: Average loss: 0.0558, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.994625\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.702624\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.997602\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.908380\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.897082\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904296\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.909343\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.899056\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.902878\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.901871\n",
      "4.07s\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.899121\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.909264\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.902757\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.894319\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.914494\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.910571\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.911947\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.911489\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.910400\n",
      "4.18s\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.899450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.024 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.904361\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.901020\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.897817\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.906924\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.901473\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.912317\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.912660\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.902878\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.900477\n",
      "3.99s\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.898493\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.910680\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.915120\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.894356\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.908438\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.894599\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.894361\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.899496\n",
      "Regularization parameter: 0.024 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.900248\n",
      "4.21s\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.896316\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.890809\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.890973\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.901639\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.890492\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.890748\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.897266\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.899135\n",
      "Regularization parameter: 0.024 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.899518\n",
      "4.30s\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.910772\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.906915\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.894646\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.900709\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.912321\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.906975\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.895265\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.911267\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.910746\n",
      "Regularization parameter: 0.024 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.883202\n",
      "4.34s\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.901690\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.908780\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.916084\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.898232\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.910718\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.913268\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.898913\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.907110\n",
      "Regularization parameter: 0.024 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.888002\n",
      "4.16s\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.891905\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.909752\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.899712\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.915442\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.903437\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.895927\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.906929\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.898084\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.906486\n",
      "Regularization parameter: 0.024 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.895674\n",
      "3.76s\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.891773\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.897675\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.897277\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.902601\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.890352\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.891850\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.905085\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.905661\n",
      "Regularization parameter: 0.024 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.897609\n",
      "4.24s\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.914163\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.907036\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.906152\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.903181\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.892557\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.916657\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.903471\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.911464\n",
      "Regularization parameter: 0.024 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.899146\n",
      "4.14s\n",
      "\n",
      "Test set: Average loss: 0.8979, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.082021\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.694790\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.975965\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.907905\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.897259\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904392\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.909479\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.899207\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.903058\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.902066\n",
      "3.97s\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.899291\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.909434\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.902875\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.894334\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.914644\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.910700\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.912050\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.911643\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.910487\n",
      "3.97s\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.899473\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.904594\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.901069\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.898017\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.907020\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.901663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.025 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.912393\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.912679\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.902928\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.900579\n",
      "3.81s\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.898620\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.910786\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.915355\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.894431\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.908636\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.894705\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.894436\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.899680\n",
      "Regularization parameter: 0.025 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.900401\n",
      "3.98s\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.896414\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.890944\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.891161\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.901778\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.890577\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.890915\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.897253\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.899230\n",
      "Regularization parameter: 0.025 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.899596\n",
      "4.09s\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.910963\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.906994\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.894879\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.900937\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.912387\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.907140\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.895381\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.911430\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.910877\n",
      "Regularization parameter: 0.025 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.883331\n",
      "4.01s\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.901726\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.908933\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.916196\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.898363\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.910884\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.913414\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.899073\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.907328\n",
      "Regularization parameter: 0.025 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.888057\n",
      "4.22s\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.891980\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.909899\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.899967\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.915439\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.903560\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.896093\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.907042\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.898266\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.906561\n",
      "Regularization parameter: 0.025 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.895733\n",
      "4.22s\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.891938\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.897734\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.897334\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.902657\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.890333\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.891913\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.905246\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.905766\n",
      "Regularization parameter: 0.025 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.897695\n",
      "4.22s\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.914353\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.907182\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.906285\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.903385\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.892618\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.916874\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.903524\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.911558\n",
      "Regularization parameter: 0.025 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.899448\n",
      "3.96s\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.169418\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.684596\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.956372\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.907721\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.897498\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904534\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.909628\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.899374\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.903115\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.902279\n",
      "3.88s\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.899530\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.909621\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.903105\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.894569\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.914752\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.910875\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.912245\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.911818\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.910762\n",
      "4.25s\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.899719\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.904710\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.901289\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.898089\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.907249\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.901835\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.912551\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.912912\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.903139\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.900861\n",
      "4.12s\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.898796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.026 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.910935\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.915572\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.894625\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.908889\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.894848\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.894589\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.899882\n",
      "Regularization parameter: 0.026 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.900479\n",
      "4.25s\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.896571\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.891115\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.891239\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.901946\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.890737\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.891118\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.897391\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.899507\n",
      "Regularization parameter: 0.026 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.899856\n",
      "4.38s\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.911196\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.907144\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.895032\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.901038\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.912668\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.907274\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.895506\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.911528\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.911059\n",
      "Regularization parameter: 0.026 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.883462\n",
      "4.02s\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.901911\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.909086\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.916363\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.898523\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.911083\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.913573\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.899299\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.907367\n",
      "Regularization parameter: 0.026 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.888196\n",
      "3.88s\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.892184\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.910046\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.900143\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.915718\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.903654\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.896259\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.907235\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.898482\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.906686\n",
      "Regularization parameter: 0.026 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.895831\n",
      "3.95s\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.892090\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.897717\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.897532\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.902730\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.890415\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.891999\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.905304\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.905961\n",
      "Regularization parameter: 0.026 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.897823\n",
      "4.06s\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.914403\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.907207\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.906461\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.903504\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.892670\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.917010\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.903699\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.911761\n",
      "Regularization parameter: 0.026 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.899564\n",
      "4.74s\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.256814\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.671981\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.939103\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.907707\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.897635\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904705\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.909909\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.899510\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.903405\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.902434\n",
      "4.11s\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.899704\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.909841\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.903252\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.894751\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.914945\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.911084\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.912407\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.911957\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.910895\n",
      "4.19s\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.899893\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.904853\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.901493\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.898241\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.907468\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.901941\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.912732\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.913072\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.903277\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.901062\n",
      "3.99s\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.899020\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.911216\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.915654\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.894772\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.909094\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.895009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.027 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.894765\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.900089\n",
      "Regularization parameter: 0.027 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.900638\n",
      "4.07s\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.896782\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.891391\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.891432\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.902133\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.890849\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.891235\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.897659\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.899680\n",
      "Regularization parameter: 0.027 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.900029\n",
      "4.01s\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.911336\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.907381\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.895210\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.901281\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.912770\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.907468\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.895724\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.911681\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.911237\n",
      "Regularization parameter: 0.027 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.883686\n",
      "4.73s\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.902030\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.909178\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.916605\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.898711\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.911092\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.913674\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.899371\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.907591\n",
      "Regularization parameter: 0.027 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.888389\n",
      "4.11s\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.892360\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.910223\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.900272\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.915937\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.903815\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.896399\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.907439\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.898629\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.906841\n",
      "Regularization parameter: 0.027 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.896067\n",
      "3.94s\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.892263\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.897968\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.897635\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.902873\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.890689\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.892092\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.905548\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.906125\n",
      "Regularization parameter: 0.027 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.897959\n",
      "3.76s\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.914681\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.907362\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.906611\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.903677\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.892892\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.917193\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.903901\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.911947\n",
      "Regularization parameter: 0.027 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.899733\n",
      "3.71s\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.344211\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.657225\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.925655\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.907827\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.897996\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.904872\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.910106\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.899776\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.903609\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.902721\n",
      "4.13s\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.899963\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.909978\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.903509\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.894840\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.915165\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.911173\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.912698\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.912116\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.911038\n",
      "4.08s\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.900130\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.905154\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.901628\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.898420\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.907643\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.902221\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.913043\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.913258\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.903557\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.901303\n",
      "4.15s\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.899157\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.911442\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.915864\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.895040\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.909279\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.895265\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.895003\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.900317\n",
      "Regularization parameter: 0.028 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.900930\n",
      "3.99s\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.896922\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.891576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.028 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.891640\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.902395\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.890989\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.891499\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.897784\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.899895\n",
      "Regularization parameter: 0.028 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.900244\n",
      "4.14s\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.911534\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.907485\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.895458\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.901484\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.912922\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.907615\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.895925\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.912015\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.911430\n",
      "Regularization parameter: 0.028 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.883820\n",
      "4.22s\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.902244\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.909494\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.916802\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.898833\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.911346\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.913901\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.899624\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.907798\n",
      "Regularization parameter: 0.028 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.888580\n",
      "4.42s\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.892598\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.910390\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.900586\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.916082\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.903994\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.896625\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.907581\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.898749\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.907144\n",
      "Regularization parameter: 0.028 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.896327\n",
      "4.56s\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.892553\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.898328\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.897878\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.903070\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.890920\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.892337\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.905659\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.906273\n",
      "Regularization parameter: 0.028 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.898133\n",
      "4.29s\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.914805\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.907602\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.906866\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.903944\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.893001\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.917467\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.904050\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.912132\n",
      "Regularization parameter: 0.028 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.899951\n",
      "4.45s\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.431607\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.641229\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.916528\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.907889\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.898067\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.905030\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.910256\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.899972\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.903808\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.902842\n",
      "3.88s\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.900099\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.910234\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.903580\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.895019\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.915402\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.911307\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.912797\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.912400\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.911308\n",
      "4.14s\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.900265\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.905299\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.901901\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.898601\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.907823\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.902405\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.913218\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.913464\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.903675\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.901503\n",
      "3.89s\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.899433\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.911599\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.916031\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.895219\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.909436\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.895304\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.895283\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.900448\n",
      "Regularization parameter: 0.029 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.901102\n",
      "4.05s\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.897168\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.891714\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.891719\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.902513\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.891272\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.891665\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.897998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.029 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.900012\n",
      "Regularization parameter: 0.029 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.900480\n",
      "4.12s\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.911823\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.907749\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.895455\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.901714\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.913194\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.907859\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.896065\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.912123\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.911620\n",
      "Regularization parameter: 0.029 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.884140\n",
      "3.93s\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.902475\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.909611\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.916911\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.899138\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.911525\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.914049\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.899768\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.908014\n",
      "Regularization parameter: 0.029 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.888688\n",
      "4.29s\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.892769\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.910650\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.900662\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.916346\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.904198\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.896704\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.907814\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.898956\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.907315\n",
      "Regularization parameter: 0.029 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.896443\n",
      "4.09s\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.892624\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.898388\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.898041\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.903310\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.891037\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.892554\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.905844\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.906571\n",
      "Regularization parameter: 0.029 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.898353\n",
      "4.23s\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.914981\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.907931\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.907023\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.904082\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.893254\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.917535\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.904235\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.912382\n",
      "Regularization parameter: 0.029 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.900164\n",
      "4.10s\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.519004\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.623774\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.910454\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.907958\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.898445\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.905339\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.910574\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.900053\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.904064\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.903197\n",
      "4.15s\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.900427\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.910501\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.903959\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.895324\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.915651\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.911630\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.913090\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.912621\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.911500\n",
      "4.07s\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.900567\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.905586\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.902062\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.898778\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.908098\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.902717\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.913441\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.913705\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.903903\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.901684\n",
      "4.41s\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [5504/60000 (9%)]\tLoss: 0.899673\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [11904/60000 (20%)]\tLoss: 0.911812\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [18304/60000 (30%)]\tLoss: 0.916242\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [24704/60000 (41%)]\tLoss: 0.895386\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [31104/60000 (52%)]\tLoss: 0.909743\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [37504/60000 (62%)]\tLoss: 0.895613\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [43904/60000 (73%)]\tLoss: 0.895322\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [50304/60000 (84%)]\tLoss: 0.900695\n",
      "Regularization parameter: 0.030 Train Epoch: 4 [56704/60000 (94%)]\tLoss: 0.901346\n",
      "4.03s\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [3072/60000 (5%)]\tLoss: 0.897335\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [9472/60000 (16%)]\tLoss: 0.892036\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [15872/60000 (26%)]\tLoss: 0.891971\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [22272/60000 (37%)]\tLoss: 0.902724\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.891401\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [35072/60000 (58%)]\tLoss: 0.891896\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [41472/60000 (69%)]\tLoss: 0.898214\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [47872/60000 (80%)]\tLoss: 0.900246\n",
      "Regularization parameter: 0.030 Train Epoch: 5 [54272/60000 (90%)]\tLoss: 0.900566\n",
      "4.59s\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.911916\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.907911\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.895639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.030 Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.901923\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.913432\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.908096\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.896335\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.912400\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.911879\n",
      "Regularization parameter: 0.030 Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.884295\n",
      "4.33s\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [4608/60000 (8%)]\tLoss: 0.902697\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [11008/60000 (18%)]\tLoss: 0.909858\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [17408/60000 (29%)]\tLoss: 0.917124\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [23808/60000 (40%)]\tLoss: 0.899372\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.911822\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [36608/60000 (61%)]\tLoss: 0.914409\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [43008/60000 (72%)]\tLoss: 0.899975\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [49408/60000 (82%)]\tLoss: 0.908215\n",
      "Regularization parameter: 0.030 Train Epoch: 7 [55808/60000 (93%)]\tLoss: 0.889009\n",
      "4.02s\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [2176/60000 (4%)]\tLoss: 0.893017\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [8576/60000 (14%)]\tLoss: 0.910968\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [14976/60000 (25%)]\tLoss: 0.900981\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [21376/60000 (36%)]\tLoss: 0.916508\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [27776/60000 (46%)]\tLoss: 0.904385\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [34176/60000 (57%)]\tLoss: 0.897066\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [40576/60000 (68%)]\tLoss: 0.908003\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [46976/60000 (78%)]\tLoss: 0.899278\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [53376/60000 (89%)]\tLoss: 0.907563\n",
      "Regularization parameter: 0.030 Train Epoch: 8 [59776/60000 (100%)]\tLoss: 0.896728\n",
      "4.37s\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [6144/60000 (10%)]\tLoss: 0.892807\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [12544/60000 (21%)]\tLoss: 0.898683\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [18944/60000 (32%)]\tLoss: 0.898239\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [25344/60000 (42%)]\tLoss: 0.903470\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [31744/60000 (53%)]\tLoss: 0.891284\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [38144/60000 (64%)]\tLoss: 0.892751\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [44544/60000 (74%)]\tLoss: 0.906148\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [50944/60000 (85%)]\tLoss: 0.906778\n",
      "Regularization parameter: 0.030 Train Epoch: 9 [57344/60000 (96%)]\tLoss: 0.898540\n",
      "4.58s\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [3712/60000 (6%)]\tLoss: 0.915195\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [10112/60000 (17%)]\tLoss: 0.908013\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [16512/60000 (28%)]\tLoss: 0.907307\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [22912/60000 (38%)]\tLoss: 0.904304\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [29312/60000 (49%)]\tLoss: 0.893505\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [35712/60000 (59%)]\tLoss: 0.917895\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [42112/60000 (70%)]\tLoss: 0.904467\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [48512/60000 (81%)]\tLoss: 0.912567\n",
      "Regularization parameter: 0.030 Train Epoch: 10 [54912/60000 (91%)]\tLoss: 0.900444\n",
      "4.38s\n",
      "\n",
      "Test set: Average loss: 0.8985, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_param in reg_params1:\n",
    "    model = NetSeq().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    model.train() #training mode\n",
    "    iteration = 0\n",
    "    a_list = [] #tracking the loss function value\n",
    "    for ep in range(epoch1):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            #compute the L1 norm of weight matrix on the last layer \n",
    "            l1_norm = torch.norm(model.fc_layers[-1].weight, p=1)\n",
    "            \n",
    "            #compute loss\n",
    "            loss = nn.MultiMarginLoss(p=2)(output, target)  + reg_param*l1_norm\n",
    "            a_list.append(loss.item())\n",
    "            \n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Regularization parameter: {:.3f} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    reg_param, ep+1, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "    \n",
    "    result_train = result_hinge(trainset_loader)\n",
    "    result_test = result_hinge(testset_loader)\n",
    "    train_ave_loss_L1_1.append(result_train[0])\n",
    "    test_ave_loss_L1_1.append(result_test[0])\n",
    "    train_acc_L1_1.append(result_train[1])\n",
    "    test_acc_L1_1.append(result_test[1])\n",
    "    matrix_norm_L1_1.append(np.linalg.norm(model.fc_layers[-1].weight.cpu().detach().numpy()))\n",
    "    listoflist_L1.append(list(a_list))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        result_test[0], result_test[2], result_test[3],\n",
    "        result_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average loss versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average Training and Test Loss (Squared Hinge Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params1, train_ave_loss_L1_1, label=\"Training\")\n",
    "plt.plot(reg_params1, test_ave_loss_L1_1, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy rate versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy Rate (Squared Hinge Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params1, train_acc_L1_1, label=\"Training\")\n",
    "plt.plot(reg_params1, test_acc_L1_1, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"100 * Accuracy rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squared hinge loss with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of regularization parameters from 0 to 0.03 with step size 0.001\n",
    "reg_params = np.linspace(0, 0.03, num=31, endpoint=True)\n",
    "\n",
    "#initiate a list storing training average loss for different reg_param\n",
    "train_ave_loss_L2 = []\n",
    "\n",
    "#initiate a list storing test average loss for different reg_param\n",
    "test_ave_loss_L2 = []\n",
    "\n",
    "#initiate a list storing training accuracy for different reg_param\n",
    "train_acc_L2 = []\n",
    "\n",
    "#initiate a list storing test accuracy for different reg_param\n",
    "test_acc_L2 = []\n",
    "\n",
    "#initiate a list storing matrix norm for different reg_param\n",
    "matrix_norm_L2 = []\n",
    "\n",
    "#initiate a list of lists tracking the training loss for different reg_param\n",
    "listoflist_L2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.000 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.897108\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.868668\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.741624\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.282473\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.144298\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.087250\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.105387\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.044324\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.049693\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.078139\n",
      "4.46s\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.055671\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.032661\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.024736\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.059849\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.070747\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.034400\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.082478\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.033394\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.039403\n",
      "4.09s\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.048872\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.033079\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.077784\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.063687\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.060368\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.042814\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.030498\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.028419\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.041494\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.031717\n",
      "4.30s\n",
      "\n",
      "Test set: Average loss: 0.0351, Accuracy: 9361/10000 (94%)\n",
      "\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.900471\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.872099\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.745971\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.288251\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.149056\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092132\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.110409\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.049062\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.054563\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.083146\n",
      "4.09s\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.060886\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.037691\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.029822\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.065337\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.075780\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.039757\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.087276\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.038345\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.044674\n",
      "3.90s\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.054270\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.038501\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.082896\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.069170\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.065598\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.048117\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.035908\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.033651\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.046912\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.037069\n",
      "3.95s\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9357/10000 (94%)\n",
      "\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.903834\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.875503\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.750293\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.293907\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.153882\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.096954\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114765\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.053826\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.059201\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.087859\n",
      "3.99s\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.065641\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.042305\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.034600\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.070109\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.080465\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.044501\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.091683\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.042999\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.049065\n",
      "4.19s\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.058956\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.043187\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.087536\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.073523\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.070479\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.052650\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.040491\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.038163\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.051383\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.041440\n",
      "3.95s\n",
      "\n",
      "Test set: Average loss: 0.0358, Accuracy: 9350/10000 (94%)\n",
      "\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.907197\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.878898\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.754729\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.299783\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.158439\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.101580\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.119199\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.058126\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.063582\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092286\n",
      "4.50s\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.070166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.003 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.046646\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.039045\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.074554\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.084918\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.048779\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.095747\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.046943\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.053055\n",
      "4.15s\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.063026\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.047385\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.091315\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.077673\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.074468\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.056721\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.044455\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.042113\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.055097\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.044995\n",
      "4.31s\n",
      "\n",
      "Test set: Average loss: 0.0362, Accuracy: 9349/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.910560\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.882262\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.759055\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.305593\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.162993\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.106006\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.123507\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.062531\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.067779\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.096547\n",
      "4.20s\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.074422\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.050637\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.043143\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.078838\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.089062\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.052785\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.099230\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.050678\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.056578\n",
      "4.11s\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.066649\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.050968\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.094578\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.081051\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.078117\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.060022\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.047901\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.045337\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.058206\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.048022\n",
      "4.12s\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9343/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.913923\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.885603\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.763346\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.311284\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.167446\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.110356\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.127712\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.066532\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.071740\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.100408\n",
      "4.14s\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.078469\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.054376\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.046848\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.082781\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.092615\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.056424\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.102392\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.053912\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.059889\n",
      "4.18s\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.069827\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.054194\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.097305\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.084060\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.081055\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.063226\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.050850\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.048134\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.060798\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.050512\n",
      "4.20s\n",
      "\n",
      "Test set: Average loss: 0.0368, Accuracy: 9339/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.917286\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.888915\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.767380\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.316652\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.171657\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.114620\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.131692\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.070418\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.075531\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.104226\n",
      "4.01s\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.082221\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.057855\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.050312\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.086307\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.096200\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.059667\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.105263\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.056813\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.062662\n",
      "4.14s\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.072613\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.056950\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.099740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.006 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.086485\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.083636\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.065539\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.053269\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.050516\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.062960\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.052550\n",
      "4.27s\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9336/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.920650\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.892203\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.771560\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.322195\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.175918\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.118758\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.135470\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.074198\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.079168\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.107561\n",
      "3.74s\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.085911\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.061115\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.053587\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089678\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.099650\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.062625\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.107806\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.059343\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.065099\n",
      "4.11s\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.075131\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.059574\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.101785\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.088453\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.085651\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.067641\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.055325\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.052372\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.064827\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.054143\n",
      "4.27s\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9333/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.924013\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.895468\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.775541\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.327660\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.180021\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.122779\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.139183\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.077585\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.082613\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.110887\n",
      "4.88s\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089212\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.064088\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.056571\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.092592\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.102279\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.065270\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.110129\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.061511\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.067217\n",
      "4.11s\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.077305\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.061685\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.103530\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090307\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.087936\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.069473\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.057077\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.053985\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.066272\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.055466\n",
      "4.27s\n",
      "\n",
      "Test set: Average loss: 0.0378, Accuracy: 9326/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.927376\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.898732\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.779583\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.333161\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.183891\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.126498\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.142830\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.080920\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.085821\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.113909\n",
      "4.21s\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092330\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.066846\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.059317\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.095351\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.104790\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.067713\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.112022\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.063542\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.069117\n",
      "4.13s\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.079208\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.063518\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.104964\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.091843\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.089526\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.070886\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.058617\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.055299\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.067573\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.056607\n",
      "4.36s\n",
      "\n",
      "Test set: Average loss: 0.0382, Accuracy: 9322/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.930739\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.901943\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.783471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.010 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.338458\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.187806\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.130284\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.146154\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.084352\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.088929\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.116745\n",
      "3.79s\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.095195\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.069391\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.061785\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.097848\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.107117\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.069837\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.113755\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.065353\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.070729\n",
      "4.25s\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.081006\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.065085\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.106115\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.093201\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090838\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.072202\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.059821\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.056367\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.068645\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.057379\n",
      "4.15s\n",
      "\n",
      "Test set: Average loss: 0.0385, Accuracy: 9320/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.934102\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.905139\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.787329\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.343660\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.191792\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.133985\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.149367\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.087518\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091710\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.119579\n",
      "4.59s\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.097939\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.071771\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.064169\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.100225\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.109056\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.071800\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.115276\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.066955\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.072236\n",
      "4.23s\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.082437\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.066523\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.107288\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.094197\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.092155\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.073346\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.060911\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.057345\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.069597\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.058044\n",
      "4.15s\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9315/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.937465\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.908317\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.791150\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.349039\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.195493\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.137554\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.152573\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090626\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.094450\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.122148\n",
      "4.06s\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.100448\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.073949\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.066332\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.102307\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.110778\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.073525\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.116792\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.068356\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.073484\n",
      "4.11s\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.083572\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.067575\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.108180\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.095085\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.093011\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.074300\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.061699\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.058062\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.070268\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.058576\n",
      "4.26s\n",
      "\n",
      "Test set: Average loss: 0.0392, Accuracy: 9307/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.940829\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.911473\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.794865\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.354186\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.199194\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.141020\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.155596\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.093243\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.097024\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.124427\n",
      "4.21s\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.102829\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.075986\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.068306\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.104254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.013 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.112406\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.075103\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.117877\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.069389\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.074604\n",
      "4.26s\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.084665\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.068683\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.108829\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.095837\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.093812\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.074977\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.062363\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.058697\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.070882\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059077\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.0395, Accuracy: 9305/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.944192\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.914603\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.798640\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.359394\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.202696\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.144140\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.158568\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.095952\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.099682\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.126767\n",
      "4.19s\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.105095\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.078001\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.070072\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.106100\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.114057\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.076553\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.119059\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.070499\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.075604\n",
      "3.87s\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.085589\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.069582\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.109513\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.096493\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.094495\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.075587\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.063032\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.059212\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.071514\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059389\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9300/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.947555\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.917694\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.802292\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.364470\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.206246\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.147342\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.161326\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.098619\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.102009\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.128870\n",
      "4.12s\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.107176\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.079771\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.071710\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.107794\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.115389\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.077707\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.119858\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.071339\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.076361\n",
      "4.13s\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.086444\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.070402\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.109989\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.097010\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.095000\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.076148\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.063531\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.059666\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.071935\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059644\n",
      "3.93s\n",
      "\n",
      "Test set: Average loss: 0.0402, Accuracy: 9298/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.950918\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.920787\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.805973\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.369704\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.209688\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.150585\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.164080\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.101140\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.104293\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.130844\n",
      "4.11s\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.109154\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.081317\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.073357\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.109265\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.116574\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.078821\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.120794\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.072207\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.077090\n",
      "4.49s\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.087119\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.071160\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.110322\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.097440\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.095297\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.076825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.016 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.063860\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.059989\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.072150\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059781\n",
      "4.01s\n",
      "\n",
      "Test set: Average loss: 0.0405, Accuracy: 9294/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.954281\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.923838\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.809666\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.374998\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.213073\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.153673\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.166717\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.103373\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.106376\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.132691\n",
      "4.08s\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.110998\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.082785\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.074683\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.110594\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.117776\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.079770\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.121435\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.072867\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.077603\n",
      "4.24s\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.087677\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.071716\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.110746\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.097778\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.095835\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.077206\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.064179\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.060274\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.072376\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059888\n",
      "4.08s\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 9293/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.957644\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.926874\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.813118\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.379878\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.216362\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.156564\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.169165\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.105784\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.108182\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.134479\n",
      "4.26s\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.112655\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.084083\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.075986\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.111973\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.118698\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.080758\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.122170\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.073317\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.078152\n",
      "4.19s\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.088188\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.072301\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.110904\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.098076\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.096111\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.077619\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.064420\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.060527\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.072646\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059940\n",
      "4.55s\n",
      "\n",
      "Test set: Average loss: 0.0411, Accuracy: 9290/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.961007\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.929881\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.816622\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.384901\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.219680\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.159453\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.171584\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.107784\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.110141\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.136170\n",
      "3.97s\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.114251\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.085399\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.077144\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.113062\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.119643\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.081555\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.122725\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.073834\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.078552\n",
      "4.17s\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.088645\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.072786\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111138\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.098308\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.096422\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.077884\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.064604\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.060831\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.072820\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059997\n",
      "4.20s\n",
      "\n",
      "Test set: Average loss: 0.0415, Accuracy: 9285/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.964370\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.932873\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.819950\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.389689\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.222840\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.020 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.173896\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.109832\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.111814\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.137777\n",
      "4.21s\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.115722\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.086510\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.078220\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.114001\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.120535\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.082206\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.123209\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.074313\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.078867\n",
      "4.14s\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.088997\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.073072\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111340\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.098517\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.096774\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.078130\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.064859\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.061006\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.072926\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060025\n",
      "4.27s\n",
      "\n",
      "Test set: Average loss: 0.0418, Accuracy: 9282/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.967734\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.935837\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.823303\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.394572\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.225904\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.164849\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.176081\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.111825\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.113453\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.139111\n",
      "4.07s\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.117180\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.087609\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.079230\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.115018\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.121254\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.082902\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.123587\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.074578\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.079185\n",
      "4.27s\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089318\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.073423\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111414\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.098767\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097076\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.078435\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.064996\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.061262\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073018\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060090\n",
      "4.08s\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9280/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.971097\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.938783\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.826647\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.399422\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.228955\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.167580\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.178193\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.113627\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.115018\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.140495\n",
      "4.48s\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.118490\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.088603\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.080144\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.115844\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.121934\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.083374\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.123980\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.074819\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.079376\n",
      "4.36s\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089597\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.073764\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111406\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.098905\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097258\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.078583\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.065116\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.061482\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073049\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060047\n",
      "4.47s\n",
      "\n",
      "Test set: Average loss: 0.0424, Accuracy: 9275/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.974460\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.941705\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.829883\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.404161\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.231913\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.170091\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.180327\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.115427\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.116387\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.141969\n",
      "4.02s\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.119602\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.089494\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.080982\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.116835\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.122806\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.083929\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.124450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.023 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075060\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.079601\n",
      "3.89s\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089728\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.074060\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111330\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099069\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097280\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.078870\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.065311\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.061578\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073135\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060051\n",
      "4.08s\n",
      "\n",
      "Test set: Average loss: 0.0426, Accuracy: 9274/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.977823\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.944598\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.833088\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.408932\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.234750\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.172612\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.182297\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.117091\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.117799\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.142936\n",
      "4.08s\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.120855\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090294\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.081702\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.117294\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.123091\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.084340\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.124578\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075193\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.079808\n",
      "4.05s\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089981\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.074356\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111315\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099169\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097398\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.079082\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.065458\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.061780\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073144\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060051\n",
      "4.20s\n",
      "\n",
      "Test set: Average loss: 0.0429, Accuracy: 9275/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.981186\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.947490\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.836233\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.413515\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.237660\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175023\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.184095\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.118640\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.119199\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.144367\n",
      "4.32s\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.121649\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090934\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.082337\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.118258\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.124217\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.084745\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.124945\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075260\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.079921\n",
      "4.22s\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090104\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.074499\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111418\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099314\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097509\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.079288\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.065748\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.061899\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073137\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060109\n",
      "4.04s\n",
      "\n",
      "Test set: Average loss: 0.0432, Accuracy: 9276/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.984549\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.950351\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.839424\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.418379\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.240392\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.177367\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.185846\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.120188\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.120451\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.145425\n",
      "4.34s\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.122652\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091586\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.082893\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.118802\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.124752\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.085030\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.125187\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075238\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.080017\n",
      "3.83s\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090281\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.074683\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111414\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099529\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097640\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.079500\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.065842\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.062182\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.026 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060085\n",
      "4.12s\n",
      "\n",
      "Test set: Average loss: 0.0434, Accuracy: 9274/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.987913\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.953189\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.842530\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.423038\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.243093\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.179533\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.187379\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.121577\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.121584\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.146161\n",
      "4.25s\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.123714\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092222\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.083428\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.118948\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.124670\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.085216\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.125344\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075311\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.080097\n",
      "3.91s\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090410\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.074827\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111326\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099538\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097763\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.079667\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.065938\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.062314\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073076\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.060053\n",
      "4.25s\n",
      "\n",
      "Test set: Average loss: 0.0436, Accuracy: 9272/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.991276\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.955987\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.845573\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.427819\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.245744\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.181760\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.189085\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.122967\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.122663\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.147203\n",
      "4.58s\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.124498\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092719\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.083978\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.119855\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.125364\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.085536\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.125418\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075331\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.080110\n",
      "3.97s\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090545\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.074969\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111330\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099644\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097723\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.079848\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.066114\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.062554\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073136\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059998\n",
      "3.96s\n",
      "\n",
      "Test set: Average loss: 0.0438, Accuracy: 9273/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.994639\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.958784\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.848607\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.432501\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.248515\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.183910\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.190749\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.124313\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.123681\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.147920\n",
      "4.20s\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.125360\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.093170\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.084461\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.120287\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.125641\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.085799\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.125589\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075356\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.080136\n",
      "4.11s\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090720\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.075121\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111134\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099748\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097820\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.079992\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.066223\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.062740\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073093\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059977\n",
      "3.83s\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 9272/10000 (93%)\n",
      "\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.998002\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.961546\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.851566\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.437029\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.251094\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.185986\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.192307\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.125556\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.124622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.030 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148547\n",
      "4.00s\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.125912\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.093535\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.084870\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.120697\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.125835\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.085994\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.125576\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.075429\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.080215\n",
      "4.09s\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090765\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.075239\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111055\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099762\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097844\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.080271\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.066242\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.062890\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.073157\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.059909\n",
      "3.94s\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9269/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_param in reg_params:\n",
    "    model = NetSeq().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    model.train() #training mode\n",
    "    iteration = 0\n",
    "    a_list = [] #tracking the loss function value\n",
    "    for ep in range(epoch):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            #compute the L2 norm of weight matrix on the last layer \n",
    "            l2_norm = torch.norm(model.fc_layers[-1].weight)**2\n",
    "            \n",
    "            \n",
    "            #compute loss\n",
    "            loss = nn.MultiMarginLoss(p=2)(output, target)  + reg_param*l2_norm\n",
    "            a_list.append(loss.item())\n",
    "            \n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Regularization parameter: {:.3f} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    reg_param, ep+1, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "    \n",
    "    result_train = result_hinge(trainset_loader)\n",
    "    result_test = result_hinge(testset_loader)\n",
    "    train_ave_loss_L2.append(result_train[0])\n",
    "    test_ave_loss_L2.append(result_test[0])\n",
    "    train_acc_L2.append(result_train[1])\n",
    "    test_acc_L2.append(result_test[1])\n",
    "    matrix_norm_L2.append(np.linalg.norm(model.fc_layers[-1].weight.cpu().detach().numpy()))\n",
    "    listoflist_L2.append(list(a_list))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        result_test[0], result_test[2], result_test[3],\n",
    "        result_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAFNCAYAAABST1gVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXhV1bn48e+bGUggEBICmZlHmQIICoiiIFVxwHmq4qXeamsHvVp/3hZte6/2eq+16r3WOlQtjjihdWxVQOZZ5jkjQ0KAQAIhOcn7+2PtyCEmECCHk+H9PA8P5+y9z97vHrLPe9baay1RVYwxxhhjTOMWEuwAjDHGGGPMiVnSZowxxhjTBFjSZowxxhjTBFjSZowxxhjTBFjSZowxxhjTBFjSZowxxhjTBFjSZpoVEdkoIqMbetlgEZEwEVERSQ92LI2FiGSKyNxgx3G6RCTaO7eJp7GOOSJyVUPG1RyIyO9F5I/HmX+3iHx6JmMKNhHZIyJnn+Jn24hIiYgkNHBMfURkTwOsJ0REVolI14aIqzGzpK2JEZGvRWSfiEQGO5bTJSJrvRtBiYhUikiZ3/sHT2WdqtpLVev1hX4yyzZGXtJZ1/H7t9NY70wReeA480872ThNvwce84vnAhFZJCLFIrLXS2QGBCm2BiMiS0XkuhrTLhGRDdXvVXWMqr4ThNhOOQE4E1T1/6nqzwBEpL+IlJ3O+uraXxEZKyJfeffkAhGZISIdj7OepX5/pwUi8ubxlm8sVLVUVaNVteB01lPzOKrqelU97f1X1SrgSeA3p7uuxs6StibEK20ZDShwWYC2ERaI9dZGVft5N4JoYC5wd/V7Vf2PYMbWFHhJZ/XxWwDc6Xf8/hDs+ALB+yU9GPjUe58AvAv8J9AeSMEldBVnOC4RkdAzuU3TKMTikoUUoCsQCjx7gs/80Pub7QN0wf0IabSa0H13JjBZRNoHO5BAsqStabkFWAj8Fbi1eqKInC0iu/y/NETkChH51nsdIiIPiMhWESkSkbdEpIM3L90rNZkqIjnAl970t711FnslF/381h0nIh+KyAERWSIivxORb/zm9xaRL7xSj40ics2p7KyI3OFt+08ishd4SER6eL9si7xfba+KSDu/z+SJyHne69+JyOsi8jcROSgia0RkyCkumykiK715b3jHZ3odcdcnxl+IyGrv+L4ufiWn3rnaJSL5+J3nUzyGP/bOwV4R+UhEunjTQ0Xk/0Sk0IthpYh0F5FfAJcDD3ulAa+f5PZCvWOZ6+3D8yIS7c2L9q69vV7JxMLq4yIid4pItnd8t4rIlXVsYiIwX1V93vu+wEFVfV9Vq7wSgb+r6gZvveEi8rS3zS0i8lPxK3WRGr/8ReRxEXnWex0hIu+KyG4R2S8i/xSRHn7LzhSRP4rIP4BSYJiItBaRp7xzvFNEnhSRCL/P/FpcCUsucP3JHNs6jvd3pXHiqvw+F5FnvHO6RUTG+S3bS0QWeMf4Y+/cPOs3f6yILPb2dZmIjDzFmH4qItu8YztTvCq1uq45b96V3nV60Lt27qplveJ9tpf3/k5x96407/3PRORv3uvvziMwB4iUo6XQ1aWwIXUdq/pS1Q+8a69EVUuA/wXOqedni4APgUF++xgmItNFZLscvXe09Zv/I+/aKhCRX/pfv1KjhFxqlMr6E5Ex3rkuFpEdIvLf4n1/yNGS9DtFZCuwym9aooj09DuWJSJyWERKvM/2FXfP3uvF+JIc/ft/D4gDvvQ+92OpUQoqImki8om4+8NGEbnJb97jIvKKuNLJg+KqQ78rUVfVA8B64Pz6HP+mypK2puUWYIb3b4KIdAJQ1YW4Lw3/i/UG4DXv9U9xX8Rjcb/s9gHP1Fj3WNwvvwne+0+AHkACsNzbZrVnvO0l4pIK/wSyDfCFt+0E3BfT/4pf0neSRuH+EONxJSgC/A7ojPvC7gr8+3E+fznwKu4X8SfAn052WXEJ1fvA80AH4B1v2brUJ8ZrgAu9eUOBm71tXQLcgzuXPTl6Pk6ad8O7C/gB0AlYDbzst6/9gW64EqqbgWJV/R9vX3/jldidbGJxF3AF7ourF5AE/Jc370e4UuIuuPP5U6BcROKB/wDGqWoMMAZYV8f6BwAb/d6vBdqKyHMicpH4Jceee4BzgX64a+lk9kdwpXjdcOdyK/BSjWVuAn4FRAMrcNdMgre9Pt7/9wGIyBTgX7x4+uLOS0MbB8zDXafPAn/xti3A28DnuC/O/8bdI/Dmd8Pt6wPeZx8GPqjleB6XiFyGOx6X4UqfijnBNefF9iJwg3f+B3v7cAx1Yy7OBc7zJo0Btnn/V7+fXUtYY4AjfqXQq73ptR6r0zQGd02ekJfMTga2+E1+ALd/I3HHD9y5QkQycffAKzhashd7inGWAz/G7ftYb5231VhmEjAEyPSfqKqb/Er4Y4DPAP8fd7/G3W8G4q7/B7zPXQEUAed7n/9f//V618E7uOOXiLs+npJjq6WvAv6M2+85wBM1Yl7vbbf5UlX71wT+4W70FUBH7/0G4Od+838HvOi9jsElVWne+/XABX7LdvbWFQak475Iux5n27HeMu1wxf8VQK8a2/7Ge30tMLfG5/+MSwKOt39fA3fUmHYHsO0En5sCLPF7nwec5xfXp37zzgJKTnZZXAKVU2O7C4Hp9Tx3tcV4nd/7/wGe9l6/AvzOb15f79inn2Ab3+CqXfynzQWu9XsfCVTivrQvwyVxwwCp8bmZwAPH2Va0F1NiLfMWAbf4vR+KKwkDl6R9CfSt8Zk43A+JS4HIE+zn68BDNaadhUu2d3rX5ttAB2/eYuAmv2WvBMr83u8BzvZ7/zjwbB3bTvaOX4Tfcfpfv/kR3vYT/aZdCKz2Xr/lHzvuC7HW4+jNX4r7O97v968E2FBjmeu813cDK/3mJXjrj/auo5Lq2L3571fvK/Bb4P9qbH8ecFUdsR1z3Pymvwn8usa5VaBjXdccLjneg/tRGn2C838P8Jr3OhuYhvsxJUAh0LPmecQlimU11lPnsTqZ/a2xzHDvHGUeZ5nqc3rA296iGtdLLjDC730Pjv79/AH4i9+89t46zq7t7xa4pMa1Uuc+AA8Br9b4+x5+or9577qZ539d1Zh/E37fBzVj8D83uB85h4Aov/lPcfTe+Djwfo3jvafG9p4E/nS889TU/1lJW9NxK/C5qla3tHmNY6vOXgOu9EqFrgSWq2q2Ny8NeM+r9tiPS+Iqcb+GquVWv/CqMR4VV011AMjyZnXElZCE+S9f43UaMKJ6W972bsT9cjoV/uvGK55/S0Tyvdj+6sVVl11+rw8BbU5h2S64RKvOuE4hxprbivbblv+6szl1acDzfudhN+4XdjKuWuZlXOnCLnFViK1PY1vVunBszNlAtFdi8xwwH3ct5oqrRg1RV010K/BzYLeIfOCV/NRmH+5HyXdU9VtVvVlVO+NKaXpztKHCKR9PcVWrT3hVVQeANbjaCf9nZvzXnYz729jgd8xn4hKCU41lqqrGVv8DrjvB8jWvK3DXVhegQFXL64g9Dfhhjb/bQd7nTsYx5987t6W4Etdarzl137aTcT/4csVVQw/5/qoBV5I2VtyzjXtxiecYXImOT1U3nUSsdR2rk+bVJMzCna+lJ1h8qqq2xZVgJeF+RONVTyYBn/udgyVAuIjEUuP6UdV9uET8VOLtLyKfiqv6PwA8yPfvUXXe47x1TMH93V5ZfV2JSLJXTbvDW++ztay3Ll2A3arq32gkG3dMqtV136wWg0ucmy1L2poAEWmFq04bK+45oV24L7iBIjIQQFXX4S7wizm2ahTcH9/F/jd/VY1S1Xy/ZdTv9Q24m+h4XOlaenUouF+zPtwXVLUUv9e5wOwa24pW1X89xd3XGu8fA44AA7wb3w+9uAJpJ8fuLxy7zzWdTow7a6w7tZ6fq00urpTJ/1y0UtVV6jyuqoNwX86ZwE+8z9U85idjBy4BqJaKK7EsVtUyVX1IVXvhqqZuAK4GUNVZqno+7ga9A3i6jvV/i6s2rpWqrsFV5ff3Jp3oeJYC/smq/4+LO3BVVWO881i9Tv9z6X+sduB+DKX7He92qlr946ghz+3J2gkkiEi437Saf7fP1rhW2qjqUye5nWPOv7hnZ9sA+ce75lR1nqpWV+N/ybGPY/j7Fne+7sDdZwpw96MbqL1qFE7vej4hEemJq3b+Nz2JlryqugxX9fmU974Sd57G1HKv3k+N+5CXyPknLce7lmt6AVcK3dW7tv+D79+j6jxuInIW8H/AFaq622/Wf+OqxPt6672Tuv9eatoBdJJje0ZIBfLrWL42fYBVJ7F8k2NJW9NwOe7LoC/uZjcId3HOxVUpVHsNVwU1BldFVO1Z4Pdy9IHdeBGZfJztxeCSjiLcTeC7lpzejeVdYLq4h65714jhI6CniNzslVSEi8gwEelzCvtdV2yluGdhUoB7G2i9x/MNECoi/yruQeGrcNV+gYjxLeB2cY052nB6TdifBX7tfakgIu3Fe8BfREaKyFBxLcNKcCVwld7nduOelzmRSBGJ8vsXgqu+vM/7xd0WV30yw9vmheL6ZQrBVQ/5gEoRSRGRSd6PkzLcsausfZN8Coz04kZEBol78L26gUUGLhFc6C3/FvBLr/Qznu+fi5XA9d55HYWroq0W48WzT0RicFXodfJKCF4GnhTXWEdEJFVExvvFMk1cQ5Vojv8sZkNbB2wHHvT+Jsfhqm6r/RV3HMaJa7jUSkTGi/fcbB0iapz/UNz5/5GI9PPO52PAZ6q6p65rTkRiROQa7xhXePNqPf/qunaYi6verE7SZtd4X1MB7lpNqmN+fX1vf73r7Z+4RxpeOYV1Pgf0EZELvPfPAo9VxyoincQ95wru+rnGO4aRuL8t/+O0ErhURNqJSDLumNQlBtivqqXiHua/o74Bi0gc8AFwj5d41lzvQeCAuN4Ofl5j/vHuLRu8f78V1wgoE1e9WlcCXzOuGNz34lf1Wb6psqStabgVeElVc1R1V/U/XGnEjXK0SfbruJKBL/2qUcHV88/CFbsfxH2hjTjO9l7Bldrl4272C2vMvxtXArcL9yzR67gkD1U9CFyEq8bZ4S3zGO55qobwG9yzDMW4fQp4H1WqegT3oO6duOq5a4CP8fa5IWNU1Q9xDT1mA5twjTpONe5XcV8C73tVFSuB6i+HDrjzvB/3MPdWjjZOeRYYJa4F1/FumFnAYb9/1+CuyQ9xz+psxt2k7/OWT/HmHcT9Gv4A9wMgDPh/3rJ7cD9K7qljn7JwD/xXN9Aoxl3zy0SkFPdw8nxvfeCu/QW463gB7pkrfw/gEvD9Xpxv+M17zpu+y4u3rqTA309wpdHLvNg+5uiX1ExcQ4Z5uEcUPqnH+hqEVwV5Da4kfh9uX2dy9O92My7Z/R3ux1oWbl+OV0I8m2PP/32q+j7u2aOPcI8UxHH0MY7jXXPTcKV9+3H3jpoPxdfcbnU3QdXvY3DnvrZ9L8SVAK3yqh1PtQ+/7+0v7mH+JOC/5Ghryl3HWUfN2EpxLU5/7U36T28/Znt/s9/gqvxR1SW4Rh4f4I5tFu4HTvV96C+4+3Yu7r7jX9tS08+AfxXX6vNJjr3uT2QkrvbluVr2+SFcKfoB3PX1do3P/g74g3dvOab2xbtGr8I1JNjtxf9zVV1Qz7imALNUde9J7EuTI+44GXPqROQx3AOqt55w4WZCRJYBf/QSI3MGichQ4AlVHXPChb//2f7AUlWNavjImhYR+Tvwtar+1wkXNo2OuE55C4EELzFtsbzS++W4hjNbgx1PIFlJmzlpXtXdWV71z3BgKvBesOMKJBE5z6uqCBORqbiH3T8PdlwtkaouO5WEraXzqidTvWq9y3HPrM4Kdlym/kRksld1HYNrdT6vpSds4KrNVXVQc0/YwFVLGHOyYnBVol1wz4v8N67Ivjnrg6taa4Or1rmqxgO4xjR2Kbgqq1ggB7hVVTce/yOmkbkWV8WsuMdWbg5uOOZMs+pRY4wxxpgmwKpHjTHGGGOaAEvajDHGGGOagIA+0yYiE3HNiUOB51X10RrzI3H180Nxzcyv9Zr0V89PxTXVn66qj3vTfo7rU0ZxQ6LcVqMH5e/p2LGjpqenN9BeGWOMMcYEzrJly/aoanzN6QFL2ryOFp/BdeCYBywRkVlez/3VpgL7VLW7iFyH68/rWr/5T+DXl5HX4eBPcb0tHxaRt3B9+vz1eLGkp6ezdOmJRhYxxhhjjAk+Eal1mLtAVo8OB7ao6jZvXLI3cEMj+ZuM60EcXKumC0REALwm6duAtTU+Ewa08jqUbY3rwNUYY4wxplkLZNKWxLEDzuZx7MCvxyyjqj5cD+Jx4obvuR942H9hb6zMx3HN1XcCxapaa19ZIjJNRJaKyNLCwhbfjY0xxhhjmrhAJm21DX9Ss3+RupZ5GNfjeckxC4u0x5XOZeD6CGsjIjfVtnFVfU5VM1U1Mz7+e9XCxhhjjDFNSiAbIuThOnOslsz3qzKrl8nzqjvbAXtx42JOEZE/4DqCrBKRMtx4ZNure4AWkXeBUcDfTja4iooK8vLyKCs7bhsG44mKiiI5OZnw8PBgh2KMMca0SIFM2pYAPUQkAzfw+HXADTWWmYUbTHgBbrDXL71BY0dXLyAi04ESVX1aREYAZ4tIa9yAvRcAp9TCIC8vj5iYGNLT0/EeozN1UFWKiorIy8sjIyMj2OEYY4wxLVLAqke9Z9TuBj4D1gNvqepaEXlERC7zFnsB9wzbFuAXwAMnWOciXIOF5bjuPkKA504lvrKyMuLi4ixhqwcRIS4uzkoljTHGmCAKaD9tqvox8HGNab/2e10GXH2CdUyv8f43wG8aIj5L2OrPjpUxxhgTXDYiQhAUFRUxaNAgBg0aRGJiIklJSd+9Ly8vr9c6brvtNjZuPP5Yz8888wwzZsxoiJCNMcYYE2QBLWkztYuLi2PlypUATJ8+nejoaO69995jllFVVJWQkNrz6pdeeumE27nrrrtOP1hjjDHGNApW0taIbNmyhf79+3PnnXcyZMgQdu7cybRp08jMzKRfv3488sgj3y177rnnsnLlSnw+H7GxsTzwwAMMHDiQkSNHUlBQAMBDDz3EH//4x++Wf+CBBxg+fDi9evVi/vz5AJSWlnLVVVcxcOBArr/+ejIzM79LKI0xxhgD+0rLmbOpkDeX5AQ1Ditpa2TWrVvHSy+9xLPPPgvAo48+SocOHfD5fIwbN44pU6bQt2/fYz5TXFzM2LFjefTRR/nFL37Biy++yAMPfL9Nh6qyePFiZs2axSOPPMKnn37KU089RWJiIu+88w6rVq1iyJAhZ2Q/jTHGmMZoX2k5q/OLWZ1fzBrv/7x9hwGICA3hisHJRIQFp8zLkjbg4Q/Xsm7HgQZdZ98ubfnNpf1O+nPdunVj2LBh371//fXXeeGFF/D5fOzYsYN169Z9L2lr1aoVF198MQBDhw5l7ty5ta77yiuv/G6ZrKwsAL755hvuv/9+AAYOHEi/ficfszHGGNMU7fUStDX5xazOcwla/v7D381Pi2vNoJRYbj47jQFJ7eiX1C5oCRtY0tbotGnT5rvXmzdv5sknn2Tx4sXExsZy00031drtRkRExHevQ0ND8fl8ta47MjLye8u4bvGMMcaY5u1ECVp6XGsGp8Zyy8ijCVq7Vo2rQ3lL2uCUSsTOhAMHDhATE0Pbtm3ZuXMnn332GRMnTmzQbZx77rm89dZbjB49mtWrV7Nu3boGXb8xxhgTDL7KKpbn7OfLDQV8vbGADbsOfjcvPa41Q9Lac+uoNPontaNfl8aXoNXGkrZGbMiQIfTt25f+/fvTtWtXzjnnnAbfxk9+8hNuueUWzjrrLIYMGUL//v1p165dg2/HGGOMCbQ9JUeYvbGQLzcWMHdTIQfKfISFCMMzOnD/xN4MTGk6CVptpCVUj2VmZurSpceOdrV+/Xr69OkTpIgaD5/Ph8/nIyoqis2bN3PRRRexefNmwsK+n8/bMTPGGNOYVFUpq/OL+WpjAV9tLOTbvP2oQnxMJON6xXN+7wTO6d6RmKimlaSJyDJVzaw53UraWriSkhIuuOACfD4fqsqf//znWhM2Y4wxpjEoPlzB3M2FfLWhkNmbCthTUo4IDE6J5RfjezKudwJ9O7clJKT5jeRj384tXGxsLMuWLQt2GMYYY0ytVJVNu0v4amMBX24oYFn2PiqrlNjW4YztGc+4XgmM6RlPhzYRJ15ZE2dJmzHGGGMaleLDFczfsofZmwqZvamQncWu54S+ndty59iunN87gUEp7QlthqVpx2NJmzHGGGOCqqpKWbOjmNkbXZK2Inc/lVVKTFQYo3t05J4e8ZzXK4HEdlHBDjWoLGkzxhhjzBm3p+QIczcXMntjIXM376GotByAs5Lb8ePzujG2ZzyDUmIJC7URN6tZ0maMMcaYgPNVVrEid/93pWmr84sBiGsTwZie8YztGc+5PTrSMToyyJE2Xpa0BUFRUREXXHABALt27SI0NJT4+HgAFi9efMwIB8fz4osvMmnSJBITEwMWqzHGGHOqdhYf/i5J+2bLHg6W+QgNEYakxnLvRT0Z2zOBfl2aZ0vPQLCkLQji4uJYuXIlANOnTyc6Opp77733pNfz4osvMmTIEEvajDHGNArlviqWZu3l602u2nPjbjcKQed2UfxgQGfG9oxnVPeOTbZz22CzpK2Refnll3nmmWcoLy9n1KhRPP3001RVVXHbbbexcuVKVJVp06bRqVMnVq5cybXXXkurVq1OqoTOGGOMaSh5+w7xtVeaNn/LHkrLKwkPFYald+DBob0Z2zOBnp2iEbHStNNlSVsjsmbNGt577z3mz59PWFgY06ZN44033qBbt27s2bOH1atXA7B//35iY2N56qmnePrppxk0aFCQIzfGGNNSHPFVsnj73u8StS0FJQAkxbbi8sFJnNcrgZHd4oiOtBSjodkRBfjkAdi1umHXmTgALn70pD7yj3/8gyVLlpCZ6UauOHz4MCkpKUyYMIGNGzdyzz33MGnSJC666KKGjdUYY4w5jpyiQ3y9qYDZGwuZv7WIwxWVRISGMKJrB64blsJ5vRLoFt/GStMCzJK2RkRVuf322/ntb3/7vXnffvstn3zyCX/605945513eO6554IQoTHGmJbAV1nF4qy9fLFuN7M3FrJtTykAqR1ac3VmMuf1iufsrnG0jrA04kyyow0nXSIWKOPHj2fKlCncc889dOzYkaKiIkpLS2nVqhVRUVFcffXVZGRkcOeddwIQExPDwYMHgxy1McaY5uBweSVzNhfy+drd/HPDbvYfqiAiLISRXeO4eWQaY3vGk9HRStOCyZK2RmTAgAH85je/Yfz48VRVVREeHs6zzz5LaGgoU6dORVURER577DEAbrvtNu644w5riGCMMeaU7C0t55/rd/P5ut3M3VxIWUUV7VqFc0HvBC7q14kxPeOtNK0REVUNdgwBl5mZqUuXLj1m2vr16+nTp0+QImqa7JgZY0zTl7v3EF+s283n63axePteqtR1yXFR305M6JfIsIwOhNsoBEElIstUNbPmdEufjTHGmGZMVdmw6yCfrd3F52t3s27nAQB6dYrhrnHduahvIv2T2lq1ZxNgSZsxxhjTzFRWKUuz9vK5V6KWu/cwIjA0tT0PTurNhX0TyejYJthhmpNkSZsxxhjTDPgqq1i4bS+frNnJZ2t3s6fkCBGhIZzTPY4fn9ed8X06ER9j43o2ZS06aat+sN+cWEt49tEYY5qacl8V87bu4ZPVO/li3W72HaqgVXgo5/dOYGL/RMb1TrBObpuRFnsmo6KiKCoqIi4uzhK3E1BVioqKiIqKCnYoxhjT4pVVVDJnUyGfrtnFF+t3c7DMR3RkGOP7JDCxvxvfs1VEaLDDNAHQYpO25ORk8vLyKCwsDHYoTUJUVBTJycnBDsMYY1qkQ+U+vt5YyMerd/LVhgJKyytp1yqcCf0SmTQgkXO6dyQyzBK15i6gSZuITASeBEKB51X10RrzI4FXgKFAEXCtqmb5zU8F1gHTVfVxb1os8DzQH1DgdlVdcLKxhYeHk5GRcSq7ZYwxxgTcwbIKvtxQwCerd/H1pgLKKqqIaxPBZYOSuLh/IiO7xVnXHC1MwJI2EQkFngEuBPKAJSIyS1XX+S02Fdinqt1F5DrgMeBav/lPAJ/UWPWTwKeqOkVEIoDWgdoHY4wx5kw6XF7JF+t3M2tlPnM27aG8soqEmEiuzUxhYv/ODM/oQGiIPdLTUgWypG04sEVVtwGIyBvAZFzJWbXJwHTv9UzgaRERVVURuRzYBpRWLywibYExwA8BVLUcKA/gPhhjjDEB5ausYt7WIj5Ykc9na3dRWl5JYtsobh6ZxqQBiQxOaU+IJWqGwCZtSUCu3/s8YERdy6iqT0SKgTgROQzcjyulu9dv+a5AIfCSiAwElgH3qGopxhhjTBOhqqzM3c8HK3fw0bc72FNSTtuoMC4d2IXJg5IYkdHBEjXzPYFM2mq72mr2G1HXMg8DT6hqSY2WnWHAEOAnqrpIRJ4EHgD+/XsbF5kGTANITU09+eiNMcaYBratsIT3V+5g1sp8sooOEREWwvg+CVw2MIlxveOtMYE5rkAmbXlAit/7ZGBHHcvkiUgY0A7YiyuRmyIifwBigSoRKcNVoeap6iLv8zNxSdv3qOpzwHPgxh5tkD0yxhhjTlLBgTJmrdrBByt3sDq/GBEY1S2OH4/rzsT+ibSNCg92iKaJCGTStgToISIZQD5wHXBDjWVmAbcCC4ApwJfqenEdXb2AiEwHSlT1ae99roj0UtWNwAUc+4ycMcYYE3QHyyr4dM0uPli5g/lb91ClMCCpHQ/9oA+XDuxCp7bW76U5eQFL2rxn1O4GPsN1+fGiqq4VkUeApao6C3gBeFVEtuBK2K6rx6p/AszwWo5uA24LzB4YY4wx9VdRWcXsjYW8tyKfL9bvptxXRWqH1tw9rjuXDUqie0J0sEM0TZy0hOGJMjMzdenSpcEOwxhjTDOjqqzKK+a95Xl8+O1O9paW06FNBJee1ZnJg5MYnBJro+6YkyYiy1Q1s+b0FjsigjHGGHOqcvce4v0V+YefMyAAACAASURBVLy3Ip9te0qJCAvhwr6duHJwEmN6xluntyYgLGkzxhhj6qH4UAV/X72T91bksSRrHwAjMjrwo7FduXhAZ2tQYALOkjZjjDGmDuW+KmZvKuS9FXn8Y10B5ZVVdItvw30TejF5UBeS29ugPObMsaTNGGOM8VPd8e17K/L5cNUO9h2qIK5NBDeMSOXKIUkMSGpnz6mZoLCkzRhjjAHy9x/m3WV5vLsin+17Somsfk5tSBKje9hzaib4LGkzxhjTYh0ur+Sztbt4e1ku87cWoQpnd+3Av47txsQB1vGtaVwsaTPGGNOiqCrLc/bx9tI8Pvp2JyVHfKR0aMXPLujJlUOSSOlgz6mZxsmSNmOMMS3CzuLDvLs8n3eW5bFtTymtwkOZNKAzV2cmMzzdBmg3jZ8lbcYYY5qtsopKPl+3m5nL8vhmcyFVCsMzOnDned2YNKAz0ZH2NWiaDrtajTHGNCvVrT9nLstj1qodHCzzkRTbirvHdeeqocmkxbUJdojGnBJL2owxxjQLBQfKeHdFPjOX5bGloISo8BAu7t+ZKUOTGdk1zqo/TZNnSZsxxpgmy1dZxVcbC3lzSS5fbSygskoZmtaeR68cwKSzbJQC07xY0maMMabJ2b6nlLeW5vLOsjwKDh4hPiaSfxndlaszk+kWHx3s8IwJCEvajDHGNAllFZV8smYnbyzOZdH2vYQInN87gWsyUxjXO8E6vzXNniVtxhhjGrU1+cW8uSSX91fmc7DMR1pca+6b0IspQ5Pp1DYq2OEZc8ZY0maMMabRKT5UwQer8nlzSS5rdxwgMiyEi/sncu2wVEZkWJ9qpmWypM0YY0yjoKos3LaXN5fk8MmaXRzxVdG3c1semdyPyQOTaNfaGhWYls2SNmOMMUG1r7ScN5fm8sbiHLKKDhETFcY1mSlcOyyF/kntgh2eMY2GJW3GGGOCYnVeMa8syGLWqh0c8VUxPKMD94zvwcR+nWkVERrs8IxpdCxpM8YYc8Yc8VXyyepdvLwgixU5+2kdEcrVmcncfHY6vRJjgh2eMY2aJW3GGGMCbmfxYWYszOGNJTnsKSkno2MbfnNpX64ammwd4BpTT5a0GWOMCYjqhgWvLMji83W7qVLlgt4J3DIynXO7d7QWoMacJEvajDHGNKjSIz7eXZHPqwuy2LS7hNjW4dwxOoObRqSR0qF1sMMzpsmypM0YY0yD2FpYwqsLsnlnWR4Hj/jon9SWP0w5i8sGdiEq3BoWGHO6LGkzxhhzyiqrlK82FPDygizmbt5DeKjwgwGduWVUOoNTYhGxKlBjGoolbcYYY05a8eEK3l6ay8sLssjde5hObSP55YU9uW54KvExkcEOz5hmyZI2Y4wx9bZ590H+Oj+Ld5fnc7iikmHp7XlgYh8u6tfJBmw3JsAsaTPGGHNclVXKP9fv5uUFWczbUkREWAiTB3bh1lHpNmKBMWeQJW3GGGNqVXyogre8KtC8fYfp3C6K+yb04vrhqXRoExHs8IxpcSxpM8YYc4yNu1wV6PsrXBXo8IwOPDipDxf17USYVYGalqyyAkKD1xm0JW3GGGOorFL+sX43L8/PYv7WIiLDQrh8UBK3jkqnb5e2wQ7PmDPLVw5FW6BgHRRugIL17t+RA3DvZghSq+iAJm0iMhF4EggFnlfVR2vMjwReAYYCRcC1qprlNz8VWAdMV9XH/aaHAkuBfFW9JJD7YIwxzdm+0nLeWprLqwuzydt3mC7torh/Ym+uG5ZCe6sCNc1dpQ/2bXfJWcGGo0la0Rao8rllJAQ6dINO/SChj5sepNK2gCVtXmL1DHAhkAcsEZFZqrrOb7GpwD5V7S4i1wGPAdf6zX8C+KSW1d8DrAfs558xxpwkVWVp9j5mLMzm4zW7KPdVMSKjAw/9oA/j+1gVqGmGVGFflldqVp2grYc9m6DyiLeQQPs0SOgLvSa5/xN6Q1wPCI8KZvTfCWRJ23Bgi6puAxCRN4DJuJKzapOB6d7rmcDTIiKqqiJyObANKPVfqYgkAz8Afg/8IoDxG2NMs1J8uIL3lucxY1EOmwtKiIkM47phKdwwIpXeifYb2DQz+3Nh+2zYNtv9X7L76Ly2ya7UrNt5LjmL7w3xvSCiTdDCrY9AJm1JQK7f+zxgRF3LqKpPRIqBOBE5DNyPK6W7t8Zn/gj8GxATiKCNMaY5UVVW5u5nxqIcPvp2B2UVVZyV3I7HrhrApQO70DrCHm02zcShvbB9ztFEbe9WN71NPGSMgfRzoVN/l5xFNc2uagL511rbU3paz2UeBp5Q1RL/IVBE5BKgQFWXich5x924yDRgGkBqaupJhG2MMU3fwbIK3l+5g9cW5bB+5wFaR4RyxeBkbhyRan2rmeah/BDkLIBtX7tEbee3gEJENKSdA8PugK5jXUlaMxlOLZBJWx6Q4vc+GdhRxzJ5IhIGtAP24krkpojIH4BYoEpEynAlc5eJyCQgCmgrIn9T1ZtqblxVnwOeA8jMzKyZLBpjTLO0Oq+Y1xZn88HKHRwqr6Rv57b87vL+TB7UhZio4HVVYMxpq/TBjuWuFG3b15C3GCrLISQcUobDuAchYywkDQlqtxyBFMikbQnQQ0QygHzgOuCGGsvMAm4FFgBTgC9VVYHR1QuIyHSgRFWf9ib9ypt+HnBvbQmbMca0JKVHfHy4agczFuWwOr+YqPAQLj2rCzeencbA5HY2aLtpmip9sOtbyJ4PWd+4f+UHAYHEATDiR9D1PEgd2eifRWsoAUvavGfU7gY+w3X58aKqrhWRR4ClqjoLeAF4VUS24ErYrgtUPMYY09xsKTjIy/OzeW9FPiVHfPTsFM3Dl/Xj8sFJtGvVPEsaTDPmOwI7VkD2PMiaB7mLoLzEzevQDQZMcdWd6WOgTVxwYw0ScQVbzVtmZqYuXbo02GEYY8xpU1UWbC3iL3O38dXGQiLCQvjBgM7cOCKVoWntrVTNNB3lhyBviStJy57nXvvK3Lz4PpB+DqSNgtRR0LZzcGM9w0Rkmapm1pxuzYaMMaYJKPdV8ffVO/jLnO2s23mAuDYR/Gx8D24+O4246Mhgh2fMiZUdcKVn2fNcopa/HKoqXOe1iQMg83bXgCB1ZIstSTsRS9qMMaYRKz5UwWuLc3h5fha7DpTRPSGaR68cwOWDk4gKDw12eMbU7fD+o8+jZc9zz6dpFYSEQZchMPIuL0kb0WS74DjTLGkzxphGKHfvIV74ZjtvLc3lUHklo7rF8Z9XDmBsz3hCQqwK1DRCZQdcFxxZc2H73KNJWmgkJA+DMfe56s7kYS2m4UBDs6TNGGMakWXZ+3h+7jY+W7uLEBEuG9iFqaMz6NfFSiJMI3OkBHIXugQtay7sWAlaCaERXpL2b5AxGpIyG80wUE2dJW3GGBNklVXK52t38Ze521ies5+2UWFMG9ONH45KJ7GdfdmZRqL8kHsmrbokbcdyN3h6SJhLzEb/AtJHuz7TwlsFO9pmyZI2Y4wJktIjPt5amsuL87aTu/cwKR1aMf3SvlydmUKbSLs9myDzHYHcxUeTtLwlXsOBUNeB7aifuqGhUs+26s4zxO4Kxhhzhm0pKOG1RTnMXJbLgTIfQ1JjefDiPlzUL5FQe17NBIsq7NkMW7+Erf90DQgqDrnWnZ0Hwdn/6sbwTD0bIm3472CwpM0YY86Acl8Vn6/bxYyFOSzYVkR4qDChXyK3nZPB0LT2wQ7PtFSH9rohobZ+CVu/ggN5bnpcdxh8E3Qd5/pLs9adjYIlbcYYE0B5+w7x+uIc3lySx56SIyTFtuK+Cb24JjOF+BjrX82cYZUVrppz65ew5Z9uBALUJWUZY2HMvdDtfGifFuxITS0saTPGmAZWWaXM3lTA3xbm8NXGAgDO75XATWenMaZnvFWBmjNHFfZu80rSvoTtc9zQUBLqWnie9yuXpHUZDKGWEjR2doaMMaaBFBws4+2leby2KIf8/YfpGB3JXed157rhKSS3bx3s8ExLUX4Its+GTZ+5Z9P257jpsWlw1jUuScsYY1WeTZAlbcYYcxpUlYXb9vK3Rdl8tmYXviplVLc4HpzUh4v6dSI8NCTYIZqWYF82bP7cJWrb50DlEYiIhq7nwTn3uEStQ9dgR2lOkyVtxhhzCooPVfDO8jxmLMpma2Ep7VqFc+uodG4YkUq3+Ohgh2eau0qfezZt06cuUStc76Z36ArDpkLPCW6g9bCI4MZpGpQlbcYYcxI27z7IS/OzeHd5HmUVVQxKieXxqwdyyVmdbSxQE1iH9rrGA5s/g81fQNl+17Ft2ijX0rPnROjYPdhRmgCypM0YY06gqkqZs7mQF+dlMWdTIRFhIVwxKImbR6bRP8meCzIBogqFG7zStM/dkFFaBa07Qq9JrjSt2zh7Nq0FsaTNGGPqcKjcx7vL83lp3na2FpaSEBPJvRf15PrhqcRFW3cdJgAqfZA9DzZ8BBs/hWKvEUHiWTD6l640rcsQCLFnJVsiS9qMMaaGHfsP88qCbF5fnEPx4QoGJLXjj9cOYtKAzkSE2ZelaWAVZa6D2/UfwsaP4fBeCGvlStHG/BJ6XARtuwQ7StMInDBpE5FzgJWqWioiNwFDgCdVNTvg0RljzBm0LHsfL87bzqdrdqGqTOyfyO3eiAUi1reaaUBHDrrn0tZ/6Fp9lpdAZDvoNRH6XArdLoAI6ybGHKs+JW3/BwwUkYHAvwEvAK8AYwMZmDHGnAkVlVV8smYXL36znZW5+4mJCmPquRncMjLN+lYzDevQXtj4iUvUtn7puuVoEw8DprhELX2MtfY0x1WfpM2nqioik3ElbC+IyK2BDswYYwJpX2k5ry/J4ZX52ew6UEbXjm14ZHI/rhqSTJtIe3LENJADO93zaes/dAOwayW0S3HdcvS5FFJGQIi1Ojb1U58700ER+RVwEzBGREKB8MCGZYwxgbG1sIQXvtn+XZcdo3t05D+vHMDYnvGE2PBSpiHs3e6StPUfQt5iNy2uB5z7M5eodR4EVt1uTkF9krZrgRuAqaq6S0RSgf8KbFjGGNNwVJWl2ft4bs42/rF+N+GhIVw1JIkfjsqgV2JMsMMzzUHhJlj/Aaz7AHatdtM6D4TzH4I+l0F8r+DGZ5qFepW04apFK0WkJ9AbeD2wYRljzOmrrFI+X7uL5+ZuY0XOftq3Ducn5/fglpFpdLQuO8zpUIXda2H9LJeoFW5w01NGwEW/dyVq7dOCG6NpduqTtM0BRotIe+CfwFJc6duNgQzMGGNO1eHySmYuy+X5b7aTXXSI1A6t+e3kfkwZmkKrCHt+yJwiVdi50iVp62bB3q0gIW64qIv/C/pcYl1zmICqT9ImqnpIRKYCT6nqH0RkZaADM8aYk1VUcoSXF2Tz6oIs9h2qYGBKLPdP7M2EfomE2vNq5lRUVUH+UpeorZ8F+3NAQiFjDIz6CfS+BKLjgx2laSHqlbSJyEhcydpUb5r9VDXGNBrbCkt4/pvtvLMsjyO+Ksb36cS0MV0Zlm79q5lTUFUJOQtcadr6D+HgDggJh27nw9j73RBSrTsEO0rTAtUnafsZ8CvgPVVdKyJdga8CG5YxxpzYsuy9/Hn2Nr7wa1ww9dyudE+IDnZopqmp9EH2N16J2odQWghhUdB9PPR92I3zaWN8miA7YdKmqrOB2SISIyLRqroN+GngQzPGmO+rrFK+WLeb5+ZsZXnOftq1Cufucd25ZWQ68THWuMCchOpEbe17sP4jOLQHwlu7YaP6Tnb/R9oPANN41GcYqwG4ERA6uLdSCNyiqmsDHZwxxlTbW1rOzGW5zFiUQ3bRIZLbt2L6pX25ZlgKrSOsM1xTT5U+yJoL6953JWqHiiC8jStJ63c5dL/Qho8yjVZ97nR/Bn6hql8BiMh5wF+AUQGMyxhjUFWWZe9jxqIc/r56J+W+Koalt+e+Cb2Y2C+RsFAbvN3UQ6UPsubA2vfd6ATViVqvidD3clcFaomaaQLqk7S1qU7YAFT1axFpE8CYjDEt3IGyCt5fkc+MhTls3H2QmMgwrh+Wwg0j0qwzXFM/lRWwfY5XovYRHN57NFHrd4VL1MJbBTtKY05KfZK2bSLy78Cr3vubgO31WbmITASexLU2fV5VH60xPxJX9ToUKAKuVdUsv/mpwDpguqo+LiIp3vKJQBXwnKo+WZ9YjDGN3+q8YmYsyuaDlTs4XFHJgKR2PHbVAC4d2MWqQM2JVSdqa9+DDX93iVpENPSc6FV9WqJmmrb63AVvBx4G3gUE19nubSf6kDdG6TPAhUAesEREZqnqOr/FpgL7VLW7iFwHPIbruLfaE8Anfu99wC9VdbmIxADLROSLGus0xjQhh8p9fLRqJzMWZbMqr5hW4aFcNrALN56dylnJscEOzzR2VZWQPR/WvONaflYnar0u9qo+L7BEzTQb9Wk9uo9Tay06HNjitTZFRN4AJuNKzqpNBqZ7r2cCT4uIqKqKyOXANqDUL5adwE7v9UERWQ8k1VinMaYJ2LT7IK8tyuGd5XkcLPPRs1M0D1/Wj8sHJ9GuVXiwwzONmSrkL4c1M12p2sGdrtVnr0l+VZ9RwY7SmAZXZ9ImIh8CWtd8Vb3sBOtOAnL93ucBI+paRlV9IlIMxInIYeB+XCndvXXElw4MBhbVMX8aMA0gNTX1BKEaY86EI75KPl2zixkLc1ictZeI0BAmDUjkxrPTyEyzjnDNcahCwTpYPdOVqu3PhtAI19pzwFWuCjTCHrc2zdvxStoeP81113b3rZkE1rXMw8ATqlpS201cRKKBd4CfqeqB2jauqs8BzwFkZmbWmXwaYwKv4EAZf1uYzYxFORSVlpMW15oHJ/VmytAUOrSJCHZ4pjEr2gpr3nWlaoUb3BBSXce6kQl6/wBaWRW6aTnqTNq8TnVPRx6Q4vc+GdhRxzJ5IhIGtAP24krkpojIH4BYoEpEylT1aREJxyVsM1T13dOM0RgTQKty9/PSvO38ffVOfFXKBb0TuGVkOud270iIjQVq6lKc56o9V890A7SDG5T9B/8NfSbbWJ+mxQpkc6wlQA8RyQDygeuAG2osMwu4FVgATAG+VFUFRlcvICLTgRIvYRPgBWC9qv5PAGM3xpwiX2UVn67dxUvzsliWvY/oyDBuOjuNW0emk97Rqq9MHUqLYO27ruozZ4Gb1mUwXPR795xau6TgxmdMIxCwpM17Ru1u4DNclx8vemOXPgIsVdVZuATsVRHZgithu+4Eqz0HuBlYLSLezy8eVNWPA7MXxpj62ldazutLcnh1QTY7i8tIi2vNry/py9WZycREWcMCU4uqStj2FSx/BTZ8DFUVEN8Hzn8I+l0Jcd2CHaExjYq4gq16LCjSRlVLT7xk45OZmalLly4NdhjGNEubdh/kpXlZvLcij7KKKs7pHsdtozIY1zuBUKsCNbXZnwMrZsCKv8GBPGjVAQZeD4NvhE79gh2dMUEnIstUNbPm9PqMPToKeB6IBlJFZCDwI1X9ccOHaYxpCqqqlK83FfDSvCzmbt5DZFgIVwxO4ofnpNM7sW2wwzONke+I6/B2+Suw7Ws3rdv5MOF3rquOsMighmdMU1Cf6tEngAm4589Q1VUiMiagURljGqWSIz5mLs3l5QXZbN9TSqe2kdw3oRfXD0+1VqCmdrvXwvJX4ds3Xce37VLgvAdg0I0Qm3LizxtjvlOvZ9pUNbdG1xuVgQnHGNMY5e8/zEvfbOfNJbkcPOJjcGosf7p+MBf3TyTcBm03NZUdcI0Klr8C+ctcf2q9fwCDb4au50FIaLAjNKZJqk/SlutVkaqIROBGR1gf2LCMMY3Bmvxi/jJ3Gx99uxOASQM6c/s56QxObR/kyEyjowq5i1yitvY9qDgECX1hwn/CWddCm7hgR2hMk1efpO1O3KDvSbh+1T4H7gpkUMaY4FFVZm8q5C9ztzFvSxFtIkK5bVQ6t52bQVKsjeFoaigtglWvwbKXoWizG/dzwNUw5BZIGgo2yoUxDaY+Y4/uAW48A7EYY4Ko3FfFrFU7+MucbWzcfZBObSN54OLeXD881cYCNcdShay5sOyvsP5DqCyHlBFw7jNukPbI6GBHaEyzVJ/Wo3+qZXIxrq+1Dxo+JGPMmVR8uILXF+fw0rzt7D5whF6dYnj86oFcNrALEWH2vJrxU1J4tFRt71aIioXMqTD0VkjoE+zojGn26lM9GgX0Bt723l8FrAWmisg4Vf1ZoIIzxgROdeOCN5bkUnLExznd43jsqrMY2zPeBm43R1VVQdYcr1TtI9cBbuooN/Zn38sg3KrMjTlT6pO0dQfOV1UfgIj8H+65tguB1QGMzRgTAGvyi3l+7jY+9BoXXHJWZ/5ldFf6J7ULcmSmUSkpgJUzXKnavu3Qqj0Mn+ZK1eJ7BTs6Y1qk+iRtSUAbXJUo3usuqlopIkcCFpkxpsGoKnM27+G5OVu/a1zww1Hp3HZOOsntWwc7PNNYVFXB9q9dqdqGv0OVD9LOhXH/D/pcCuFRwY7QmBatPknbH4CVIvI1IMAY4D9EpA3wjwDGZow5TcWHK3hveR4zFuWwuaCEhJhI7p/YmxtGWOMC4+fgblj5N1eqtj/bDSs14k4Y+kPo2CPY0RljPPUae1REOgPDcUnbYlXdEejAGpKNPWpamm/z9jNjYQ6zVu3gcEUlA5PbcdPZaUwelGSNC4xzpAQ2fgJrZsKWf7hStfTRLlHrc6kNK2VMEJ3y2KOeMmAnrlFCdxHprqpzGjJAY8zpOVTu48NVO/jbwhxW5xfTKjyUyYO6cOOINAYk2/NqBvCVw9Z/wuq3XcJWcQjaJsPIu2DwLdCxe7AjNMYcR326/LgDuAdIBlYCZwMLgPMDG5oxpj427T7IjIXZvLs8n4NHfPTsFM0jk/tx+eAk2kZZFWiLV1UF2fNcorbuAyjb76o/B14PA6ZAytkQYqWvxjQF9SlpuwcYBixU1XEi0ht4OLBhGWOO54ivkk/X7GLGwhwWZ+0lIjSESQMSufHsNDLT2luXHS2dKuxcCatnwpp34eAOCG/jxv8ccDV0GwehltAb09TUJ2krU9UyEUFEIlV1g4hYe29jgiCn6BAzFmfz9tI89paWkxbXml9d3JspQ5OJi7ZnkFq8PZtdorb6bdf5bUg49LgQBvwOel4MEdZS2JimrD5JW56IxALvA1+IyD6gSTVEMKYp81VW8c8NBcxYlMOcTYWEhgjj+yRw44g0zu3ekZAQK1Vr0YrzYe27LlHbuQoQyBgN59zjOr9t1T7YERpjGkh9xh69wns5XUS+AtoBnwY0KmMMxYcreHNJDi/PzyZ//2ES20bxs/E9uG5YKontrL+sFq2qEjZ/AUuedy0/UegyBCb8B/S7Etp2DnaExpgAOG7SJiIhwLeq2h9AVWefkaiMacGy9pTy0rztvL0sj0PllYzI6MC/X9KX8X0SCAu1B8ZbtNI9sPwVWPoSFOdAdCKMuQ8GXgdx3YIdnTEmwI6btKlqlYisEpFUVc05U0EZ09KoKgu37eWFb7bzzw27CQsRLh3YhdvPybDhpVo6Vchd7ErV1r0PleWuP7WLfusaFliDAmNajPo809YZWCsii4HS6omqelnAojKmhTjiq+SjVTt54ZvtrNt5gA5tIrh7XHduPjuNhLZWBdqiHSlxz6kteQF2r4bItjD0Nsi8HRJ6Bzs6Y0wQ1Cdps+49jGlgRSVHmLEoh1cXZlN48Ag9EqJ59MoBXD44iajw0GCHZ4KpYAMsfQFWvQFHDkCnAXDJH11XHZHRwY7OGBNE9WmIMFtE0oAeqvoPEWkN2LeKMadg0+6DvPjNdt5bkc8RXxVje8Yz9eoMRvfoaH2rtWSVFbDhI1eqljUXQiOg3xUw7A5IHgZ2bRhjqN+ICP8CTAM6AN2AJOBZ4ILAhmZM86CqzN5UyAvfbGfu5j1EhYdw1dBkbj8nne4JMcEOzwRTcT4sf9kN1F6yC9qlwvjpMPhmaNMx2NEZYxqZ+lSP3oUbLH4RgKpuFpGEgEZlTDNQesTHeyvy+ev8LLYUlJAQE8l9E3pxw/BU2reJCHZ4JljKDrhStdUzYdvXoFWuA9xhf4Lu4yHEKjKMMbWrT9J2RFXLq6tuRCQM0IBGZUwTtn1PKa8uyObtZbkcLPPRP6ktT1w7kB8M6EJEmHXZ0SJVlMGWL1zDgk2fga8MYlPh3J+5UrUOGcGO0BjTBNQnaZstIg8CrUTkQuDHwIeBDcuYpqWqylWB/nV+FrM3FRIeKkwa0JlbRqYzJDXWnldriaoqYfscV6K2/kM4Ugxt4mHIra5RQXKmPatmjDkp9UnaHgCmAquBHwEfA88HMihjmoriQxW8vSyXVxdmk110iISYSH4+vifXj0ghIca67GhxVCF/mUvU1r4LJbshIgb6XAoDpkDGWAitz23XGGO+rz53j8nAK6r6l0AHY0xTsWHXAV6en837K/I5XFHJsPT23DehFxP6JRJuoxa0PIUbXdXn6rdhX5Zr/dlzgitR63ERhLcKdoTGmGagPknbZcAfRWQO8Abwmar6AhuWMY1PRWUVX6zbzcvzs1i0fS9R4SFcPiiJm0em0a+LjVrQ4hTnwZp3XKK2azVICGSMccNK9b4EWsUGO0JjTDNTn37abhORcOBi4Abgf0XkC1W940SfFZGJwJO4ft2eV9VHa8yPBF4BhgJFwLWqmuU3PxVYB0xX1cfrs05jGtqekiO8sTiHvy3MYdeBMpLbt+LBSb25JjOF2NbWCrRF8ZXDxo9dNx1bvwIUkjJh4mOuX7WYTsGO0BjTjNXr4QpVrRCRT3CtRlvhqkyPm7SJSCjwDHAhkAcsEZFZqrrOb7Gp8P/bu+/wqq4z3+PfVwgQHdMxIIpN7yDA2Lg3cMMd3HCJQzxpcz2TyTg3yY2Tmck4ceZmkjgN2xgDxhjbeIKTG7c4MTZVooPBmCJA9N4EEtJ57x9rkyiywELS0dE5+n2eqnhGJQAAIABJREFU5zycsvY6715s0Ku1V+Ggu19oZuOBHwHjSnz+U+CP51inSKW5O8u2HWLagi38YeVOCotjXNq9Ff9+az+u7NWGOmkaQF6r7NsQErXlMyB/HzTtCJf/KwwcBy26JTo6EaklyrO47mhgPHAl8BfCJIS7y1H3cGCDu2+K6plJSPZKJlhjgSej568Bz5iZubub2a3AJkrsd1rOOkUq7HhBEb9bvoPpC7fw8c4jNK6fzr0jMnlgZGcuaK0thGqVUydh7Zyw8O2Wj8DqQM8xYfbnhVdrPTURqXbl6Wl7iDCW7UvuXnAOdXcAtpV4nQeMOFMZdy8ys8NASzM7AfwroUftG+dYp8g5+3T3UaYv3MLspds5WlBEr3ZN+Pdb+3Hr4A40rq/ZfrXK7jWwdGrY+/PkITivK1z9PRh0n25/ikhClWdM2/iSr83sEuBed//K5xxa1v2j0ovynqnM94GfuvuxUutblafO03FOJGy/RWZm5ueEKrVRYVGMt9fsYvrCLSzafIB6ddK4oX87HhjZmSGZ52lttdqk4FhYomPJi7A9J8z+7H1z6FXrcimkaUawiCReuboQzGwQYRLC3cBmYHY5DssDOpV43RHYcYYyedFOC82AA4TeszvN7MdAcyBmZieBJeWoEwB3nwRMAsjKytIODvJX2w+d4OVFW5mZvY19xwro1KIBT4zpxV1DO9Kycf1EhyfVxR12LAtj1Va9DoVHoVVPuP6HMGA8NGqZ6AhFRP7OGZM2M+tBGMt2D2Fm5yuAufuV5aw7G+huZl2B7VFd95YqMwd4EFgA3Am87+4OXFoijieBY+7+TJTYfV6dIp8RizlzP93L9IVbeX/dbhy4qmcb7h/Zmcu7tyZNEwtqj4KjsPIVWDIlLNWR3gD63Q5DJkCnEdqlQERqrLP1tK0DPgRudvcNAGb2eHkrjsaofRV4m7A8x2R3X2NmPwBy3H0O8Dwwzcw2EHrYxp+5xjPXWd6YpPY5cLyQWTnbmLFoK1sP5NOqcT0eu/wC7hmeSacWDRMdnlSnvZ/A4mfDWLXCo9BuANz4X2EB3AytsyciNZ+Fjq0yPjC7jZBEXQy8RZiM8Jy7J93OxllZWZ6Tk5PoMKQarco7zOR5m/nDqp0UFsUY3qUF94/szOi+7bRpe21SXBTWVct+NuwDWqce9LsDhn0ROg5NdHQiImUysyXunlX6/TP2tLn7G8AbZtYIuBV4HGhrZr8G3nD3d+IWrUgFuDtzP93Hbz/YyPyN+2lUrw7jh3XivhGd6dmuSaLDk+p0bE8Yq5bzAhzZDs06hRmgQyZAo1aJjk5EpELKM3v0OPAS8JKZtQDuImwir6RNaoRTxTHeXLGDSXM3sW7XUdo2rc8TY3px74hMmmbUTXR4Ul3cIS8bFk+CNf8DsVPQ7Uq44WnoMVrrqolI0junBajc/QDw2+ghklDHCoqYuXgrkz/azI7DJ+nepjE/vnMAYwedT/10/YCuNQrzYfVrYbzarpVQvykM+wIMexRadU90dCIiVUarhkrS2XP0JFPm5TJ94RaOnCxiRNcW/Ptt/biiRxvNAq1NDmyC7Odh2fSwCG6bPnDj/4UB46C+dq8QkdSjpE2SxoY9x3juw03MXrqdU7EYY/q1Y+JlFzCoU/NEhybVpbgINv4Jsp+DT98FSwuL4A7/InS+RMt1iEhKU9ImNV5O7gF+O3cT7368m/rpadw9rCOPjupGl1aNEh2aVJddq8JSHStnwfE90LgtXP5NGPoQND0/0dGJiFQLJW1SI8Vizrtrd/PbDzaydOshmjesy9ev7s6EkZ1ppV0Laoeju2DVqyFZ270a0upCj+th4D3Q/TpIr5foCEVEqpWSNqlRjhUUMXtpHlPm5bJp33E6tWjA92/py11ZHWlYT5dryivMD+uqrXgZNr4PHoMOQ+GGn4T11Rq2SHSEIiIJo5+CUiNs2nuMqQu28PqSPI4WFDGgYzN+cc9gxvRrR3odLYab0mIx2LoAVsyANb8LuxU07QijHg97gLbukegIRURqBCVtkjCxmPPB+r1MmZ/LB+v3UreOcWP/9jx4cRcGZ56X6PAk3vZvjMapzYRDW6FeY+gzFgaOh86jIE3JuohISUrapNodOXmKV3PymLYgl9z9+bRpUp/Hr+nBPSM60aZJRqLDk3g6cRBWzw7JWt5iwOCCK+Gq70KvG6GeJpeIiJyJkjapNp/uPsqLC3KZvXQ7+YXFDO18Hv90XU/tB5rqTi/TsXxGGK9WXAite8O1PwibtWv2p4hIuShpk7gqjjl/WrubFxfkMm/Dfuqlp3HLwPN56OIu9OvQLNHhSTzt/jiMU1s5C47thoYtIeuRMPuz/UCtqSYico6UtElcHMov5JXsbUxbuIW8gydo3yyDf7m+J+OHdaKlluxIXcf3hy2lls+AncshLR26Xw+D7tUyHSIilaSkTarUul1HmDIvlzeWbaegKMaIri349g29ubZPW80CTVXFp8LuBCtmwCdvhY3a2w2A0U+F25+NWiU6QhGRlKCkTSotFnPeX7eHyfM2M3/jfjLqpnH7kA5MGNmF3u2bJjo8iZddq0KP2spZkL8PGrWG4RNh0D3Qrn+ioxMRSTlK2qTCjhUU8WrONqbMz2XL/nzaN8vgm6N7cs+wTM5rpNtgKenY3miXghkhaUurCz3HhNufF14DdeomOkIRkZSlpE3O2db9+UyZn8usnG0cKyhiSGZz/uX6nlzftx11dQs09bjD5g9g8bOw/i2IFcH5g2HM09D/Tu1SICJSTZS0Sbm4Ows3HWDyvM28t3Y3dcy4cUB7Hr6kK4M6NU90eBIPJ4+E9dSyn4V966FBC7joH2DQfdCmd6KjExGpdZS0yVmdPFXMnBU7mPzRZtbtOkqLRvX4yhUX8sDIzrRtqoVwU9KedSFRWzETCo+FvT9v/Q30vQ3q6u9cRCRRlLRJmfYcOcn0hVt4adFW9h8vpFe7Jvzojv6MHdSBjLp1Eh2eVLXiorDwbfazsHku1KkP/W6HYV+EjkMTHZ2IiKCkTUpZlXeYyfM28/uVOyiKOVf3assjl3Rh5AUtMS2GmnqO7YWlUyDnBTiyHZp1gqu/B0MmaKkOEZEaRkmbEIs5f1m/h99+sIlFmw/QuH4691/UmQdHdqFLK+0FmXLcIS8n9KqteSNsK9XtCrjhaegxGtLUkyoiUhMpaavFCoqK+d2yHTz74SY+3XOM85tl8J0bezNuWCeaZGjphpRz6gSsfj3MAt25HOo1gaEPw7BHoXWPREcnIiKfQ0lbLXQ4/xTTF21hyvxc9h4toHf7pvz3uEHcOKC9luxINe6wc0VYAHfFDDhxEFr3ghv/CwaMg/pNEh2hiIiUk5K2WiTvYD6TP8plZvZW8guLubR7K3569yAuuVDj1VLOwS1hEdyVs2DfJ39bBHf4ROgySpu1i4gkISVttcDq7YeZNHcTf1i1EwNuGXg+j17ajT7na4uplJJ/IIxRW/UqbF0Q3su8GG76b+gzVovgiogkOSVtKcrd+WD9XibN3cT8jftpXD+dRy7pwsOXdOX85g0SHZ5UlVMnwi4FK2eFTdtjp8Ltz6v/D/S7E87rnOgIRUSkiihpSzGFRTHmrNjBs3M38cnuo7RrmsG3xvTinhGZNNXkgtQQK4bcj0KitnYOFByBJu1hxJfCOLV2/XX7U0QkBSlpSxGH8guZmb2NKfNy2XXkJL3aNeG/7hrIzQPPp166JhckPfewQfuqWbDqNTi6M8z+7HMLDLgbulyqpTpERFKckrYkt27XEV6cn8sby7Zz8lSMiy9oyVN39OfyHq01uSAVHN8Hy2eEx961kJYO3a+D/j8MEwvq6la3iEhtEdekzcxGAz8D6gDPuftTpT6vD0wFhgL7gXHunmtmw4FJp4sBT7r7G9ExjwOPAg6sAh5295PxPI+apjjmvLd2N1Pm5bJg037qp6dx+5AOTBjZhd7tNbkg6blD7oewZAqsfTMsfttxeFimo89t0KhloiMUEZEEiFvSZmZ1gF8C1wJ5QLaZzXH3j0sU+wJw0N0vNLPxwI+AccBqIMvdi8ysPbDCzN4E2gJfB/q4+wkzmwWMB6bE6zxqkkP5hbySvY2pC7aw/dAJOjRvwBNjejEuqxPnNaqX6PCkso7vh+UvhWTtwEbIaAZZX4ChD0GbXomOTkREEiyePW3DgQ3uvgnAzGYCY4GSSdtY4Mno+WvAM2Zm7p5fokwGoVfttHSggZmdAhoCO+ITfs3xya6jTJmfyxvL8jh5KsZF3Vrw3Zv6cE3vNqRrMdzk5h4mFSx54W+9ap0ugsu/GZbp0O1PERGJxDNp6wBsK/E6DxhxpjJRr9phoCWwz8xGAJOBzsAD7l4EbDeznwBbgRPAO+7+ThzPIWF0CzTFHd8fdihYMgX2b4h61R6JetV6Jzo6ERGpgeKZtJU1Ct7LW8bdFwF9zaw38KKZ/RFoQOid6wocAl41s/vdffpnvtxsIjARIDMzs8InUd0O55/ilZytTF2whbyDugWaUtxhyzzIeSEs1XG6V+3Sb0DfW9WrJiIiZxXPpC0P6FTidUc+eyvzdJk8M0sHmgEHShZw97VmdhzoR0jWNrv7XgAzmw1cDHwmaXP3SUSTGbKyskonizVOWbdAv3Njb67p3Va3QJPd8f2w4uWoV+1TqN8sbNQ+9CFo2yfR0YmISJKIZ9KWDXQ3s67AdsKEgXtLlZkDPAgsAO4E3nd3j47ZFt0y7Qz0BHIJs1AvMrOGhNujVwM5cTyHuIrFnL+s38Pkj3L5aMM+6qencdvgDjx4sW6BJr1YDHLnwtJpJXrVRsClv4Y+t0K9homOUEREkkzckrYo4foq8DYh2Zrs7mvM7AdAjrvPAZ4HppnZBkIP2/jo8FHAE9FkgxjwZXffRxjr9hqwFCgClvG3pUGSxvGCImYvzeOFebls2necdk0z+ObontwzLFO3QJPd4e1hTbVl0+DQljBWbehDUa9a30RHJyIiSczca/ydw0rLysrynJzEd8htP3SCqfNzeXnxVo6cLGJgp+Z8YVRXxvRrR13dAk1exafC/p9Lp8KG98Bj0PUyGDwBet+ksWoiInJOzGyJu2eVfl87IsSZu7N06yEmz9vMW6t3ATC6XzseuaQrQzKba9eCZLbv05CorXgZju8N+3+O+icYfD+06Jro6EREJMUoaYuTU8Ux/t+qnUyel8uKbYdompHOo6O6MuHiLnRorp6XpFV4HNb8T7j9uXVB2Faqx2gYMgEuuBrq6J+UiIjEh37CVLGDxwt5OXsrU+dvYdeRk3Rt1Yh/G9uX24d0pFF9NXdScocdS0Ov2qrXofAotLwQrvk+DLwHmrRNdIQiIlILKIuoIhv2HGXyvFxmLw1Ldoy6sBU/vL0fV/RoQ1qaboEmpfwDsHJWSNb2rIH0BtD3NhjyAGSOBN3aFhGRaqSkrZLcnYnTlvDux7upl57GbYM68PCoLvRqpyU7klJxEWz8EyybDp/8EWKn4PzBcNNPod8dYTaoiIhIAihpqyQzo1urRvzztT24d0QmLRvXT3RIUhF71oXN2le+Asd2Q8OWMPyLMOheaNc/0dGJiIgoaasK37pBe0UmpROHYPXrIVnbvgSsDvS4HgbdB92vg3StmSciIjWHkjapXWLFsOnPYQHctb+H4gJo0weu+w8YcDc0bpPoCEVERMqkpE1qh30bYMUMWDETjmyHjOZhmY7B90H7QZpUICIiNZ6SNkldJ4/AmjdCr9q2hWBpcOE1cP1/QM8bIF3jD0VEJHkoaZPUs3MFLH42jFc7lQ+tesA1T8KA8dC0faKjExERqRAlbZIaigph7RxYPAm2LQprqg24K+z/2TFLtz9FRCTpKWmT5HZkB+S8AEumwPE9cF5XuP6HYamOBuclOjoREZEqo6RNko87bJkXetXW/h48FpboGD4RLrgK0tISHaGIiEiVU9ImyaPgWFj8dvGzsHdtmAE68suQ9QVo0TXR0YmIiMSVkjap+fZ9GhK1FS9DwRFoPxBueSZsK1WvYaKjExERqRZK2qRmihXD+rdCsrbpz5BWN2zWPnyiJhaIiEitpKRNapaDW0KP2rLpcHgbNO0AV30Hhjyo3QpERKRWU9ImiVeYD2vfhOXTYfNcwKDb5WEWaM8boI4uUxEREf00lMRwh7wcWDYt7FpQcASad4Yr/jcMugeaZyY6QhERkRpFSZtUr6O7wv6fy1+CfevDIrh9b4VB90HnS7Rch4iIyBkoaZP4KyoMkwqWTYcN74EXQ6cRcPPPw+SCjKaJjlBERKTGU9Im8bNrFSx7CVbNgvz90LgdXPL10KvWqnuioxMREUkqStqkap04BCtnhbFqu1aGpTp63QCD7g+7FWhSgYiISIXoJ6hUje1LIed5WPU6FJ2Adv1hzI+h/13QsEWioxMREUl6Stqk4gqPw+rXIft52Lkc6jaEAXdB1iNw/uBERyciIpJSlLTJuduzDnImh1mgBYehdW8Y8zQMHAcZzRIdnYiISEpS0iblU1QQFsDNmQxb5kGdetBnbOhVyxypbaVERETiTEmbnN3BXFgyBZZOg/x9cF4XuOb7MPh+aNQqwcGJiIjUHkra5LNixbD+7dCrtuG90IvWYwwMewS6XaUFcEVERBJASZv8zfF9kPNC6Fk7khfWVbv8m2Gz9mYdEh2diIhIrRbXpM3MRgM/A+oAz7n7U6U+rw9MBYYC+4Fx7p5rZsOBSaeLAU+6+xvRMc2B54B+gAOPuPuCeJ5HytuzFhb+Cla8AsUF0O1KGP2f0HMM1Kmb6OhERESEOCZtZlYH+CVwLZAHZJvZHHf/uESxLwAH3f1CMxsP/AgYB6wGsty9yMzaAyvM7E13LyIkgW+5+51mVg9oGK9zSGnusOFPsPCXsPH9sAfooHvhon+A1j0THZ2IiIiUEs+etuHABnffBGBmM4GxQMmkbSzwZPT8NeAZMzN3zy9RJoPQo4aZNQUuAx4CcPdCoDB+p5CCTp2Ala/Awl/D3nXhFuhV3w2zQLUIroiISI0Vz6StA7CtxOs8YMSZykS9aoeBlsA+MxsBTAY6Aw9En3cD9gIvmNlAYAnwj+5+vPSXm9lEYCJAZmZmlZ5YUjq6G7KfDZML8vdDuwFw22+h7+2QXi/R0YmIiMjniGfSVtbCXV7eMu6+COhrZr2BF83sj4R4hwBfc/dFZvYz4Angu5+pxH0S0bi4rKys0t9be+xcGcarrXoNYkVhnNpFX4Yuo7S2moiISBKJZ9KWB3Qq8bojsOMMZfLMLB1oBhwoWcDd15rZccLEgzwgL0roINxSfSIOsSe3WAw+fRsW/BJyP4S6jSDrYRjxGLS8INHRiYiISAXEM2nLBrqbWVdgOzAeuLdUmTnAg8AC4E7gfXf36Jht0S3RzkBPINfd95nZNjPr6e6fAFfz92PkarfC47B8RhivdmAjNO0QFsId+iA0OC/R0YmIiEglxC1pixKurwJvE5b8mOzua8zsB0COu88BngemmdkGQg/b+OjwUcATZnYKiAFfdvd90WdfA16KZo5uAh6O1zkkjZ0rYelUWDkr7AXaYSjc8XzYZkpLdoiIiKQEc0/94V5ZWVmek5OT6DCq1skjsPq1kKztWAZ16kOfW2DYo9BphMariYiIJCkzW+LuWaXf144IycQd8rJhyYuwZjacyoc2fWHMj6H/XVqyQ0REJIUpaUsG+QdgxczQq7Z3bZhY0P9OGPIQdBiiXjUREZFaQElbTRWLQe7ckKitfROKC6FDFtz8c+h3O9RvkugIRUREpBopaatpju6CZdNh2TQ4mAsZzcNuBYMfgHb9Eh2diIiIJIiStpogVgwb3oMlU2D92+DF0OVSuPI70PsmqNsg0RGKiIhIgilpS6T8A+H2Z87zcGgrNGoDF38NhkzQIrgiIiLyd5S0JcL2JbD4OVj9OhQXQOdRcO0PoNdNWldNREREyqSkrbqcOhmW6Vj8LOxYGmaADr4/rKvWtk+ioxMREZEaTklbvB3cEm5/Lp0GJw5Aqx4w5mkYOB4ymiY6OhEREUkSStriIRaDje9D9rNhYoGlQa8bYNgXoetlWldNREREzpmStqp04mDYsD37OTiwCRq1hsu+AUMfhmYdEh2diIiIJDElbVVh58rQq7byVSg6Efb+vPLb0PsWSK+X6OhEREQkBShpqyx3eO1hOLIDBtwdJha0H5DoqERERCTFKGmrLDO4czI0z4QG5yU6GhEREUlRStqqQvuBiY5AREREUlxaogMQERERkc+npE1EREQkCShpExEREUkCStpEREREkoCSNhEREZEkoKRNREREJAkoaRMRERFJAkraRERERJKAkjYRERGRJKCkTURERCQJmLsnOoa4M7O9wJY4f00rYF+cv6M2UXtWPbVp1VObVi21Z9VTm1at6mrPzu7euvSbtSJpqw5mluPuWYmOI1WoPaue2rTqqU2rltqz6qlNq1ai21O3R0VERESSgJI2ERERkSSgpK3qTEp0AClG7Vn11KZVT21atdSeVU9tWrUS2p4a0yYiIiKSBNTTJiIiIpIElLSVwcxGm9knZrbBzJ4o4/P6ZvZK9PkiM+tS4rNvRe9/YmbXl7fOVBenNs01s1VmttzMcqrnTGqGiranmbU0sz+b2TEze6bUMUOj9txgZj83M6ues6kZ4tSmf4nqXB492lTP2dQMlWjTa81sSXQ9LjGzq0ocU2uv0zi1p67RirXp8BJttsLMbitvnZXi7nqUeAB1gI1AN6AesALoU6rMl4HfRM/HA69Ez/tE5esDXaN66pSnzlR+xKNNo89ygVaJPr8ka89GwCjgMeCZUscsBkYCBvwRGJPoc02BNv0LkJXo80vCNh0MnB897wdsL3FMrbxO49ieukYr1qYNgfToeXtgD5Benjor81BP22cNBza4+yZ3LwRmAmNLlRkLvBg9fw24Ovptbyww090L3H0zsCGqrzx1prJ4tGltVuH2dPfj7v4RcLJkYTNrDzR19wUe/heaCtwa17OoWaq8TaVSbbrM3XdE768BMqIej9p8nVZ5e1ZL1DVbZdo0392LovczgNMTBOL6815J22d1ALaVeJ0XvVdmmegv7TDQ8izHlqfOVBaPNoXwj+SdqLt/Yhzirqkq055nqzPvc+pMZfFo09NeiG6hfLc23cqj6tr0DmCZuxdQu6/TeLTnabpGg3NqUzMbYWZrgFXAY9Hncf15r6Tts8q6YEtPsT1TmXN9v7aIR5sCXOLuQ4AxwFfM7LKKh5hUKtOelakzlcWjTQHuc/f+wKXR44EKxJasKt2mZtYX+BHwpXOoM1XFoz1B12hp5W5Td1/k7n2BYcC3zCyjnHVWmJK2z8oDOpV43RHYcaYyZpYONAMOnOXY8tSZyuLRppzu7nf3PcAb1J7bppVpz7PV2fFz6kxl8WhT3H179OdRYAa15xqFSrapmXUk/Lue4O4bS5SvrddpPNpT12gV/Lt397XAccJ4wbj+vFfS9lnZQHcz62pm9QgDD+eUKjMHeDB6fifwfjS+Yg4wPhp70RXoThg0W546U1mVt6mZNTKzJgBm1gi4DlhdDedSE1SmPcvk7juBo2Z2UXR7ZALwu6oPvcaq8jY1s3QzaxU9rwvcRO25RqESbWpmzYE/AN9y93mnC9fy67TK21PXaKXatGuUxGFmnYGehMlx8f15H+/ZGcn4AG4A1hNmgHw7eu8HwC3R8wzgVcKg+MVAtxLHfjs67hNKzGoqq87a9KjqNiXMzFkRPdbUtjatZHvmEn5TPEb4rbBP9H4W4T/sjcAzRItv15ZHVbcpYVbpEmBldI3+jGjmc215VLRNge8Qei6Wl3i0qe3XaVW3p67RSrXpA1GbLQeWAreerc6qemhHBBEREZEkoNujIiIiIklASZuIiIhIElDSJiIiIpIElLSJiIiIJAElbSIiIiJJQEmbiJwzMyuOtr1ZbWZvRutAVfV3XGFmvz/HY843s9cq8F3NzezLla0nmUTte3Gi4xCR8lPSJiIVccLdB7l7P8L6ZF9JdEBmlu7uO9z9zgoc3hz4a9JWiXqq1OnFO+PkCuCckrY4xyMin0NJm4hU1gJKbIhsZv9iZtlmttLMvl/i/e+a2Toze9fMXjazb0Tv/8XMsqLnrcwst/QXmNlwM5tvZsuiP3tG7z9kZq+a2ZvAO2bWxcxWR589F/UGLjezvWb2PTNrbGZ/MrOlZrbKzMZGX/EUcEFU9ulS9WSY2QtR+WVmdmWJ755tZm+Z2adm9uOyGsfMcs3sR2a2OHpcGL1/s5ktiup8z8zaRu8/aWaTzOwdYGoUy4dRzEtP945FPWUfmNksM1tvZk+Z2X3Rd6wyswuicq3N7PXo7yTbzC4xsy7AY8Dj0TlfWla5M8TTN/qO5dHfcfdzvmJEpEL0W5OIVJiZ1QGuBp6PXl9H2GpsOGHj5DlmdhmQD9wBDCb8v7OUsBJ7ea0DLnP3IjO7BvhhVB/ASGCAux+IkhEA3P3RKKbOwNvAFOAkcJu7H7Gwfc9CM5sDPAH0c/dB0TF/rYeoF9Hd+5tZL0Jy2CP6bFB0TgXAJ2b2C3ffVkb8R9x9uJlNAP6bsF3QR8BF7u5m9ijwTeCfo/JDgVHufsLMGgLXuvvJKEF6mbArAMBAoDeht3MT8Fz0Pf8IfA34X4RV7n/q7h+ZWSbwtrv3NrPfAMfc/SfROc8oXS6qu3Q8vwB+5u4vWdimp04Z5ysicaCkTUQqooGZLQe6EJKvd6P3r4sey6LXjQlJXBPgd+5+AiDqGTsXzYAXo6TFgbolPnvX3cvcuN3MTm9B81V332Jhf8UfRolkjNBD2PZzvnsU8AsAd19nZluA00nbn9z9cPRdHwOdgbKStpdL/PnT6HlH4BUzaw/UAzaXKD/ndFtF5/qMmQ0Cikt8N0C2h/04MbONwDvR+6uAK6Pn1wB9zOz0MU0t2re3lLOVKxmxk/NSAAACZUlEQVTPAuDbFjYgn+3un5ZRl4jEgW6PikhFnIh6pToTEo7TY9oM+M9ovNsgd7/Q3Z+P3j+TIv72f1HGGcr8G/DnaAzdzaXKHT9L3b8hJBbvRa/vA1oDQ6P4d5/lO087W+wFJZ4Xc+ZfhL2M578AnnH3/sCXOPM5PR7FOZDQw1bvDN8fK/E6ViKWNGBkib+TDu5+tIwYz1bur/G4+wzgFuAE8LaZXXWGcxaRKqakTUQqLOpl+jrwjagX623gETNrDGBmHcysDeFW4M3R+LDGwI0lqskl3H4DONPg/2bA9uj5Q+WJzcy+AjRx96dK1bPH3U9FY9M6R+8fJfQGlmUuIdkjui2aCXxSnhhKGFfizwUlYjl9Tg+e5dhmwE53jxE2qT7X25HvAF89/SLqsYPPnvOZyv0dM+sGbHL3nwNzgAHnGI+IVJCSNhGpFHdfBqwAxrv7O8AMYIGZrQJeIyRO2YQf8CuA2UAOcDiq4ifAP5jZfKDVGb7mx8B/mtk8yp+0fAPob3+bjPAY8BKQZWY5hERsXXQO+4F5FpYwebpUPb8C6kTn8wrwkLsXcG7qm9ki4B8JPWcATwKvmtmHwL6zHPsr4EEzW0i4NXq2nsWyfJ1wziujW7iPRe+/Cdx2eiLCWcqVNg5YHd0e7wVMPcd4RKSCzN0/v5SISCWZWWN3PxYNrJ8LTHT3pYmOK94szIbNcvezJWYiIp9LExFEpLpMMrM+hLFbL9aGhE1EpCqpp01EREQkCWhMm4iIiEgSUNImIiIikgSUtImIiIgkASVtIiIiIklASZuIiIhIElDSJiIiIpIE/j9WKcpU0F4IQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average loss versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average Training and Test Loss (Squared Hinge Loss with L2 Regularization)\")\n",
    "plt.plot(reg_params, train_ave_loss_L2, label=\"Training\")\n",
    "plt.plot(reg_params, test_ave_loss_L2, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy rate versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy Rate (Squared Hinge Loss with L2 Regularization)\")\n",
    "plt.plot(reg_params, train_acc_L2, label=\"Training\")\n",
    "plt.plot(reg_params, test_acc_L2, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"100 * Accuracy rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the Frobenius norm of the last weight matrix versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Frobenius Norm of the Last Weight Matrix (Squared Hinge Loss with L2 Regularization)\")\n",
    "plt.plot(reg_params, matrix_norm_L2)\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Frobenius norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax + MSE loss with l1 relularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "#create an array of regularization parameters from 0 to 0.03 with step size 0.001\n",
    "reg_params = np.linspace(0, 0.03, num=31, endpoint=True)\n",
    "\n",
    "#initiate a list storing training average loss for different reg_param\n",
    "train_ave_loss_MSE_L1= []\n",
    "\n",
    "#initiate a list storing test average loss for different reg_param\n",
    "test_ave_loss_MSE_L1 = []\n",
    "\n",
    "#initiate a list storing training accuracy for different reg_param\n",
    "train_acc_MSE_L1 = []\n",
    "\n",
    "#initiate a list storing test accuracy for different reg_param\n",
    "test_acc_MSE_L1 = []\n",
    "\n",
    "#initiate a list storing matrix norm for different reg_param\n",
    "matrix_norm_MSE_L1 = []\n",
    "\n",
    "#initiate a list of lists tracking the training loss for different reg_param\n",
    "listoflist_L1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ty367/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.000 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.089950\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.090005\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090032\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.089779\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.089903\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.089903\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.089663\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.089808\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089589\n",
      "4.27s\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089511\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.089427\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.089398\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.088976\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.089102\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.088815\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.088613\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.088074\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.087773\n",
      "4.33s\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.087620\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.082765\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.080002\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.067138\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.055752\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.037933\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.029477\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.027146\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.021117\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.018512\n",
      "4.36s\n",
      "\n",
      "Test set: Average loss: 0.2079, Accuracy: 8694/10000 (87%)\n",
      "\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.177347\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.152123\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.129194\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.111483\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.099018\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092179\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090122\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.089963\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090105\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089956\n",
      "4.10s\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089976\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090036\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090119\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089877\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090053\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090059\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090106\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090131\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090131\n",
      "4.09s\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089963\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090094\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090045\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090108\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.090016\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090066\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.090041\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089985\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090060\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.264743\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.172180\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.110189\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090434\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.090050\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.090239\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090217\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090069\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090214\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090065\n",
      "4.20s\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090077\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090142\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090223\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089977\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090162\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090162\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090210\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090237\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090236\n",
      "4.24s\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090063\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090200\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090152\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090107\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090215\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.090121\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090173\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.090145\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.090087\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090163\n",
      "3.77s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.352140\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.163079\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.090918\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090445\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.090228\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.090406\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090390\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090243\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090380\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090234\n",
      "4.32s\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.003 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090309\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090401\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090149\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090330\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090338\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090383\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090411\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090403\n",
      "4.31s\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090236\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090371\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090322\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090276\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090394\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.090296\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090339\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.090320\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.090268\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090330\n",
      "4.08s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.439537\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.139215\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.090632\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090684\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.090458\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.090649\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090610\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090492\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090616\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090464\n",
      "4.01s\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090485\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090552\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090624\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090373\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090569\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090571\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090621\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090642\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090646\n",
      "4.25s\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090479\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090606\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090572\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090515\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090617\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.090522\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090578\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.090552\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.090487\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090568\n",
      "3.77s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.526933\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.113000\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.090902\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090994\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.090737\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.090954\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090921\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090761\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090926\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090768\n",
      "4.51s\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090786\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090824\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090923\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090687\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090860\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090884\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090911\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090934\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090925\n",
      "4.08s\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090762\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090905\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090863\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090811\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090909\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.090796\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090868\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.090881\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.090775\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090873\n",
      "4.24s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.614330\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.097368\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.091295\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.091373\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.091117\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091322\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091317\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.091131\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091283\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091155\n",
      "4.31s\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.091185\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091224\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.091310\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091049\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.091236\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.091220\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.091275\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.091317\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.091299\n",
      "3.97s\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.091153\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.091270\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.091251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.006 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.091164\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.091292\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.091163\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.091234\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.091218\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.091160\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.091250\n",
      "4.25s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.701726\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.095813\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.091717\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.091791\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.091561\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091753\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091717\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.091547\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091733\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091634\n",
      "4.31s\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.091614\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091631\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.091760\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091446\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.091609\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.091710\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.091723\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.091718\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.091743\n",
      "4.19s\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.091542\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.091687\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.091640\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.091616\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.091750\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.091589\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.091660\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.091640\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.091526\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.091659\n",
      "4.33s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.789123\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.094966\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.092271\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.092280\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.092078\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092256\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.092235\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.092063\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.092255\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092068\n",
      "4.09s\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092074\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092104\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.092239\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091993\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.092124\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.092128\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.092225\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.092221\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.092282\n",
      "4.44s\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.092065\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.092208\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.092146\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.092091\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.092189\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.092090\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.092159\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.092109\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.092089\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.092112\n",
      "4.12s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.876519\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.094918\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.092859\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.092966\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.092703\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092852\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.092933\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.092636\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.092776\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092690\n",
      "4.00s\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092631\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092694\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.092752\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.092548\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.092766\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.092742\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.092729\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.092792\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.092755\n",
      "3.95s\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.092598\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.092701\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.092685\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.092656\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.092764\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.092662\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.092677\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.092679\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.092647\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.092682\n",
      "4.24s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.963916\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.095131\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.093563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.010 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.093552\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.093318\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.093542\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.093417\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.093245\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.093389\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093228\n",
      "4.23s\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.093393\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.093385\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.093404\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.093195\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.093315\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.093404\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.093370\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.093446\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.093491\n",
      "3.80s\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.093308\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.093378\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.093330\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.093334\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.093378\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.093294\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.093380\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.093331\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.093263\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.093361\n",
      "4.53s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.051312\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.095709\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.094176\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.094203\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.093965\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.094118\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.094088\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.093868\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.094077\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093916\n",
      "4.18s\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.093915\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.094048\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.094048\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.093830\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.093999\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.093960\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.094012\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.094061\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.094102\n",
      "4.42s\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.093926\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.093999\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.094026\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.094053\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.094048\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.093878\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.093997\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.093898\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.093923\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.094030\n",
      "4.02s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.138709\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.096328\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.094938\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.095037\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.094758\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.094922\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.094887\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.094653\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.094858\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.094727\n",
      "4.07s\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.094639\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.094793\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.094840\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.094499\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.094739\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.094663\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.094774\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.094715\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.094873\n",
      "4.29s\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.094605\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.094797\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.094768\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.094650\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.094652\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.094760\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.094791\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.094773\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.094638\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.094817\n",
      "4.31s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.226105\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.097156\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.095907\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.095929\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.095652\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.095816\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.095752\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.095537\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.095718\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.095539\n",
      "4.45s\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.095450\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.095468\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.095632\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.095241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.013 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.095593\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.095420\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.095599\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.095659\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.095698\n",
      "4.20s\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.095414\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.095613\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.095515\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.095566\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.095575\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.095508\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.095575\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.095561\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.095505\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.095607\n",
      "4.42s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.313502\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.098030\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.096839\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.096755\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.096410\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.096643\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.096630\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.096306\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.096592\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.096432\n",
      "4.01s\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.096360\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.096448\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.096495\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.096290\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.096433\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.096533\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.096591\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.096614\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.096496\n",
      "3.86s\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.096332\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.096484\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.096326\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.096470\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.096447\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.096430\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.096567\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.096437\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.096501\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.096407\n",
      "4.05s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.400898\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.099149\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.097851\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.097938\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097584\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.097808\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.097771\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.097484\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.097650\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.097420\n",
      "4.26s\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.097554\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.097519\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.097558\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.097301\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.097628\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.097569\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.097672\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.097616\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.097442\n",
      "4.37s\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.097286\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.097608\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.097417\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.097528\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.097524\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.097575\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.097313\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.097400\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.097275\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.097632\n",
      "4.08s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.488295\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.100097\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.098979\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.098826\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.098515\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.098616\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.098535\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.098512\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.098693\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.098337\n",
      "4.09s\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.098544\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.098570\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.098519\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.098301\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.098448\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.098446\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.098569\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.098515\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.098693\n",
      "4.51s\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.098479\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.098479\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.098348\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.098549\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.098562\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.098428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.016 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.098372\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.098412\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.098356\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.098359\n",
      "4.15s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.575692\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.101070\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.100015\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.099982\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.099560\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.099980\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.099480\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.099639\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.099662\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.099402\n",
      "3.99s\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.099673\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.099477\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.099597\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.099418\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.099541\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.099497\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.099642\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.099605\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.099582\n",
      "4.06s\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.099527\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.099362\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.099497\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.099680\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.099541\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.099412\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.099399\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.099278\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.099221\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.099625\n",
      "4.40s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.663088\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.102678\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.101537\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.101349\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.101025\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.101092\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.100669\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.100484\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.100785\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.100544\n",
      "4.14s\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.100900\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.100445\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.100723\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.100429\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.100550\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.101030\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.100932\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.100738\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.100871\n",
      "3.93s\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.100159\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.100567\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.100677\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.100790\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.100840\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.100854\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.100796\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.100735\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.100672\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.100541\n",
      "4.12s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.750485\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.104695\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.102764\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.102322\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.102351\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.102621\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.102270\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.102349\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.102220\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.101723\n",
      "4.19s\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.101804\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.102046\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.102048\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.101875\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.102123\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.102083\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.102250\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.101966\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.101981\n",
      "4.03s\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.101964\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.102004\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.101933\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.102196\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.102154\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.101961\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.101939\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.102049\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.101815\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.102063\n",
      "4.21s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.837881\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.105622\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.104196\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.103846\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.103642\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.103606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.020 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.103192\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.103200\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.103586\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.103076\n",
      "4.14s\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.103138\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.103160\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.102980\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.102922\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.103335\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.102876\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.103357\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.103178\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.103315\n",
      "4.20s\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.103114\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.103180\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.103140\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.103074\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.103223\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.103074\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.102961\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.103402\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.103090\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.103257\n",
      "4.17s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.925278\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.106914\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105726\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.105337\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.105004\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.104930\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.104543\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.104440\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.104796\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.104038\n",
      "4.09s\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.104922\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.104745\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.104799\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.104208\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.104748\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.104609\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.104746\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.104642\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.104718\n",
      "4.17s\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.104593\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.104794\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.104388\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.104617\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.104651\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.104372\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.104629\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.104693\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.104520\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.104663\n",
      "4.18s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.012674\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.108681\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.107324\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.106699\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.107146\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.106940\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.106713\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.106770\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.106482\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.106424\n",
      "4.17s\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.106214\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.106133\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.106334\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.106045\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.106358\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.105870\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.106498\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.106238\n",
      "Regularization parameter: 0.022 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.106523\n",
      "4.39s\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.105957\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.106011\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.105794\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.106049\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.106350\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.106137\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.106446\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.106334\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.105776\n",
      "Regularization parameter: 0.022 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.106742\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.100071\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.110563\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.108644\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.107793\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.108083\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.107535\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.107802\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.107216\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.107064\n",
      "Regularization parameter: 0.023 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.106955\n",
      "3.91s\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.107622\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.106972\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.107409\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.107582\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.107090\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.107150\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.107566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.023 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.107511\n",
      "Regularization parameter: 0.023 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.107273\n",
      "3.82s\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.107189\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.107361\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.107172\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.107317\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.107003\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.107134\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.107371\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.107438\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.107023\n",
      "Regularization parameter: 0.023 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.107815\n",
      "4.06s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.187467\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.112479\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.110710\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.109827\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.109656\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.109842\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.109673\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.109284\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.109255\n",
      "Regularization parameter: 0.024 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.109619\n",
      "4.24s\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.110004\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.108899\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.109719\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.109375\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.108947\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.109018\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.109375\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.109242\n",
      "Regularization parameter: 0.024 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.109015\n",
      "4.30s\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.108964\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.109058\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.108702\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.109365\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.109459\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.108996\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.109178\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.109324\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.109505\n",
      "Regularization parameter: 0.024 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.109537\n",
      "4.32s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.274864\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.114593\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.112148\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.111953\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.111882\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.111042\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.111268\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.111039\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.111449\n",
      "Regularization parameter: 0.025 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.111532\n",
      "3.98s\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.111246\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.111263\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.111240\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.110543\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.111039\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.110785\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.111306\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.111196\n",
      "Regularization parameter: 0.025 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.110772\n",
      "4.13s\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.110782\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.110719\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.110536\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.111183\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.110359\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.111001\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.110734\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.110712\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.111047\n",
      "Regularization parameter: 0.025 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.110723\n",
      "4.25s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.362261\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.115385\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.113592\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.113778\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.112999\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.113198\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.112746\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.111976\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.112842\n",
      "Regularization parameter: 0.026 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.112652\n",
      "4.32s\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.113023\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.112177\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.112891\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.112249\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.112907\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.112980\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.112918\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.112384\n",
      "Regularization parameter: 0.026 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.112794\n",
      "3.93s\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.112379\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.112331\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.111999\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.112360\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.112588\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.112773\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.112466\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.112717\n",
      "Regularization parameter: 0.026 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.112244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.026 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.112676\n",
      "3.70s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.449657\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.117652\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.115772\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.115207\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.115003\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.114978\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114451\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.114448\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.114359\n",
      "Regularization parameter: 0.027 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.114339\n",
      "4.12s\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.114522\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.114043\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.114367\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.113980\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.114661\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.114214\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.114302\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.114257\n",
      "Regularization parameter: 0.027 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.114072\n",
      "4.38s\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.114138\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.113596\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.113957\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.114056\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.114058\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.114097\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.114443\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.113728\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.113995\n",
      "Regularization parameter: 0.027 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.113945\n",
      "4.23s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.537054\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.119763\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.117330\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.117378\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.116350\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.116236\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.116521\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.116151\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.116411\n",
      "Regularization parameter: 0.028 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.115884\n",
      "4.21s\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.116085\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.115852\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.115786\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.115840\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.115362\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.116300\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.115476\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.115694\n",
      "Regularization parameter: 0.028 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.115647\n",
      "3.90s\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.115768\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.115335\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.115836\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.116546\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.115805\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.115594\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.115824\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.116017\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.115315\n",
      "Regularization parameter: 0.028 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.115769\n",
      "3.99s\n",
      "\n",
      "Test set: Average loss: 0.8989, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.624450\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.122872\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.119980\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.118834\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.119477\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.119282\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.118869\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.117695\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.118214\n",
      "Regularization parameter: 0.029 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.117917\n",
      "4.05s\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.117983\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.118409\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.117260\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.118167\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.117885\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.117061\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.117756\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.117371\n",
      "Regularization parameter: 0.029 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.117238\n",
      "4.08s\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.117802\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.118222\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.116529\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.118574\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.118119\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.117817\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.117722\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.117543\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.117686\n",
      "Regularization parameter: 0.029 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.117557\n",
      "4.31s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.711847\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.124189\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.121844\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.121237\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.120735\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.121341\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.120453\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.119848\n",
      "Regularization parameter: 0.030 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.120719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.030 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.120114\n",
      "4.23s\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.119996\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.119332\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.120348\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.119009\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.119055\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.119531\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.119755\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.119836\n",
      "Regularization parameter: 0.030 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.119609\n",
      "4.36s\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.120322\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.118888\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.119354\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.119854\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.119841\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.119305\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.119335\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.120494\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.119135\n",
      "Regularization parameter: 0.030 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.119935\n",
      "4.24s\n",
      "\n",
      "Test set: Average loss: 0.8990, Accuracy: 1010/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_param in reg_params:\n",
    "    model = NetSeq().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    model.train() #training mode\n",
    "    iteration = 0\n",
    "    a_list = [] #tracking the loss function value\n",
    "    for ep in range(epoch):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            #compute the L1 norm of weight matrix on the last layer \n",
    "            l1_norm = torch.norm(model.fc_layers[-1].weight, p=1)\n",
    "            \n",
    "            #softmax for MSE\n",
    "            m = nn.Softmax(dim=1)\n",
    "            output_softmax = m(output)\n",
    "            target_onehot = torch.FloatTensor(target.size()[0], output.size()[1]).to(device)\n",
    "            target_onehot.zero_()\n",
    "            target_onehot.scatter_(1, target.view(-1,1), 1)\n",
    "\n",
    "#             print('output_softmax', output_softmax.size())\n",
    "#             print('target_onehot', target_onehot.size())\n",
    "            \n",
    "            #compute loss\n",
    "            loss_f = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "            loss = loss_f(output_softmax, target_onehot) + reg_param*l1_norm\n",
    "\n",
    "            a_list.append(loss.item())\n",
    "            \n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Regularization parameter: {:.3f} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    reg_param, ep+1, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "    \n",
    "    result_train = result_hinge(trainset_loader)\n",
    "    result_test = result_hinge(testset_loader)\n",
    "    train_ave_loss_MSE_L1.append(result_train[0])\n",
    "    test_ave_loss_MSE_L1.append(result_test[0])\n",
    "    train_acc_MSE_L1.append(result_train[1])\n",
    "    test_acc_MSE_L1.append(result_test[1])\n",
    "    matrix_norm_MSE_L1.append(np.linalg.norm(model.fc_layers[-1].weight.cpu().detach().numpy()))\n",
    "    listoflist_L1.append(list(a_list))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        result_test[0], result_test[2], result_test[3],\n",
    "        result_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 6, 2, 7, 1, 6, 2, 7, 0, 6, 9, 7, 7, 3, 6, 7, 0, 6, 5, 1, 3, 3, 2, 0,\n",
       "        8, 3, 4, 8, 2, 4, 9, 2, 6, 0, 8, 6, 9, 7, 3, 0, 1, 3, 6, 2, 9, 0, 5, 5,\n",
       "        4, 6, 8, 4, 3, 8, 8, 7, 3, 3, 7, 8, 9, 3, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average loss versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average Training and Test Loss (MSE Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params, train_ave_loss_MSE_L1, label=\"Training\")\n",
    "plt.plot(reg_params, test_ave_loss_MSE_L1, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy rate versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy Rate (MSE Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params, train_acc_MSE_L1, label=\"Training\")\n",
    "plt.plot(reg_params, test_acc_MSE_L1, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"100 * Accuracy rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the Frobenius norm of the last weight matrix versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Frobenius Norm of the Last Weight Matrix (MSE Loss with L1 Regularization)\")\n",
    "plt.plot(reg_params, matrix_norm_MSE_L1)\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Frobenius norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Softmax + MSE loss with l2 relularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "#create an array of regularization parameters from 0 to 0.03 with step size 0.001\n",
    "reg_params = np.linspace(0, 0.03, num=31, endpoint=True)\n",
    "\n",
    "#initiate a list storing training average loss for different reg_param\n",
    "train_ave_loss_MSE_L2= []\n",
    "\n",
    "#initiate a list storing test average loss for different reg_param\n",
    "test_ave_loss_MSE_L2 = []\n",
    "\n",
    "#initiate a list storing training accuracy for different reg_param\n",
    "train_acc_MSE_L2 = []\n",
    "\n",
    "#initiate a list storing test accuracy for different reg_param\n",
    "test_acc_MSE_L2 = []\n",
    "\n",
    "#initiate a list storing matrix norm for different reg_param\n",
    "matrix_norm_MSE_L2 = []\n",
    "\n",
    "#initiate a list of lists tracking the training loss for different reg_param\n",
    "listoflist_L2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.000 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.089950\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.090005\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090032\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.089779\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.089903\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.089903\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.089663\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.089808\n",
      "Regularization parameter: 0.000 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089589\n",
      "4.27s\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089511\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.089427\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.089398\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.088976\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.089102\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.088815\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.088613\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.088074\n",
      "Regularization parameter: 0.000 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.087773\n",
      "4.16s\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.087620\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.082765\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.080002\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.067139\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.055752\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.037931\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.029480\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.027138\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.021114\n",
      "Regularization parameter: 0.000 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.018508\n",
      "4.08s\n",
      "\n",
      "Test set: Average loss: 0.2078, Accuracy: 8690/10000 (87%)\n",
      "\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.093313\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.093244\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.093121\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.093033\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.092669\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092693\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.092590\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.092269\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.092325\n",
      "Regularization parameter: 0.001 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092038\n",
      "4.40s\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.091901\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091791\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.091733\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091310\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.091420\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.091238\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.091151\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090880\n",
      "Regularization parameter: 0.001 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090792\n",
      "4.15s\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090668\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.089727\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089473\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.088446\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.088085\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.083909\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.079422\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.075364\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.064281\n",
      "Regularization parameter: 0.001 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.048122\n",
      "4.42s\n",
      "\n",
      "Test set: Average loss: 0.1484, Accuracy: 8065/10000 (81%)\n",
      "\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.096677\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.096253\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.095777\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.095372\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.094720\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.094488\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.094148\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.093624\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.093488\n",
      "Regularization parameter: 0.002 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093040\n",
      "4.49s\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092762\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092541\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.092379\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091881\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.091904\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.091695\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.091575\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.091343\n",
      "Regularization parameter: 0.002 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.091236\n",
      "3.99s\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.091026\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090662\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090523\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090231\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090228\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089468\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089105\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.088862\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089140\n",
      "Regularization parameter: 0.002 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.086694\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.6349, Accuracy: 3485/10000 (35%)\n",
      "\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.100040\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.099040\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.098021\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.097156\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.096111\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.095552\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.094929\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.094178\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.093844\n",
      "Regularization parameter: 0.003 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093240\n",
      "4.64s\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.003 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092523\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.092282\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091735\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.091702\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.091476\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.091335\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.091127\n",
      "Regularization parameter: 0.003 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.091010\n",
      "4.15s\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090770\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090589\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090459\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090292\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090318\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089961\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089873\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089805\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089899\n",
      "Regularization parameter: 0.003 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089369\n",
      "4.19s\n",
      "\n",
      "Test set: Average loss: 0.7951, Accuracy: 3116/10000 (31%)\n",
      "\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.103403\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.101616\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.099897\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.098476\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.096990\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.096086\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.095190\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.094237\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.093744\n",
      "Regularization parameter: 0.004 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.093028\n",
      "4.30s\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092550\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.092187\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.091915\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.091361\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.091314\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.091101\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090971\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090800\n",
      "Regularization parameter: 0.004 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090697\n",
      "4.39s\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090462\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090376\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090265\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090148\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090198\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089972\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089945\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089911\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089935\n",
      "Regularization parameter: 0.004 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089730\n",
      "4.28s\n",
      "\n",
      "Test set: Average loss: 0.8467, Accuracy: 2608/10000 (26%)\n",
      "\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.106766\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.103993\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.101447\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.099411\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097472\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.096244\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.095115\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.094008\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.093409\n",
      "Regularization parameter: 0.005 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092635\n",
      "4.39s\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.092130\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091762\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.091499\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090968\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090938\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090759\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090658\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090528\n",
      "Regularization parameter: 0.005 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090450\n",
      "4.37s\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090235\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090211\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090122\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.090035\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090105\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089940\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089941\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089922\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.005 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089832\n",
      "3.95s\n",
      "\n",
      "Test set: Average loss: 0.8675, Accuracy: 1916/10000 (19%)\n",
      "\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.110129\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.106180\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.102706\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.100029\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097655\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.096143\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.094834\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.093627\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.092976\n",
      "Regularization parameter: 0.006 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092189\n",
      "4.65s\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.091696\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091355\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.091125\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090636\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090638\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090498\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090431\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090340\n",
      "Regularization parameter: 0.006 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090287\n",
      "4.30s\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090092\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090111\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.090039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.006 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089973\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090056\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089925\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089942\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089929\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089914\n",
      "Regularization parameter: 0.006 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089881\n",
      "4.50s\n",
      "\n",
      "Test set: Average loss: 0.8776, Accuracy: 1342/10000 (13%)\n",
      "\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.113492\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.108187\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.103708\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.100387\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097614\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.095872\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.094437\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.093182\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.092523\n",
      "Regularization parameter: 0.007 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091758\n",
      "4.45s\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.091304\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.091007\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090824\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090382\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090418\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090317\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090280\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090221\n",
      "Regularization parameter: 0.007 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090189\n",
      "4.09s\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.090010\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090058\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089998\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089944\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090035\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089923\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089948\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089938\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089915\n",
      "Regularization parameter: 0.007 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089912\n",
      "4.21s\n",
      "\n",
      "Test set: Average loss: 0.8832, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.116856\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.110023\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.104484\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.100535\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097410\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.095493\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.093987\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.092727\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.092095\n",
      "Regularization parameter: 0.008 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091375\n",
      "4.14s\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090975\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090730\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090595\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090197\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090267\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090197\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090185\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090150\n",
      "Regularization parameter: 0.008 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090134\n",
      "3.99s\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089965\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090033\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089981\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089933\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090029\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089927\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089958\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089948\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089919\n",
      "Regularization parameter: 0.008 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089934\n",
      "4.37s\n",
      "\n",
      "Test set: Average loss: 0.8867, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.120219\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.111698\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105060\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.100514\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097092\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.095055\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.093524\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.092294\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091712\n",
      "Regularization parameter: 0.009 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091052\n",
      "4.44s\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090710\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090518\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090427\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.090069\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090166\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090121\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090128\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090110\n",
      "Regularization parameter: 0.009 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090104\n",
      "4.23s\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089941\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090024\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089975\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089931\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090030\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089934\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089968\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089957\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089923\n",
      "Regularization parameter: 0.009 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089950\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.8891, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.123582\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.113220\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.010 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.100359\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.096698\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.094592\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.093075\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.091900\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091383\n",
      "Regularization parameter: 0.010 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090787\n",
      "4.23s\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090504\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090360\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090309\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089982\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090102\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090075\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090095\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090089\n",
      "Regularization parameter: 0.010 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090089\n",
      "4.05s\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089929\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090021\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089975\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089933\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090033\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089940\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089977\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089964\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089926\n",
      "Regularization parameter: 0.010 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089962\n",
      "4.31s\n",
      "\n",
      "Test set: Average loss: 0.8907, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.126945\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.114596\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105709\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.100101\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.096256\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.094128\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.092655\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.091553\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091107\n",
      "Regularization parameter: 0.011 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090576\n",
      "4.06s\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090348\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090246\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090227\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089925\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090061\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090047\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090076\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090078\n",
      "Regularization parameter: 0.011 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090081\n",
      "4.20s\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089924\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090022\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089977\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089936\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090037\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089946\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089984\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089970\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089929\n",
      "Regularization parameter: 0.011 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089971\n",
      "4.39s\n",
      "\n",
      "Test set: Average loss: 0.8919, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.130308\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.115836\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105825\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.099763\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.095790\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.093678\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.092273\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.091253\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090881\n",
      "Regularization parameter: 0.012 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090411\n",
      "4.02s\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090230\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090165\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090172\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089888\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090036\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090031\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090066\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090073\n",
      "Regularization parameter: 0.012 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090079\n",
      "4.10s\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089922\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090025\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089980\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089939\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090041\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089950\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089990\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089975\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089932\n",
      "Regularization parameter: 0.012 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089978\n",
      "4.47s\n",
      "\n",
      "Test set: Average loss: 0.8928, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.133671\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.116946\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105826\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.099368\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.095316\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.093253\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091933\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090999\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090698\n",
      "Regularization parameter: 0.013 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090283\n",
      "4.50s\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090144\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090107\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090135\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.013 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090021\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090022\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090061\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090072\n",
      "Regularization parameter: 0.013 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090078\n",
      "4.09s\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090028\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089983\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089942\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090044\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089954\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089994\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089978\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089933\n",
      "Regularization parameter: 0.013 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089983\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.8936, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.137034\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.117934\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105729\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.098932\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.094847\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092860\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091634\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090787\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090553\n",
      "Regularization parameter: 0.014 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090186\n",
      "4.24s\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090082\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090068\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090110\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089851\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090012\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090017\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090058\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090072\n",
      "Regularization parameter: 0.014 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090078\n",
      "4.14s\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090030\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089985\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089944\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090047\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089957\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.089998\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089981\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089935\n",
      "Regularization parameter: 0.014 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089988\n",
      "4.40s\n",
      "\n",
      "Test set: Average loss: 0.8942, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.140398\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.118806\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105547\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.098469\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.094393\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092501\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091375\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090612\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090438\n",
      "Regularization parameter: 0.015 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090113\n",
      "3.90s\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090037\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090041\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090095\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089842\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090007\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090014\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090057\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090073\n",
      "Regularization parameter: 0.015 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090079\n",
      "4.68s\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090033\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089988\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089946\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090049\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089959\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090001\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089984\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089936\n",
      "Regularization parameter: 0.015 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089991\n",
      "5.17s\n",
      "\n",
      "Test set: Average loss: 0.8946, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.143761\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.119570\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.105295\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.097991\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.093959\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092177\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.091153\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090469\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090349\n",
      "Regularization parameter: 0.016 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090059\n",
      "4.51s\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.090005\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090023\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090085\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089837\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090005\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090013\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090057\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090074\n",
      "Regularization parameter: 0.016 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090080\n",
      "4.39s\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090035\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089990\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089948\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090051\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.016 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090004\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089986\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089937\n",
      "Regularization parameter: 0.016 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089994\n",
      "4.10s\n",
      "\n",
      "Test set: Average loss: 0.8950, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.147124\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.120231\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.104983\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.097508\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.093550\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091888\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090965\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090353\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090279\n",
      "Regularization parameter: 0.017 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090019\n",
      "4.19s\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089982\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090011\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090079\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089834\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090013\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090058\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090075\n",
      "Regularization parameter: 0.017 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090081\n",
      "4.65s\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090037\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089991\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089950\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090053\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089963\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090006\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089987\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089938\n",
      "Regularization parameter: 0.017 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089996\n",
      "4.09s\n",
      "\n",
      "Test set: Average loss: 0.8954, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.150487\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.120795\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.104622\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.097027\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.093169\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091632\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090805\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090260\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090226\n",
      "Regularization parameter: 0.018 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089989\n",
      "4.31s\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089966\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090075\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089832\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090013\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090058\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090076\n",
      "Regularization parameter: 0.018 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090082\n",
      "4.63s\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089921\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090039\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089993\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089951\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090054\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089964\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090008\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089989\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089939\n",
      "Regularization parameter: 0.018 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.089998\n",
      "4.12s\n",
      "\n",
      "Test set: Average loss: 0.8957, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.153850\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.121269\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.104221\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.096554\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.092817\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091407\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090672\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090185\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090185\n",
      "Regularization parameter: 0.019 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089968\n",
      "4.08s\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089955\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.089998\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090073\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089832\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090014\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090059\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090078\n",
      "Regularization parameter: 0.019 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090082\n",
      "4.31s\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089922\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090040\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089994\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089952\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090056\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089966\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090010\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089990\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089939\n",
      "Regularization parameter: 0.019 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090000\n",
      "4.15s\n",
      "\n",
      "Test set: Average loss: 0.8959, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.157213\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.121658\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.103788\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.096093\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.092493\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter: 0.020 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090561\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090126\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090154\n",
      "Regularization parameter: 0.020 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089952\n",
      "4.42s\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089948\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.089994\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090072\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089831\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090003\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090014\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090059\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090079\n",
      "Regularization parameter: 0.020 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090083\n",
      "4.15s\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089922\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090042\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089995\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089953\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090057\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089967\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090011\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089991\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089940\n",
      "Regularization parameter: 0.020 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090002\n",
      "4.31s\n",
      "\n",
      "Test set: Average loss: 0.8961, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.160576\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.121966\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.103330\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.095649\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.092198\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.091041\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.090469\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090079\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.090131\n",
      "Regularization parameter: 0.021 Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.089941\n",
      "4.23s\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.089943\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.089993\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.090071\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.089831\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.090004\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.090014\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.090060\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.090080\n",
      "Regularization parameter: 0.021 Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.090084\n",
      "4.13s\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [1536/60000 (3%)]\tLoss: 0.089922\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [7936/60000 (13%)]\tLoss: 0.090043\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [14336/60000 (24%)]\tLoss: 0.089996\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [20736/60000 (35%)]\tLoss: 0.089954\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [27136/60000 (45%)]\tLoss: 0.090058\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [33536/60000 (56%)]\tLoss: 0.089968\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [39936/60000 (67%)]\tLoss: 0.090013\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [46336/60000 (77%)]\tLoss: 0.089992\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [52736/60000 (88%)]\tLoss: 0.089940\n",
      "Regularization parameter: 0.021 Train Epoch: 3 [59136/60000 (99%)]\tLoss: 0.090003\n",
      "4.13s\n",
      "\n",
      "Test set: Average loss: 0.8963, Accuracy: 1010/10000 (10%)\n",
      "\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.163940\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.122200\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.102854\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.095223\n",
      "Regularization parameter: 0.022 Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.091930\n"
     ]
    }
   ],
   "source": [
    "for reg_param in reg_params:\n",
    "    model = NetSeq().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    model.train() #training mode\n",
    "    iteration = 0\n",
    "    a_list = [] #tracking the loss function value\n",
    "    for ep in range(epoch):\n",
    "        start = time()\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "                       #compute the L2 norm of weight matrix on the last layer \n",
    "            l2_norm = torch.norm(model.fc_layers[-1].weight)**2\n",
    "            \n",
    "            #softmax for MSE\n",
    "            m = nn.Softmax(dim=1)\n",
    "            output_softmax = m(output)\n",
    "            target_onehot = torch.FloatTensor(target.size()[0], output.size()[1]).to(device)\n",
    "            target_onehot.zero_()\n",
    "            target_onehot.scatter_(1, target.view(-1,1), 1)\n",
    "\n",
    "#             print('output_softmax', output_softmax.size())\n",
    "#             print('target_onehot', target_onehot.size())\n",
    "            \n",
    "            #compute loss\n",
    "            loss_f = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "            loss = loss_f(output_softmax, target_onehot) + reg_param*l2_norm\n",
    "\n",
    "            a_list.append(loss.item())\n",
    "            \n",
    "            #backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Regularization parameter: {:.3f} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    reg_param, ep+1, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        end = time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "    \n",
    "    result_train = result_hinge(trainset_loader)\n",
    "    result_test = result_hinge(testset_loader)\n",
    "    train_ave_loss_MSE_L2.append(result_train[0])\n",
    "    test_ave_loss_MSE_L2.append(result_test[0])\n",
    "    train_acc_MSE_L2.append(result_train[1])\n",
    "    test_acc_MSE_L2.append(result_test[1])\n",
    "    matrix_norm_MSE_L2.append(np.linalg.norm(model.fc_layers[-1].cpu().weight.detach().numpy()))\n",
    "    listoflist_L2.append(list(a_list))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        result_test[0], result_test[2], result_test[3],\n",
    "        result_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average loss versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average Training and Test Loss (MSE Loss with L2 Regularization)\")\n",
    "plt.plot(reg_params, train_ave_loss_MSE_L2, label=\"Training\")\n",
    "plt.plot(reg_params, test_ave_loss_MSE_L2, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy rate versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy Rate (MSE Loss with L_2 Regularization)\")\n",
    "plt.plot(reg_params, train_acc_MSE_L2, label=\"Training\")\n",
    "plt.plot(reg_params, test_acc_MSE_L2, label=\"Test\")\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"100 * Accuracy rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the Frobenius norm of the last weight matrix versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Frobenius Norm of the Last Weight Matrix (MSE Loss with L_2 Regularization)\")\n",
    "plt.plot(reg_params, matrix_norm_MSE_L2)\n",
    "plt.xlabel(\"Regularization parameters\")\n",
    "plt.ylabel(\"Frobenius norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy loss + early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "def create_datasets(batch_size):\n",
    "\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # choose the training and test datasets\n",
    "    train_data = datasets.MNIST(root='./mnist_png/training', \n",
    "                                train=True,\n",
    "                                download=True, \n",
    "                                transform=transform)\n",
    "\n",
    "    test_data = datasets.MNIST(root='./mnist_png/testing',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transform)\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load test data in batches\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=0)\n",
    "    \n",
    "    return train_loader, test_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetSeq()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_par = np.linspace(1, 20, num=20, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "       14., 15., 16., 17., 18., 19., 20.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patience_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(train_loader, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate a list storing training average loss for different reg_param\n",
    "train_loss_early= []\n",
    "\n",
    "#initiate a list storing test average loss for different reg_param\n",
    "test_loss_early = []\n",
    "\n",
    "#initiate a list storing training accuracy for different reg_param\n",
    "train_acc_early = []\n",
    "\n",
    "#initiate a list storing test accuracy for different reg_param\n",
    "test_acc_early = []\n",
    "\n",
    "#initiate a list storing matrix norm for different reg_param\n",
    "#matrix_norm_MSE_L2 = []\n",
    "\n",
    "#initiate a list of lists tracking the training loss for different reg_param\n",
    "listoflist_L2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 1.76121 valid_loss: 0.49268\n",
      "Validation loss decreased (inf --> 0.492682).  Saving model ...\n",
      "[ 2/30] train_loss: 0.34056 valid_loss: 0.28238\n",
      "Validation loss decreased (0.492682 --> 0.282381).  Saving model ...\n",
      "[ 3/30] train_loss: 0.22250 valid_loss: 0.21386\n",
      "Validation loss decreased (0.282381 --> 0.213862).  Saving model ...\n",
      "[ 4/30] train_loss: 0.16524 valid_loss: 0.16211\n",
      "Validation loss decreased (0.213862 --> 0.162110).  Saving model ...\n",
      "[ 5/30] train_loss: 0.13258 valid_loss: 0.12775\n",
      "Validation loss decreased (0.162110 --> 0.127752).  Saving model ...\n",
      "[ 6/30] train_loss: 0.11112 valid_loss: 0.11681\n",
      "Validation loss decreased (0.127752 --> 0.116813).  Saving model ...\n",
      "[ 7/30] train_loss: 0.09733 valid_loss: 0.10799\n",
      "Validation loss decreased (0.116813 --> 0.107988).  Saving model ...\n",
      "[ 8/30] train_loss: 0.08769 valid_loss: 0.08896\n",
      "Validation loss decreased (0.107988 --> 0.088961).  Saving model ...\n",
      "[ 9/30] train_loss: 0.07970 valid_loss: 0.08386\n",
      "Validation loss decreased (0.088961 --> 0.083863).  Saving model ...\n",
      "[10/30] train_loss: 0.07377 valid_loss: 0.08055\n",
      "Validation loss decreased (0.083863 --> 0.080545).  Saving model ...\n",
      "[11/30] train_loss: 0.06760 valid_loss: 0.07689\n",
      "Validation loss decreased (0.080545 --> 0.076892).  Saving model ...\n",
      "[12/30] train_loss: 0.06474 valid_loss: 0.07228\n",
      "Validation loss decreased (0.076892 --> 0.072277).  Saving model ...\n",
      "[13/30] train_loss: 0.05950 valid_loss: 0.07161\n",
      "Validation loss decreased (0.072277 --> 0.071612).  Saving model ...\n",
      "[14/30] train_loss: 0.05696 valid_loss: 0.06857\n",
      "Validation loss decreased (0.071612 --> 0.068574).  Saving model ...\n",
      "[15/30] train_loss: 0.05403 valid_loss: 0.06961\n",
      "EarlyStopping counter: 1 out of 1.0\n",
      "Early stopping\n",
      "class_correct [972.0, 1125.0, 1017.0, 993.0, 959.0, 882.0, 938.0, 1005.0, 951.0, 974.0]\n",
      "Test Loss: 0.054242\n",
      "\n",
      "Test Accuracy of     0: 99% (972/980)\n",
      "Test Accuracy of     1: 99% (1125/1135)\n",
      "Test Accuracy of     2: 98% (1017/1032)\n",
      "Test Accuracy of     3: 98% (993/1010)\n",
      "Test Accuracy of     4: 97% (959/982)\n",
      "Test Accuracy of     5: 98% (882/892)\n",
      "Test Accuracy of     6: 97% (938/958)\n",
      "Test Accuracy of     7: 97% (1005/1028)\n",
      "Test Accuracy of     8: 97% (951/974)\n",
      "Test Accuracy of     9: 96% (974/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9816/10000)\n",
      "[ 1/30] train_loss: 0.05638 valid_loss: 0.06335\n",
      "Validation loss decreased (inf --> 0.063349).  Saving model ...\n",
      "[ 2/30] train_loss: 0.05338 valid_loss: 0.05822\n",
      "Validation loss decreased (0.063349 --> 0.058216).  Saving model ...\n",
      "[ 3/30] train_loss: 0.04954 valid_loss: 0.05558\n",
      "Validation loss decreased (0.058216 --> 0.055583).  Saving model ...\n",
      "[ 4/30] train_loss: 0.04764 valid_loss: 0.05492\n",
      "Validation loss decreased (0.055583 --> 0.054918).  Saving model ...\n",
      "[ 5/30] train_loss: 0.04499 valid_loss: 0.05465\n",
      "Validation loss decreased (0.054918 --> 0.054645).  Saving model ...\n",
      "[ 6/30] train_loss: 0.04235 valid_loss: 0.05734\n",
      "EarlyStopping counter: 1 out of 2.0\n",
      "[ 7/30] train_loss: 0.04119 valid_loss: 0.05998\n",
      "EarlyStopping counter: 2 out of 2.0\n",
      "Early stopping\n",
      "class_correct [973.0, 1128.0, 1023.0, 987.0, 975.0, 883.0, 940.0, 1013.0, 949.0, 969.0]\n",
      "Test Loss: 0.048461\n",
      "\n",
      "Test Accuracy of     0: 99% (973/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 99% (1023/1032)\n",
      "Test Accuracy of     3: 97% (987/1010)\n",
      "Test Accuracy of     4: 99% (975/982)\n",
      "Test Accuracy of     5: 98% (883/892)\n",
      "Test Accuracy of     6: 98% (940/958)\n",
      "Test Accuracy of     7: 98% (1013/1028)\n",
      "Test Accuracy of     8: 97% (949/974)\n",
      "Test Accuracy of     9: 96% (969/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9840/10000)\n",
      "[ 1/30] train_loss: 0.04662 valid_loss: 0.04432\n",
      "Validation loss decreased (inf --> 0.044318).  Saving model ...\n",
      "[ 2/30] train_loss: 0.04348 valid_loss: 0.04124\n",
      "Validation loss decreased (0.044318 --> 0.041236).  Saving model ...\n",
      "[ 3/30] train_loss: 0.04221 valid_loss: 0.05007\n",
      "EarlyStopping counter: 1 out of 3.0\n",
      "[ 4/30] train_loss: 0.03960 valid_loss: 0.04022\n",
      "Validation loss decreased (0.041236 --> 0.040223).  Saving model ...\n",
      "[ 5/30] train_loss: 0.03707 valid_loss: 0.04238\n",
      "EarlyStopping counter: 1 out of 3.0\n",
      "[ 6/30] train_loss: 0.03662 valid_loss: 0.04187\n",
      "EarlyStopping counter: 2 out of 3.0\n",
      "[ 7/30] train_loss: 0.03396 valid_loss: 0.04040\n",
      "EarlyStopping counter: 3 out of 3.0\n",
      "Early stopping\n",
      "class_correct [976.0, 1130.0, 1024.0, 990.0, 971.0, 870.0, 945.0, 1014.0, 954.0, 976.0]\n",
      "Test Loss: 0.043673\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1130/1135)\n",
      "Test Accuracy of     2: 99% (1024/1032)\n",
      "Test Accuracy of     3: 98% (990/1010)\n",
      "Test Accuracy of     4: 98% (971/982)\n",
      "Test Accuracy of     5: 97% (870/892)\n",
      "Test Accuracy of     6: 98% (945/958)\n",
      "Test Accuracy of     7: 98% (1014/1028)\n",
      "Test Accuracy of     8: 97% (954/974)\n",
      "Test Accuracy of     9: 96% (976/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9850/10000)\n",
      "[ 1/30] train_loss: 0.03952 valid_loss: 0.03871\n",
      "Validation loss decreased (inf --> 0.038708).  Saving model ...\n",
      "[ 2/30] train_loss: 0.03763 valid_loss: 0.03178\n",
      "Validation loss decreased (0.038708 --> 0.031785).  Saving model ...\n",
      "[ 3/30] train_loss: 0.03581 valid_loss: 0.03474\n",
      "EarlyStopping counter: 1 out of 4.0\n",
      "[ 4/30] train_loss: 0.03396 valid_loss: 0.03471\n",
      "EarlyStopping counter: 2 out of 4.0\n",
      "[ 5/30] train_loss: 0.03336 valid_loss: 0.03367\n",
      "EarlyStopping counter: 3 out of 4.0\n",
      "[ 6/30] train_loss: 0.03104 valid_loss: 0.03690\n",
      "EarlyStopping counter: 4 out of 4.0\n",
      "Early stopping\n",
      "class_correct [976.0, 1128.0, 1022.0, 997.0, 973.0, 878.0, 941.0, 1015.0, 957.0, 978.0]\n",
      "Test Loss: 0.038818\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 99% (1022/1032)\n",
      "Test Accuracy of     3: 98% (997/1010)\n",
      "Test Accuracy of     4: 99% (973/982)\n",
      "Test Accuracy of     5: 98% (878/892)\n",
      "Test Accuracy of     6: 98% (941/958)\n",
      "Test Accuracy of     7: 98% (1015/1028)\n",
      "Test Accuracy of     8: 98% (957/974)\n",
      "Test Accuracy of     9: 96% (978/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9865/10000)\n",
      "[ 1/30] train_loss: 0.03629 valid_loss: 0.03708\n",
      "Validation loss decreased (inf --> 0.037080).  Saving model ...\n",
      "[ 2/30] train_loss: 0.03487 valid_loss: 0.03618\n",
      "Validation loss decreased (0.037080 --> 0.036185).  Saving model ...\n",
      "[ 3/30] train_loss: 0.03322 valid_loss: 0.03934\n",
      "EarlyStopping counter: 1 out of 5.0\n",
      "[ 4/30] train_loss: 0.03183 valid_loss: 0.03169\n",
      "Validation loss decreased (0.036185 --> 0.031688).  Saving model ...\n",
      "[ 5/30] train_loss: 0.03109 valid_loss: 0.03274\n",
      "EarlyStopping counter: 1 out of 5.0\n",
      "[ 6/30] train_loss: 0.02929 valid_loss: 0.03335\n",
      "EarlyStopping counter: 2 out of 5.0\n",
      "[ 7/30] train_loss: 0.02779 valid_loss: 0.03502\n",
      "EarlyStopping counter: 3 out of 5.0\n",
      "[ 8/30] train_loss: 0.02622 valid_loss: 0.03266\n",
      "EarlyStopping counter: 4 out of 5.0\n",
      "[ 9/30] train_loss: 0.02587 valid_loss: 0.03392\n",
      "EarlyStopping counter: 5 out of 5.0\n",
      "Early stopping\n",
      "class_correct [974.0, 1128.0, 1027.0, 1001.0, 973.0, 876.0, 937.0, 1010.0, 956.0, 983.0]\n",
      "Test Loss: 0.040442\n",
      "\n",
      "Test Accuracy of     0: 99% (974/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 99% (1027/1032)\n",
      "Test Accuracy of     3: 99% (1001/1010)\n",
      "Test Accuracy of     4: 99% (973/982)\n",
      "Test Accuracy of     5: 98% (876/892)\n",
      "Test Accuracy of     6: 97% (937/958)\n",
      "Test Accuracy of     7: 98% (1010/1028)\n",
      "Test Accuracy of     8: 98% (956/974)\n",
      "Test Accuracy of     9: 97% (983/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9865/10000)\n",
      "[ 1/30] train_loss: 0.02991 valid_loss: 0.03797\n",
      "Validation loss decreased (inf --> 0.037974).  Saving model ...\n",
      "[ 2/30] train_loss: 0.02807 valid_loss: 0.03599\n",
      "Validation loss decreased (0.037974 --> 0.035993).  Saving model ...\n",
      "[ 3/30] train_loss: 0.02687 valid_loss: 0.03699\n",
      "EarlyStopping counter: 1 out of 6.0\n",
      "[ 4/30] train_loss: 0.02561 valid_loss: 0.03542\n",
      "Validation loss decreased (0.035993 --> 0.035420).  Saving model ...\n",
      "[ 5/30] train_loss: 0.02389 valid_loss: 0.03563\n",
      "EarlyStopping counter: 1 out of 6.0\n",
      "[ 6/30] train_loss: 0.02273 valid_loss: 0.03820\n",
      "EarlyStopping counter: 2 out of 6.0\n",
      "[ 7/30] train_loss: 0.02202 valid_loss: 0.03560\n",
      "EarlyStopping counter: 3 out of 6.0\n",
      "[ 8/30] train_loss: 0.02099 valid_loss: 0.03941\n",
      "EarlyStopping counter: 4 out of 6.0\n",
      "[ 9/30] train_loss: 0.01997 valid_loss: 0.03632\n",
      "EarlyStopping counter: 5 out of 6.0\n",
      "[10/30] train_loss: 0.01931 valid_loss: 0.03685\n",
      "EarlyStopping counter: 6 out of 6.0\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_correct [975.0, 1126.0, 1021.0, 998.0, 975.0, 881.0, 941.0, 1017.0, 958.0, 985.0]\n",
      "Test Loss: 0.037290\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1126/1135)\n",
      "Test Accuracy of     2: 98% (1021/1032)\n",
      "Test Accuracy of     3: 98% (998/1010)\n",
      "Test Accuracy of     4: 99% (975/982)\n",
      "Test Accuracy of     5: 98% (881/892)\n",
      "Test Accuracy of     6: 98% (941/958)\n",
      "Test Accuracy of     7: 98% (1017/1028)\n",
      "Test Accuracy of     8: 98% (958/974)\n",
      "Test Accuracy of     9: 97% (985/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9877/10000)\n",
      "[ 1/30] train_loss: 0.02776 valid_loss: 0.02494\n",
      "Validation loss decreased (inf --> 0.024945).  Saving model ...\n",
      "[ 2/30] train_loss: 0.02641 valid_loss: 0.02533\n",
      "EarlyStopping counter: 1 out of 7.0\n",
      "[ 3/30] train_loss: 0.02442 valid_loss: 0.02895\n",
      "EarlyStopping counter: 2 out of 7.0\n",
      "[ 4/30] train_loss: 0.02403 valid_loss: 0.02567\n",
      "EarlyStopping counter: 3 out of 7.0\n",
      "[ 5/30] train_loss: 0.02290 valid_loss: 0.02640\n",
      "EarlyStopping counter: 4 out of 7.0\n",
      "[ 6/30] train_loss: 0.02164 valid_loss: 0.02350\n",
      "Validation loss decreased (0.024945 --> 0.023504).  Saving model ...\n",
      "[ 7/30] train_loss: 0.02074 valid_loss: 0.02831\n",
      "EarlyStopping counter: 1 out of 7.0\n",
      "[ 8/30] train_loss: 0.01997 valid_loss: 0.02649\n",
      "EarlyStopping counter: 2 out of 7.0\n",
      "[ 9/30] train_loss: 0.01913 valid_loss: 0.02644\n",
      "EarlyStopping counter: 3 out of 7.0\n",
      "[10/30] train_loss: 0.01837 valid_loss: 0.02489\n",
      "EarlyStopping counter: 4 out of 7.0\n",
      "[11/30] train_loss: 0.01708 valid_loss: 0.02299\n",
      "Validation loss decreased (0.023504 --> 0.022989).  Saving model ...\n",
      "[12/30] train_loss: 0.01639 valid_loss: 0.02476\n",
      "EarlyStopping counter: 1 out of 7.0\n",
      "[13/30] train_loss: 0.01671 valid_loss: 0.02527\n",
      "EarlyStopping counter: 2 out of 7.0\n",
      "[14/30] train_loss: 0.01524 valid_loss: 0.02870\n",
      "EarlyStopping counter: 3 out of 7.0\n",
      "[15/30] train_loss: 0.01460 valid_loss: 0.02394\n",
      "EarlyStopping counter: 4 out of 7.0\n",
      "[16/30] train_loss: 0.01444 valid_loss: 0.02670\n",
      "EarlyStopping counter: 5 out of 7.0\n",
      "[17/30] train_loss: 0.01386 valid_loss: 0.02373\n",
      "EarlyStopping counter: 6 out of 7.0\n",
      "[18/30] train_loss: 0.01276 valid_loss: 0.02390\n",
      "EarlyStopping counter: 7 out of 7.0\n",
      "Early stopping\n",
      "class_correct [976.0, 1128.0, 1020.0, 1001.0, 972.0, 878.0, 948.0, 1015.0, 952.0, 989.0]\n",
      "Test Loss: 0.037130\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 98% (1020/1032)\n",
      "Test Accuracy of     3: 99% (1001/1010)\n",
      "Test Accuracy of     4: 98% (972/982)\n",
      "Test Accuracy of     5: 98% (878/892)\n",
      "Test Accuracy of     6: 98% (948/958)\n",
      "Test Accuracy of     7: 98% (1015/1028)\n",
      "Test Accuracy of     8: 97% (952/974)\n",
      "Test Accuracy of     9: 98% (989/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9879/10000)\n",
      "[ 1/30] train_loss: 0.01969 valid_loss: 0.01782\n",
      "Validation loss decreased (inf --> 0.017824).  Saving model ...\n",
      "[ 2/30] train_loss: 0.01844 valid_loss: 0.01645\n",
      "Validation loss decreased (0.017824 --> 0.016453).  Saving model ...\n",
      "[ 3/30] train_loss: 0.01756 valid_loss: 0.01598\n",
      "Validation loss decreased (0.016453 --> 0.015981).  Saving model ...\n",
      "[ 4/30] train_loss: 0.01611 valid_loss: 0.01839\n",
      "EarlyStopping counter: 1 out of 8.0\n",
      "[ 5/30] train_loss: 0.01495 valid_loss: 0.02181\n",
      "EarlyStopping counter: 2 out of 8.0\n",
      "[ 6/30] train_loss: 0.01497 valid_loss: 0.01772\n",
      "EarlyStopping counter: 3 out of 8.0\n",
      "[ 7/30] train_loss: 0.01421 valid_loss: 0.01579\n",
      "Validation loss decreased (0.015981 --> 0.015791).  Saving model ...\n",
      "[ 8/30] train_loss: 0.01307 valid_loss: 0.01671\n",
      "EarlyStopping counter: 1 out of 8.0\n",
      "[ 9/30] train_loss: 0.01284 valid_loss: 0.02052\n",
      "EarlyStopping counter: 2 out of 8.0\n",
      "[10/30] train_loss: 0.01225 valid_loss: 0.02211\n",
      "EarlyStopping counter: 3 out of 8.0\n",
      "[11/30] train_loss: 0.01140 valid_loss: 0.01943\n",
      "EarlyStopping counter: 4 out of 8.0\n",
      "[12/30] train_loss: 0.01112 valid_loss: 0.02097\n",
      "EarlyStopping counter: 5 out of 8.0\n",
      "[13/30] train_loss: 0.01052 valid_loss: 0.01871\n",
      "EarlyStopping counter: 6 out of 8.0\n",
      "[14/30] train_loss: 0.00988 valid_loss: 0.02149\n",
      "EarlyStopping counter: 7 out of 8.0\n",
      "[15/30] train_loss: 0.00938 valid_loss: 0.01919\n",
      "EarlyStopping counter: 8 out of 8.0\n",
      "Early stopping\n",
      "class_correct [974.0, 1128.0, 1025.0, 1001.0, 973.0, 872.0, 942.0, 1015.0, 960.0, 994.0]\n",
      "Test Loss: 0.034449\n",
      "\n",
      "Test Accuracy of     0: 99% (974/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 99% (1025/1032)\n",
      "Test Accuracy of     3: 99% (1001/1010)\n",
      "Test Accuracy of     4: 99% (973/982)\n",
      "Test Accuracy of     5: 97% (872/892)\n",
      "Test Accuracy of     6: 98% (942/958)\n",
      "Test Accuracy of     7: 98% (1015/1028)\n",
      "Test Accuracy of     8: 98% (960/974)\n",
      "Test Accuracy of     9: 98% (994/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9884/10000)\n",
      "[ 1/30] train_loss: 0.01490 valid_loss: 0.01362\n",
      "Validation loss decreased (inf --> 0.013616).  Saving model ...\n",
      "[ 2/30] train_loss: 0.01422 valid_loss: 0.01305\n",
      "Validation loss decreased (0.013616 --> 0.013051).  Saving model ...\n",
      "[ 3/30] train_loss: 0.01352 valid_loss: 0.01783\n",
      "EarlyStopping counter: 1 out of 9.0\n",
      "[ 4/30] train_loss: 0.01271 valid_loss: 0.01315\n",
      "EarlyStopping counter: 2 out of 9.0\n",
      "[ 5/30] train_loss: 0.01244 valid_loss: 0.01501\n",
      "EarlyStopping counter: 3 out of 9.0\n",
      "[ 6/30] train_loss: 0.01155 valid_loss: 0.01873\n",
      "EarlyStopping counter: 4 out of 9.0\n",
      "[ 7/30] train_loss: 0.01109 valid_loss: 0.01561\n",
      "EarlyStopping counter: 5 out of 9.0\n",
      "[ 8/30] train_loss: 0.01041 valid_loss: 0.01447\n",
      "EarlyStopping counter: 6 out of 9.0\n",
      "[ 9/30] train_loss: 0.00989 valid_loss: 0.01447\n",
      "EarlyStopping counter: 7 out of 9.0\n",
      "[10/30] train_loss: 0.00947 valid_loss: 0.01674\n",
      "EarlyStopping counter: 8 out of 9.0\n",
      "[11/30] train_loss: 0.00923 valid_loss: 0.01795\n",
      "EarlyStopping counter: 9 out of 9.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1130.0, 1026.0, 999.0, 971.0, 873.0, 944.0, 1018.0, 959.0, 992.0]\n",
      "Test Loss: 0.036594\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1130/1135)\n",
      "Test Accuracy of     2: 99% (1026/1032)\n",
      "Test Accuracy of     3: 98% (999/1010)\n",
      "Test Accuracy of     4: 98% (971/982)\n",
      "Test Accuracy of     5: 97% (873/892)\n",
      "Test Accuracy of     6: 98% (944/958)\n",
      "Test Accuracy of     7: 99% (1018/1028)\n",
      "Test Accuracy of     8: 98% (959/974)\n",
      "Test Accuracy of     9: 98% (992/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9887/10000)\n",
      "[ 1/30] train_loss: 0.01349 valid_loss: 0.01307\n",
      "Validation loss decreased (inf --> 0.013068).  Saving model ...\n",
      "[ 2/30] train_loss: 0.01242 valid_loss: 0.01317\n",
      "EarlyStopping counter: 1 out of 10.0\n",
      "[ 3/30] train_loss: 0.01209 valid_loss: 0.01412\n",
      "EarlyStopping counter: 2 out of 10.0\n",
      "[ 4/30] train_loss: 0.01167 valid_loss: 0.02192\n",
      "EarlyStopping counter: 3 out of 10.0\n",
      "[ 5/30] train_loss: 0.01118 valid_loss: 0.01589\n",
      "EarlyStopping counter: 4 out of 10.0\n",
      "[ 6/30] train_loss: 0.01046 valid_loss: 0.01634\n",
      "EarlyStopping counter: 5 out of 10.0\n",
      "[ 7/30] train_loss: 0.00945 valid_loss: 0.01652\n",
      "EarlyStopping counter: 6 out of 10.0\n",
      "[ 8/30] train_loss: 0.00927 valid_loss: 0.01834\n",
      "EarlyStopping counter: 7 out of 10.0\n",
      "[ 9/30] train_loss: 0.00889 valid_loss: 0.01833\n",
      "EarlyStopping counter: 8 out of 10.0\n",
      "[10/30] train_loss: 0.00853 valid_loss: 0.01632\n",
      "EarlyStopping counter: 9 out of 10.0\n",
      "[11/30] train_loss: 0.00784 valid_loss: 0.01568\n",
      "EarlyStopping counter: 10 out of 10.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1129.0, 1021.0, 999.0, 973.0, 883.0, 940.0, 1015.0, 957.0, 990.0]\n",
      "Test Loss: 0.035929\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1129/1135)\n",
      "Test Accuracy of     2: 98% (1021/1032)\n",
      "Test Accuracy of     3: 98% (999/1010)\n",
      "Test Accuracy of     4: 99% (973/982)\n",
      "Test Accuracy of     5: 98% (883/892)\n",
      "Test Accuracy of     6: 98% (940/958)\n",
      "Test Accuracy of     7: 98% (1015/1028)\n",
      "Test Accuracy of     8: 98% (957/974)\n",
      "Test Accuracy of     9: 98% (990/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9882/10000)\n",
      "[ 1/30] train_loss: 0.01295 valid_loss: 0.01543\n",
      "Validation loss decreased (inf --> 0.015434).  Saving model ...\n",
      "[ 2/30] train_loss: 0.01252 valid_loss: 0.01529\n",
      "Validation loss decreased (0.015434 --> 0.015294).  Saving model ...\n",
      "[ 3/30] train_loss: 0.01166 valid_loss: 0.01400\n",
      "Validation loss decreased (0.015294 --> 0.014004).  Saving model ...\n",
      "[ 4/30] train_loss: 0.01053 valid_loss: 0.01572\n",
      "EarlyStopping counter: 1 out of 11.0\n",
      "[ 5/30] train_loss: 0.01021 valid_loss: 0.01339\n",
      "Validation loss decreased (0.014004 --> 0.013386).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/30] train_loss: 0.00993 valid_loss: 0.01477\n",
      "EarlyStopping counter: 1 out of 11.0\n",
      "[ 7/30] train_loss: 0.00928 valid_loss: 0.01541\n",
      "EarlyStopping counter: 2 out of 11.0\n",
      "[ 8/30] train_loss: 0.00890 valid_loss: 0.01709\n",
      "EarlyStopping counter: 3 out of 11.0\n",
      "[ 9/30] train_loss: 0.00860 valid_loss: 0.01562\n",
      "EarlyStopping counter: 4 out of 11.0\n",
      "[10/30] train_loss: 0.00802 valid_loss: 0.01517\n",
      "EarlyStopping counter: 5 out of 11.0\n",
      "[11/30] train_loss: 0.00733 valid_loss: 0.02175\n",
      "EarlyStopping counter: 6 out of 11.0\n",
      "[12/30] train_loss: 0.00725 valid_loss: 0.01776\n",
      "EarlyStopping counter: 7 out of 11.0\n",
      "[13/30] train_loss: 0.00730 valid_loss: 0.01542\n",
      "EarlyStopping counter: 8 out of 11.0\n",
      "[14/30] train_loss: 0.00665 valid_loss: 0.01987\n",
      "EarlyStopping counter: 9 out of 11.0\n",
      "[15/30] train_loss: 0.00627 valid_loss: 0.01502\n",
      "EarlyStopping counter: 10 out of 11.0\n",
      "[16/30] train_loss: 0.00589 valid_loss: 0.01802\n",
      "EarlyStopping counter: 11 out of 11.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1129.0, 1025.0, 997.0, 974.0, 880.0, 944.0, 1017.0, 964.0, 991.0]\n",
      "Test Loss: 0.034349\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1129/1135)\n",
      "Test Accuracy of     2: 99% (1025/1032)\n",
      "Test Accuracy of     3: 98% (997/1010)\n",
      "Test Accuracy of     4: 99% (974/982)\n",
      "Test Accuracy of     5: 98% (880/892)\n",
      "Test Accuracy of     6: 98% (944/958)\n",
      "Test Accuracy of     7: 98% (1017/1028)\n",
      "Test Accuracy of     8: 98% (964/974)\n",
      "Test Accuracy of     9: 98% (991/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9896/10000)\n",
      "[ 1/30] train_loss: 0.01093 valid_loss: 0.01469\n",
      "Validation loss decreased (inf --> 0.014695).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00992 valid_loss: 0.01178\n",
      "Validation loss decreased (0.014695 --> 0.011780).  Saving model ...\n",
      "[ 3/30] train_loss: 0.00922 valid_loss: 0.01407\n",
      "EarlyStopping counter: 1 out of 12.0\n",
      "[ 4/30] train_loss: 0.00896 valid_loss: 0.01335\n",
      "EarlyStopping counter: 2 out of 12.0\n",
      "[ 5/30] train_loss: 0.00868 valid_loss: 0.01418\n",
      "EarlyStopping counter: 3 out of 12.0\n",
      "[ 6/30] train_loss: 0.00794 valid_loss: 0.01434\n",
      "EarlyStopping counter: 4 out of 12.0\n",
      "[ 7/30] train_loss: 0.00771 valid_loss: 0.01380\n",
      "EarlyStopping counter: 5 out of 12.0\n",
      "[ 8/30] train_loss: 0.00712 valid_loss: 0.01362\n",
      "EarlyStopping counter: 6 out of 12.0\n",
      "[ 9/30] train_loss: 0.00712 valid_loss: 0.01922\n",
      "EarlyStopping counter: 7 out of 12.0\n",
      "[10/30] train_loss: 0.00689 valid_loss: 0.01458\n",
      "EarlyStopping counter: 8 out of 12.0\n",
      "[11/30] train_loss: 0.00643 valid_loss: 0.01478\n",
      "EarlyStopping counter: 9 out of 12.0\n",
      "[12/30] train_loss: 0.00573 valid_loss: 0.01529\n",
      "EarlyStopping counter: 10 out of 12.0\n",
      "[13/30] train_loss: 0.00571 valid_loss: 0.01653\n",
      "EarlyStopping counter: 11 out of 12.0\n",
      "[14/30] train_loss: 0.00527 valid_loss: 0.01625\n",
      "EarlyStopping counter: 12 out of 12.0\n",
      "Early stopping\n",
      "class_correct [976.0, 1129.0, 1023.0, 998.0, 964.0, 873.0, 946.0, 1016.0, 961.0, 994.0]\n",
      "Test Loss: 0.036458\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1129/1135)\n",
      "Test Accuracy of     2: 99% (1023/1032)\n",
      "Test Accuracy of     3: 98% (998/1010)\n",
      "Test Accuracy of     4: 98% (964/982)\n",
      "Test Accuracy of     5: 97% (873/892)\n",
      "Test Accuracy of     6: 98% (946/958)\n",
      "Test Accuracy of     7: 98% (1016/1028)\n",
      "Test Accuracy of     8: 98% (961/974)\n",
      "Test Accuracy of     9: 98% (994/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9880/10000)\n",
      "[ 1/30] train_loss: 0.01034 valid_loss: 0.01216\n",
      "Validation loss decreased (inf --> 0.012160).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00950 valid_loss: 0.01154\n",
      "Validation loss decreased (0.012160 --> 0.011540).  Saving model ...\n",
      "[ 3/30] train_loss: 0.00887 valid_loss: 0.01079\n",
      "Validation loss decreased (0.011540 --> 0.010785).  Saving model ...\n",
      "[ 4/30] train_loss: 0.00846 valid_loss: 0.01458\n",
      "EarlyStopping counter: 1 out of 13.0\n",
      "[ 5/30] train_loss: 0.00861 valid_loss: 0.01353\n",
      "EarlyStopping counter: 2 out of 13.0\n",
      "[ 6/30] train_loss: 0.00803 valid_loss: 0.01244\n",
      "EarlyStopping counter: 3 out of 13.0\n",
      "[ 7/30] train_loss: 0.00708 valid_loss: 0.01251\n",
      "EarlyStopping counter: 4 out of 13.0\n",
      "[ 8/30] train_loss: 0.00667 valid_loss: 0.01140\n",
      "EarlyStopping counter: 5 out of 13.0\n",
      "[ 9/30] train_loss: 0.00655 valid_loss: 0.01156\n",
      "EarlyStopping counter: 6 out of 13.0\n",
      "[10/30] train_loss: 0.00639 valid_loss: 0.01307\n",
      "EarlyStopping counter: 7 out of 13.0\n",
      "[11/30] train_loss: 0.00602 valid_loss: 0.01474\n",
      "EarlyStopping counter: 8 out of 13.0\n",
      "[12/30] train_loss: 0.00577 valid_loss: 0.01359\n",
      "EarlyStopping counter: 9 out of 13.0\n",
      "[13/30] train_loss: 0.00546 valid_loss: 0.01305\n",
      "EarlyStopping counter: 10 out of 13.0\n",
      "[14/30] train_loss: 0.00525 valid_loss: 0.01352\n",
      "EarlyStopping counter: 11 out of 13.0\n",
      "[15/30] train_loss: 0.00516 valid_loss: 0.01421\n",
      "EarlyStopping counter: 12 out of 13.0\n",
      "[16/30] train_loss: 0.00483 valid_loss: 0.01310\n",
      "EarlyStopping counter: 13 out of 13.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1131.0, 1022.0, 1005.0, 972.0, 870.0, 939.0, 1019.0, 959.0, 990.0]\n",
      "Test Loss: 0.038173\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1131/1135)\n",
      "Test Accuracy of     2: 99% (1022/1032)\n",
      "Test Accuracy of     3: 99% (1005/1010)\n",
      "Test Accuracy of     4: 98% (972/982)\n",
      "Test Accuracy of     5: 97% (870/892)\n",
      "Test Accuracy of     6: 98% (939/958)\n",
      "Test Accuracy of     7: 99% (1019/1028)\n",
      "Test Accuracy of     8: 98% (959/974)\n",
      "Test Accuracy of     9: 98% (990/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9882/10000)\n",
      "[ 1/30] train_loss: 0.00946 valid_loss: 0.00954\n",
      "Validation loss decreased (inf --> 0.009541).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00828 valid_loss: 0.00900\n",
      "Validation loss decreased (0.009541 --> 0.008996).  Saving model ...\n",
      "[ 3/30] train_loss: 0.00818 valid_loss: 0.00941\n",
      "EarlyStopping counter: 1 out of 14.0\n",
      "[ 4/30] train_loss: 0.00752 valid_loss: 0.01103\n",
      "EarlyStopping counter: 2 out of 14.0\n",
      "[ 5/30] train_loss: 0.00705 valid_loss: 0.01104\n",
      "EarlyStopping counter: 3 out of 14.0\n",
      "[ 6/30] train_loss: 0.00702 valid_loss: 0.01061\n",
      "EarlyStopping counter: 4 out of 14.0\n",
      "[ 7/30] train_loss: 0.00633 valid_loss: 0.01279\n",
      "EarlyStopping counter: 5 out of 14.0\n",
      "[ 8/30] train_loss: 0.00638 valid_loss: 0.01106\n",
      "EarlyStopping counter: 6 out of 14.0\n",
      "[ 9/30] train_loss: 0.00579 valid_loss: 0.01140\n",
      "EarlyStopping counter: 7 out of 14.0\n",
      "[10/30] train_loss: 0.00561 valid_loss: 0.01613\n",
      "EarlyStopping counter: 8 out of 14.0\n",
      "[11/30] train_loss: 0.00545 valid_loss: 0.01083\n",
      "EarlyStopping counter: 9 out of 14.0\n",
      "[12/30] train_loss: 0.00499 valid_loss: 0.01192\n",
      "EarlyStopping counter: 10 out of 14.0\n",
      "[13/30] train_loss: 0.00452 valid_loss: 0.01237\n",
      "EarlyStopping counter: 11 out of 14.0\n",
      "[14/30] train_loss: 0.00457 valid_loss: 0.01132\n",
      "EarlyStopping counter: 12 out of 14.0\n",
      "[15/30] train_loss: 0.00428 valid_loss: 0.01271\n",
      "EarlyStopping counter: 13 out of 14.0\n",
      "[16/30] train_loss: 0.00421 valid_loss: 0.01372\n",
      "EarlyStopping counter: 14 out of 14.0\n",
      "Early stopping\n",
      "class_correct [974.0, 1128.0, 1020.0, 997.0, 971.0, 879.0, 947.0, 1017.0, 957.0, 989.0]\n",
      "Test Loss: 0.037356\n",
      "\n",
      "Test Accuracy of     0: 99% (974/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 98% (1020/1032)\n",
      "Test Accuracy of     3: 98% (997/1010)\n",
      "Test Accuracy of     4: 98% (971/982)\n",
      "Test Accuracy of     5: 98% (879/892)\n",
      "Test Accuracy of     6: 98% (947/958)\n",
      "Test Accuracy of     7: 98% (1017/1028)\n",
      "Test Accuracy of     8: 98% (957/974)\n",
      "Test Accuracy of     9: 98% (989/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9879/10000)\n",
      "[ 1/30] train_loss: 0.00862 valid_loss: 0.00873\n",
      "Validation loss decreased (inf --> 0.008727).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00781 valid_loss: 0.00975\n",
      "EarlyStopping counter: 1 out of 15.0\n",
      "[ 3/30] train_loss: 0.00761 valid_loss: 0.01009\n",
      "EarlyStopping counter: 2 out of 15.0\n",
      "[ 4/30] train_loss: 0.00737 valid_loss: 0.01238\n",
      "EarlyStopping counter: 3 out of 15.0\n",
      "[ 5/30] train_loss: 0.00674 valid_loss: 0.01151\n",
      "EarlyStopping counter: 4 out of 15.0\n",
      "[ 6/30] train_loss: 0.00612 valid_loss: 0.01141\n",
      "EarlyStopping counter: 5 out of 15.0\n",
      "[ 7/30] train_loss: 0.00621 valid_loss: 0.01062\n",
      "EarlyStopping counter: 6 out of 15.0\n",
      "[ 8/30] train_loss: 0.00583 valid_loss: 0.01197\n",
      "EarlyStopping counter: 7 out of 15.0\n",
      "[ 9/30] train_loss: 0.00532 valid_loss: 0.01025\n",
      "EarlyStopping counter: 8 out of 15.0\n",
      "[10/30] train_loss: 0.00519 valid_loss: 0.01090\n",
      "EarlyStopping counter: 9 out of 15.0\n",
      "[11/30] train_loss: 0.00502 valid_loss: 0.01068\n",
      "EarlyStopping counter: 10 out of 15.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/30] train_loss: 0.00493 valid_loss: 0.01134\n",
      "EarlyStopping counter: 11 out of 15.0\n",
      "[13/30] train_loss: 0.00484 valid_loss: 0.01108\n",
      "EarlyStopping counter: 12 out of 15.0\n",
      "[14/30] train_loss: 0.00440 valid_loss: 0.01216\n",
      "EarlyStopping counter: 13 out of 15.0\n",
      "[15/30] train_loss: 0.00408 valid_loss: 0.01094\n",
      "EarlyStopping counter: 14 out of 15.0\n",
      "[16/30] train_loss: 0.00421 valid_loss: 0.01344\n",
      "EarlyStopping counter: 15 out of 15.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1130.0, 1024.0, 997.0, 964.0, 874.0, 949.0, 1014.0, 958.0, 994.0]\n",
      "Test Loss: 0.038130\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1130/1135)\n",
      "Test Accuracy of     2: 99% (1024/1032)\n",
      "Test Accuracy of     3: 98% (997/1010)\n",
      "Test Accuracy of     4: 98% (964/982)\n",
      "Test Accuracy of     5: 97% (874/892)\n",
      "Test Accuracy of     6: 99% (949/958)\n",
      "Test Accuracy of     7: 98% (1014/1028)\n",
      "Test Accuracy of     8: 98% (958/974)\n",
      "Test Accuracy of     9: 98% (994/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9879/10000)\n",
      "[ 1/30] train_loss: 0.00802 valid_loss: 0.00899\n",
      "Validation loss decreased (inf --> 0.008987).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00784 valid_loss: 0.00793\n",
      "Validation loss decreased (0.008987 --> 0.007929).  Saving model ...\n",
      "[ 3/30] train_loss: 0.00709 valid_loss: 0.00956\n",
      "EarlyStopping counter: 1 out of 16.0\n",
      "[ 4/30] train_loss: 0.00720 valid_loss: 0.00976\n",
      "EarlyStopping counter: 2 out of 16.0\n",
      "[ 5/30] train_loss: 0.00648 valid_loss: 0.00984\n",
      "EarlyStopping counter: 3 out of 16.0\n",
      "[ 6/30] train_loss: 0.00607 valid_loss: 0.00969\n",
      "EarlyStopping counter: 4 out of 16.0\n",
      "[ 7/30] train_loss: 0.00596 valid_loss: 0.01010\n",
      "EarlyStopping counter: 5 out of 16.0\n",
      "[ 8/30] train_loss: 0.00575 valid_loss: 0.00946\n",
      "EarlyStopping counter: 6 out of 16.0\n",
      "[ 9/30] train_loss: 0.00540 valid_loss: 0.01039\n",
      "EarlyStopping counter: 7 out of 16.0\n",
      "[10/30] train_loss: 0.00544 valid_loss: 0.01016\n",
      "EarlyStopping counter: 8 out of 16.0\n",
      "[11/30] train_loss: 0.00507 valid_loss: 0.01118\n",
      "EarlyStopping counter: 9 out of 16.0\n",
      "[12/30] train_loss: 0.00478 valid_loss: 0.01063\n",
      "EarlyStopping counter: 10 out of 16.0\n",
      "[13/30] train_loss: 0.00480 valid_loss: 0.01027\n",
      "EarlyStopping counter: 11 out of 16.0\n",
      "[14/30] train_loss: 0.00437 valid_loss: 0.01241\n",
      "EarlyStopping counter: 12 out of 16.0\n",
      "[15/30] train_loss: 0.00462 valid_loss: 0.01100\n",
      "EarlyStopping counter: 13 out of 16.0\n",
      "[16/30] train_loss: 0.00387 valid_loss: 0.01159\n",
      "EarlyStopping counter: 14 out of 16.0\n",
      "[17/30] train_loss: 0.00359 valid_loss: 0.01163\n",
      "EarlyStopping counter: 15 out of 16.0\n",
      "[18/30] train_loss: 0.00368 valid_loss: 0.01112\n",
      "EarlyStopping counter: 16 out of 16.0\n",
      "Early stopping\n",
      "class_correct [976.0, 1129.0, 1025.0, 1001.0, 969.0, 875.0, 946.0, 1018.0, 963.0, 989.0]\n",
      "Test Loss: 0.036600\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1129/1135)\n",
      "Test Accuracy of     2: 99% (1025/1032)\n",
      "Test Accuracy of     3: 99% (1001/1010)\n",
      "Test Accuracy of     4: 98% (969/982)\n",
      "Test Accuracy of     5: 98% (875/892)\n",
      "Test Accuracy of     6: 98% (946/958)\n",
      "Test Accuracy of     7: 99% (1018/1028)\n",
      "Test Accuracy of     8: 98% (963/974)\n",
      "Test Accuracy of     9: 98% (989/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9891/10000)\n",
      "[ 1/30] train_loss: 0.00752 valid_loss: 0.00859\n",
      "Validation loss decreased (inf --> 0.008588).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00697 valid_loss: 0.01115\n",
      "EarlyStopping counter: 1 out of 17.0\n",
      "[ 3/30] train_loss: 0.00623 valid_loss: 0.01003\n",
      "EarlyStopping counter: 2 out of 17.0\n",
      "[ 4/30] train_loss: 0.00582 valid_loss: 0.01011\n",
      "EarlyStopping counter: 3 out of 17.0\n",
      "[ 5/30] train_loss: 0.00567 valid_loss: 0.01023\n",
      "EarlyStopping counter: 4 out of 17.0\n",
      "[ 6/30] train_loss: 0.00534 valid_loss: 0.01083\n",
      "EarlyStopping counter: 5 out of 17.0\n",
      "[ 7/30] train_loss: 0.00516 valid_loss: 0.01118\n",
      "EarlyStopping counter: 6 out of 17.0\n",
      "[ 8/30] train_loss: 0.00508 valid_loss: 0.01088\n",
      "EarlyStopping counter: 7 out of 17.0\n",
      "[ 9/30] train_loss: 0.00456 valid_loss: 0.01168\n",
      "EarlyStopping counter: 8 out of 17.0\n",
      "[10/30] train_loss: 0.00432 valid_loss: 0.01245\n",
      "EarlyStopping counter: 9 out of 17.0\n",
      "[11/30] train_loss: 0.00419 valid_loss: 0.01225\n",
      "EarlyStopping counter: 10 out of 17.0\n",
      "[12/30] train_loss: 0.00390 valid_loss: 0.01131\n",
      "EarlyStopping counter: 11 out of 17.0\n",
      "[13/30] train_loss: 0.00368 valid_loss: 0.01300\n",
      "EarlyStopping counter: 12 out of 17.0\n",
      "[14/30] train_loss: 0.00359 valid_loss: 0.01268\n",
      "EarlyStopping counter: 13 out of 17.0\n",
      "[15/30] train_loss: 0.00359 valid_loss: 0.01181\n",
      "EarlyStopping counter: 14 out of 17.0\n",
      "[16/30] train_loss: 0.00325 valid_loss: 0.01264\n",
      "EarlyStopping counter: 15 out of 17.0\n",
      "[17/30] train_loss: 0.00331 valid_loss: 0.01332\n",
      "EarlyStopping counter: 16 out of 17.0\n",
      "[18/30] train_loss: 0.00302 valid_loss: 0.01310\n",
      "EarlyStopping counter: 17 out of 17.0\n",
      "Early stopping\n",
      "class_correct [976.0, 1130.0, 1025.0, 997.0, 966.0, 875.0, 946.0, 1018.0, 962.0, 992.0]\n",
      "Test Loss: 0.037335\n",
      "\n",
      "Test Accuracy of     0: 99% (976/980)\n",
      "Test Accuracy of     1: 99% (1130/1135)\n",
      "Test Accuracy of     2: 99% (1025/1032)\n",
      "Test Accuracy of     3: 98% (997/1010)\n",
      "Test Accuracy of     4: 98% (966/982)\n",
      "Test Accuracy of     5: 98% (875/892)\n",
      "Test Accuracy of     6: 98% (946/958)\n",
      "Test Accuracy of     7: 99% (1018/1028)\n",
      "Test Accuracy of     8: 98% (962/974)\n",
      "Test Accuracy of     9: 98% (992/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9887/10000)\n",
      "[ 1/30] train_loss: 0.00770 valid_loss: 0.00599\n",
      "Validation loss decreased (inf --> 0.005985).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00750 valid_loss: 0.00823\n",
      "EarlyStopping counter: 1 out of 18.0\n",
      "[ 3/30] train_loss: 0.00736 valid_loss: 0.00869\n",
      "EarlyStopping counter: 2 out of 18.0\n",
      "[ 4/30] train_loss: 0.00656 valid_loss: 0.00805\n",
      "EarlyStopping counter: 3 out of 18.0\n",
      "[ 5/30] train_loss: 0.00641 valid_loss: 0.00782\n",
      "EarlyStopping counter: 4 out of 18.0\n",
      "[ 6/30] train_loss: 0.00584 valid_loss: 0.00847\n",
      "EarlyStopping counter: 5 out of 18.0\n",
      "[ 7/30] train_loss: 0.00592 valid_loss: 0.00776\n",
      "EarlyStopping counter: 6 out of 18.0\n",
      "[ 8/30] train_loss: 0.00547 valid_loss: 0.00996\n",
      "EarlyStopping counter: 7 out of 18.0\n",
      "[ 9/30] train_loss: 0.00564 valid_loss: 0.00931\n",
      "EarlyStopping counter: 8 out of 18.0\n",
      "[10/30] train_loss: 0.00515 valid_loss: 0.00907\n",
      "EarlyStopping counter: 9 out of 18.0\n",
      "[11/30] train_loss: 0.00467 valid_loss: 0.00842\n",
      "EarlyStopping counter: 10 out of 18.0\n",
      "[12/30] train_loss: 0.00440 valid_loss: 0.01176\n",
      "EarlyStopping counter: 11 out of 18.0\n",
      "[13/30] train_loss: 0.00422 valid_loss: 0.00843\n",
      "EarlyStopping counter: 12 out of 18.0\n",
      "[14/30] train_loss: 0.00391 valid_loss: 0.00881\n",
      "EarlyStopping counter: 13 out of 18.0\n",
      "[15/30] train_loss: 0.00404 valid_loss: 0.00941\n",
      "EarlyStopping counter: 14 out of 18.0\n",
      "[16/30] train_loss: 0.00373 valid_loss: 0.00939\n",
      "EarlyStopping counter: 15 out of 18.0\n",
      "[17/30] train_loss: 0.00373 valid_loss: 0.00924\n",
      "EarlyStopping counter: 16 out of 18.0\n",
      "[18/30] train_loss: 0.00338 valid_loss: 0.00945\n",
      "EarlyStopping counter: 17 out of 18.0\n",
      "[19/30] train_loss: 0.00355 valid_loss: 0.00849\n",
      "EarlyStopping counter: 18 out of 18.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1131.0, 1023.0, 997.0, 974.0, 875.0, 940.0, 1017.0, 958.0, 991.0]\n",
      "Test Loss: 0.036757\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1131/1135)\n",
      "Test Accuracy of     2: 99% (1023/1032)\n",
      "Test Accuracy of     3: 98% (997/1010)\n",
      "Test Accuracy of     4: 99% (974/982)\n",
      "Test Accuracy of     5: 98% (875/892)\n",
      "Test Accuracy of     6: 98% (940/958)\n",
      "Test Accuracy of     7: 98% (1017/1028)\n",
      "Test Accuracy of     8: 98% (958/974)\n",
      "Test Accuracy of     9: 98% (991/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9881/10000)\n",
      "[ 1/30] train_loss: 0.00691 valid_loss: 0.00824\n",
      "Validation loss decreased (inf --> 0.008236).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00688 valid_loss: 0.00766\n",
      "Validation loss decreased (0.008236 --> 0.007655).  Saving model ...\n",
      "[ 3/30] train_loss: 0.00665 valid_loss: 0.00952\n",
      "EarlyStopping counter: 1 out of 19.0\n",
      "[ 4/30] train_loss: 0.00590 valid_loss: 0.00852\n",
      "EarlyStopping counter: 2 out of 19.0\n",
      "[ 5/30] train_loss: 0.00536 valid_loss: 0.00910\n",
      "EarlyStopping counter: 3 out of 19.0\n",
      "[ 6/30] train_loss: 0.00545 valid_loss: 0.01076\n",
      "EarlyStopping counter: 4 out of 19.0\n",
      "[ 7/30] train_loss: 0.00511 valid_loss: 0.01062\n",
      "EarlyStopping counter: 5 out of 19.0\n",
      "[ 8/30] train_loss: 0.00463 valid_loss: 0.00965\n",
      "EarlyStopping counter: 6 out of 19.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9/30] train_loss: 0.00423 valid_loss: 0.01015\n",
      "EarlyStopping counter: 7 out of 19.0\n",
      "[10/30] train_loss: 0.00427 valid_loss: 0.00922\n",
      "EarlyStopping counter: 8 out of 19.0\n",
      "[11/30] train_loss: 0.00398 valid_loss: 0.01032\n",
      "EarlyStopping counter: 9 out of 19.0\n",
      "[12/30] train_loss: 0.00368 valid_loss: 0.01153\n",
      "EarlyStopping counter: 10 out of 19.0\n",
      "[13/30] train_loss: 0.00373 valid_loss: 0.00960\n",
      "EarlyStopping counter: 11 out of 19.0\n",
      "[14/30] train_loss: 0.00335 valid_loss: 0.01054\n",
      "EarlyStopping counter: 12 out of 19.0\n",
      "[15/30] train_loss: 0.00346 valid_loss: 0.01036\n",
      "EarlyStopping counter: 13 out of 19.0\n",
      "[16/30] train_loss: 0.00318 valid_loss: 0.01027\n",
      "EarlyStopping counter: 14 out of 19.0\n",
      "[17/30] train_loss: 0.00307 valid_loss: 0.01015\n",
      "EarlyStopping counter: 15 out of 19.0\n",
      "[18/30] train_loss: 0.00342 valid_loss: 0.01068\n",
      "EarlyStopping counter: 16 out of 19.0\n",
      "[19/30] train_loss: 0.00279 valid_loss: 0.01053\n",
      "EarlyStopping counter: 17 out of 19.0\n",
      "[20/30] train_loss: 0.00281 valid_loss: 0.01069\n",
      "EarlyStopping counter: 18 out of 19.0\n",
      "[21/30] train_loss: 0.00251 valid_loss: 0.01040\n",
      "EarlyStopping counter: 19 out of 19.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1131.0, 1022.0, 999.0, 978.0, 874.0, 940.0, 1013.0, 960.0, 989.0]\n",
      "Test Loss: 0.038204\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1131/1135)\n",
      "Test Accuracy of     2: 99% (1022/1032)\n",
      "Test Accuracy of     3: 98% (999/1010)\n",
      "Test Accuracy of     4: 99% (978/982)\n",
      "Test Accuracy of     5: 97% (874/892)\n",
      "Test Accuracy of     6: 98% (940/958)\n",
      "Test Accuracy of     7: 98% (1013/1028)\n",
      "Test Accuracy of     8: 98% (960/974)\n",
      "Test Accuracy of     9: 98% (989/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9881/10000)\n",
      "[ 1/30] train_loss: 0.00654 valid_loss: 0.00907\n",
      "Validation loss decreased (inf --> 0.009069).  Saving model ...\n",
      "[ 2/30] train_loss: 0.00614 valid_loss: 0.00856\n",
      "Validation loss decreased (0.009069 --> 0.008563).  Saving model ...\n",
      "[ 3/30] train_loss: 0.00564 valid_loss: 0.00750\n",
      "Validation loss decreased (0.008563 --> 0.007500).  Saving model ...\n",
      "[ 4/30] train_loss: 0.00569 valid_loss: 0.00723\n",
      "Validation loss decreased (0.007500 --> 0.007235).  Saving model ...\n",
      "[ 5/30] train_loss: 0.00512 valid_loss: 0.00788\n",
      "EarlyStopping counter: 1 out of 20.0\n",
      "[ 6/30] train_loss: 0.00540 valid_loss: 0.00871\n",
      "EarlyStopping counter: 2 out of 20.0\n",
      "[ 7/30] train_loss: 0.00509 valid_loss: 0.00830\n",
      "EarlyStopping counter: 3 out of 20.0\n",
      "[ 8/30] train_loss: 0.00446 valid_loss: 0.00874\n",
      "EarlyStopping counter: 4 out of 20.0\n",
      "[ 9/30] train_loss: 0.00466 valid_loss: 0.00878\n",
      "EarlyStopping counter: 5 out of 20.0\n",
      "[10/30] train_loss: 0.00385 valid_loss: 0.00955\n",
      "EarlyStopping counter: 6 out of 20.0\n",
      "[11/30] train_loss: 0.00386 valid_loss: 0.00891\n",
      "EarlyStopping counter: 7 out of 20.0\n",
      "[12/30] train_loss: 0.00392 valid_loss: 0.00865\n",
      "EarlyStopping counter: 8 out of 20.0\n",
      "[13/30] train_loss: 0.00349 valid_loss: 0.00831\n",
      "EarlyStopping counter: 9 out of 20.0\n",
      "[14/30] train_loss: 0.00355 valid_loss: 0.00860\n",
      "EarlyStopping counter: 10 out of 20.0\n",
      "[15/30] train_loss: 0.00348 valid_loss: 0.00860\n",
      "EarlyStopping counter: 11 out of 20.0\n",
      "[16/30] train_loss: 0.00313 valid_loss: 0.00891\n",
      "EarlyStopping counter: 12 out of 20.0\n",
      "[17/30] train_loss: 0.00306 valid_loss: 0.00935\n",
      "EarlyStopping counter: 13 out of 20.0\n",
      "[18/30] train_loss: 0.00279 valid_loss: 0.01051\n",
      "EarlyStopping counter: 14 out of 20.0\n",
      "[19/30] train_loss: 0.00266 valid_loss: 0.00901\n",
      "EarlyStopping counter: 15 out of 20.0\n",
      "[20/30] train_loss: 0.00258 valid_loss: 0.01012\n",
      "EarlyStopping counter: 16 out of 20.0\n",
      "[21/30] train_loss: 0.00254 valid_loss: 0.00924\n",
      "EarlyStopping counter: 17 out of 20.0\n",
      "[22/30] train_loss: 0.00254 valid_loss: 0.01009\n",
      "EarlyStopping counter: 18 out of 20.0\n",
      "[23/30] train_loss: 0.00230 valid_loss: 0.01088\n",
      "EarlyStopping counter: 19 out of 20.0\n",
      "[24/30] train_loss: 0.00232 valid_loss: 0.00915\n",
      "EarlyStopping counter: 20 out of 20.0\n",
      "Early stopping\n",
      "class_correct [975.0, 1128.0, 1022.0, 1003.0, 970.0, 878.0, 942.0, 1016.0, 960.0, 994.0]\n",
      "Test Loss: 0.037693\n",
      "\n",
      "Test Accuracy of     0: 99% (975/980)\n",
      "Test Accuracy of     1: 99% (1128/1135)\n",
      "Test Accuracy of     2: 99% (1022/1032)\n",
      "Test Accuracy of     3: 99% (1003/1010)\n",
      "Test Accuracy of     4: 98% (970/982)\n",
      "Test Accuracy of     5: 98% (878/892)\n",
      "Test Accuracy of     6: 98% (942/958)\n",
      "Test Accuracy of     7: 98% (1016/1028)\n",
      "Test Accuracy of     8: 98% (960/974)\n",
      "Test Accuracy of     9: 98% (994/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9888/10000)\n"
     ]
    }
   ],
   "source": [
    "for patience in patience_par:\n",
    "    batch_size = 64\n",
    "    n_epochs = 30\n",
    "    train_loader, test_loader, valid_loader = create_datasets(batch_size)\n",
    "    model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs)\n",
    "    train_loss_early.append(train_loss)\n",
    "    \n",
    "    batch_size = 1000\n",
    "    n_epochs = 3\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    model.eval() # prep model for evaluation\n",
    "\n",
    "    for data, target in test_loader:\n",
    "    #     print(len(target.data), batch_size)\n",
    "    #     if len(target.data) != batch_size:\n",
    "    #         break\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "    #     print(pred)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(len(target.data)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    print('class_correct', class_correct)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    test_loss_early.append(test_loss)\n",
    "    \n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                str(i), 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    #     else:\n",
    "    #         print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    test_acc_early.append(100. * np.sum(class_correct) / np.sum(class_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot average loss versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average Training and Test Loss (CrossEntropy Loss with Early Stop)\")\n",
    "#plt.plot(patience_par, train_loss_early, label=\"Training\")\n",
    "plt.plot(patience_par, test_loss_early, label=\"Test\")\n",
    "plt.xlabel(\"patience\")\n",
    "plt.ylabel(\"Average loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot accuracy rate versus regularization parameters\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Accuracy Rate (CrossEntropy Loss with Early Stop)\")\n",
    "#plt.plot(reg_params, train_acc_MSE_L2, label=\"Training\")\n",
    "plt.plot(patience_par, test_acc_early, label=\"Test\")\n",
    "plt.xlabel(\"patience\")\n",
    "plt.ylabel(\"100 * Accuracy rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
